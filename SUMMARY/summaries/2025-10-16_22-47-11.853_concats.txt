Here is the file tree and contents of all files in the project:
target_folder: .//
+-- rag/ (361914 bytes)
+-- .env (193 bytes)
+-- .env.example (196 bytes)
X-- .git/ (skipped)
X-- .gitignore (612 bytes)
X-- .knowledge/ (skipped)
X-- .venv/ (skipped)
+-- DESIGN.md (42141 bytes)
+-- DESIGN_additions.md (59188 bytes)
X-- FAE/ (skipped)
+-- README.md (4168 bytes)
X-- SUMMARY/ (skipped)
+-- bin/ (138924 bytes)
    +-- dashboard_server.py (29039 bytes)
    +-- dashboard_static/ (47609 bytes)
        +-- index.html (47609 bytes)
    +-- enrich_worker.py (8064 bytes)
    +-- export_graph.py (771 bytes)
    +-- export_monofile.py (724 bytes)
    +-- ingest_build_graph.py (956 bytes)
    +-- jobs_reset.py (1103 bytes)
    +-- jobs_status.py (9377 bytes)
    +-- llm_gateway_service.py (984 bytes)
    +-- plan_enqueue.py (4393 bytes)
    +-- planner_llm.py (3719 bytes)
    +-- query_rag.py (413 bytes)
    +-- render_report.py (18985 bytes)
    +-- report_attrs.py (1348 bytes)
    +-- report_communities.py (697 bytes)
    +-- test_llm_gateway.py (1751 bytes)
    +-- test_lmstudio.py (8575 bytes)
    +-- watch_daemon.py (416 bytes)
+-- kn/ (61880 bytes)
    +-- __init__.py (0 bytes)
    X-- __pycache__/ (skipped)
    +-- chunking.py (2810 bytes)
    +-- config.py (5288 bytes)
    +-- embeddings.py (791 bytes)
    +-- exporter.py (1601 bytes)
    +-- file_index.py (2615 bytes)
    +-- graph_nx.py (1878 bytes)
    +-- hashing.py (278 bytes)
    +-- jobs_sqlite.py (6727 bytes)
    +-- llm_gateway/ (25951 bytes)
        +-- __init__.py (297 bytes)
        X-- __pycache__/ (skipped)
        +-- client.py (5072 bytes)
        +-- errors.py (470 bytes)
        +-- models.py (1891 bytes)
        +-- runner.py (3651 bytes)
        +-- service_base.py (2862 bytes)
        +-- services.py (996 bytes)
        +-- services_lmstudio.py (3983 bytes)
        +-- storage.py (6729 bytes)
    +-- retrieval.py (4150 bytes)
    +-- utils/ (5149 bytes)
        +-- __init__.py (59 bytes)
        X-- __pycache__/ (skipped)
        +-- cache.py (795 bytes)
        +-- io.py (2633 bytes)
        +-- llm_client.py (794 bytes)
        +-- skeleton.py (868 bytes)
    +-- vector_hnsw.py (4642 bytes)
X-- kn-graphRAG-starter.zip (16255 bytes)
X-- on.tool (4833 bytes)
X-- planner.log (84 bytes)
+-- plugins/ (31725 bytes)
    +-- attributes/ (31725 bytes)
        +-- bridge_candidates.py (1752 bytes)
        +-- chunk_summary.py (824 bytes)
        +-- doc_reduce.py (1778 bytes)
        +-- doc_skeleton.py (1097 bytes)
        +-- faq_pairs.py (868 bytes)
        +-- glossary.py (2949 bytes)
        +-- keyphrases.py (3004 bytes)
        +-- multi_basic.py (2190 bytes)
        +-- pii_scan.py (1369 bytes)
        +-- recent_summary.py (893 bytes)
        +-- requirements.py (879 bytes)
        +-- risk_scan.py (1303 bytes)
        +-- summaries.py (1406 bytes)
        +-- summary_20w.py (3093 bytes)
        +-- summary_long.py (870 bytes)
        +-- summary_medium.py (3055 bytes)
        +-- summary_outline.py (893 bytes)
        +-- summary_short.py (1500 bytes)
        +-- todo_items.py (725 bytes)
        +-- topic_tags.py (1277 bytes)
+-- pyproject.toml (153 bytes)
+-- requirements-win.txt (197 bytes)
+-- requirements.txt (317 bytes)
+-- setup.ps1 (1048 bytes)
X-- worker.log (0 bytes)./README.md
## kn-graphRAG-starter

Local-first Graph RAG toolkit: ingest a code/content repo, chunk it, embed, build a graph, enrich with attribute plugins, and query with lightweight retrieval. Optimized for Windows + local LLMs (LM Studio), but works cross‑platform.

### Features
- Ingest and incremental updates over a target repo or folder
- Chunking + embeddings (SentenceTransformers)
- Graph build + community summaries (NetworkX + Louvain)
- Attribute plugins (summaries, keyphrases, glossary, PII scan, FAQs, etc.)
- Simple dashboard/API to plan/enqueue jobs and inspect coverage

## Quickstart (Windows PowerShell)
1) Python 3.10+ installed
2) From the project root, run:
   - `.\u200bsetup.ps1 -RepoPath ".\FAE" -Model "llama-3.1-8b-instruct"`

What it does:
- Creates `.venv`, installs dependencies from `requirements-win.txt`
- Writes a `.env` with defaults (LLM and embedding models, repo path, knowledge root)
- Runs initial ingest: scan → chunk → embed → graph → summarize

LM Studio optional: start a server at `http://localhost:12345` (or adjust `OPENAI_BASE_URL` in `.env`).

## Manual setup
If you prefer manual steps:
1) Create and activate a venv, then install deps
   - Windows: `python -m venv .venv; . .\.venv\Scripts\Activate.ps1; pip install -r requirements-win.txt`
   - Non‑Windows: `python -m venv .venv; source .venv/bin/activate; pip install -r requirements.txt`
2) Configure `.env` (auto‑generated by `setup.ps1`, example keys):
   - `OPENAI_BASE_URL` (e.g., `http://127.0.0.1:12345/v1`)
   - `OPENAI_API_KEY` (placeholder for local providers)
   - `OPENAI_MODEL` (chat/instruct model id)
   - `EMBED_MODEL=BAAI/bge-small-en-v1.5`
   - `REPO_PATH=./FAE`
   - `KN_ROOT=.knowledge`
   - `OCR_ENABLED=false`
3) Ingest a repo/folder:
   - `python bin/ingest_build_graph.py --repo <path> --full`

## Common workflows
### Query
```bash
python bin/query_rag.py --q "What does the planner do?" --topk 12
```

### Dashboard API server
```bash
python bin/dashboard_server.py
```
Open `http://localhost:5051` for the minimal dashboard.

Selected endpoints:
- `GET /api/status` – system status, plugin coverage, queue, LLM health
- `POST /api/ingest { repo, full }` – start ingest
- `POST /api/plan { plugins, only_missing, limit, ... }` – plan jobs
- `POST /api/worker/start { plugins, batch }` – start a worker
- `POST /api/worker/stop { id? }` – stop a worker (or all)

### Attribute enrichment worker
```bash
python bin/enrich_worker.py --plugins summaries,keyphrases --batch 32 --watch
```
Runs continuously with `--watch`, pulling jobs from `.knowledge/queues/jobs.sqlite`.

### Exports and reports
- `bin/export_graph.py` – export graph
- `bin/export_monofile.py` – export consolidated artifacts
- `bin/render_report.py` + `bin/report_*` – static coverage/report generation

## Data layout (`.knowledge`)
- `indexes/chunks/` – chunk JSONs
- `indexes/attributes/<plugin>/` – plugin outputs
- `queues/jobs.sqlite` – job queue
- `exports/reports/` – generated report site

## Plugins
See `plugins/attributes/` for available attribute extractors:
- `summaries`, `topic-tags`, `pii-scan`, `glossary`, `requirements`, `todo-items`,
  `faq-pairs`, `keyphrases`, `bridge-candidates`, `risk-scan`, `recent-summary`, etc.

## Configuration
`kn.config.load_configs()` merges defaults and `.knowledge/config/*.yml`.
- Plugin defaults are persisted to `.knowledge/config/models.yml` under `plugins`.
- The dashboard exposes `GET/POST /api/plugins/config` to read/update these.

## Notes (Windows)
- `requirements-win.txt` includes `python-magic-bin` and references CUDA wheels if you want PyTorch GPU. CPU works fine for small repos.
- `hnswlib` is disabled on Windows by default (`requirements.txt` enables it for non‑Windows). Vector search falls back to a pure‑Python path when needed.

## Troubleshooting
- LLM health shows unreachable: confirm `OPENAI_BASE_URL` (LM Studio or other local endpoint) and that the server is running.
- No chunks/attributes found: ensure `REPO_PATH` points to a folder with files, and rerun ingest with `--full` for a fresh build.

## License
See repository terms or contact the author.


./requirements.txt
pyyaml
watchdog
hnswlib; sys_platform != 'win32'
networkx
scikit-learn
numpy
pandas
rapidfuzz
python-dotenv
rank-bm25
python-louvain

sentencepiece
transformers>=4.43
accelerate>=0.33
sentence-transformers>=3.0
regex
unidecode

python-magic-bin; sys_platform == 'win32'
PyMuPDF
chardet
# pytesseract  # optional
flask

./requirements-win.txt
-r requirements.txt
# CUDA PyTorch (CUDA 12.8) if needed:
# pip install --index-url https://download.pytorch.org/whl/cu128 ^
#   torch==2.6.0+cu128 torchvision==0.21.0+cu128 torchaudio==2.6.0+cu128

./.env.example
OPENAI_BASE_URL=http://localhost:12345/v1
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=llama-3.1-8b-instruct
EMBED_MODEL=BAAI/bge-small-en-v1.5
REPO_PATH=..\your-repo
KN_ROOT=.knowledge
OCR_ENABLED=false

./pyproject.toml
[project]
name = "kn-graphrag-starter"
version = "0.1.0"
description = "Local modular GraphRAG pipeline (Windows + LM Studio)"
requires-python = ">=3.10"

./.env
﻿OPENAI_BASE_URL=http://10.5.0.2:12345/v1
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=llama-3.1-8b-instruct
EMBED_MODEL=BAAI/bge-small-en-v1.5
REPO_PATH=.\FAE
KN_ROOT=.knowledge
OCR_ENABLED=false


./DESIGN.md
# kn-graphRAG-starter (Windows + LM Studio)

A local, modular GraphRAG-style pipeline for **Windows 10 Pro**, optimized for:

* **Non-intrusive** sidecar indexing (your repo stays untouched)
* **.knowledge/** for all derived artifacts (movable/portable)
* **Incremental** updates via hashing (moves/renames don’t force full rebuilds)
* **Local models first** (LM Studio by default; Ollama/vLLM optional)
* **Fast retrieval** with **HNSW** (hnswlib) by default (simple, Windows-friendly)
* **Graph** using NetworkX (DB-optional), with community summaries
* **Attribute plugins** as standalone scripts (stdin→stdout JSONL)
* **Mono-file export** under token budget for “send to big LLM” workflows

> Minimal third‑party complexity; you can later swap in FAISS/Qdrant/Neo4j/vLLM without redesign.

---

## Final requirements & design choices (v1)

**Platform & runtime**

* OS: **Windows 10 Pro**.
* GPUs: **RTX 5090** primary; **RTX 3090** optional (no device pinning by default; auto device selection).
* LLM runtime: **LM Studio** local server (OpenAI‑compatible) as default; can swap to **vLLM**/**Ollama** later.
* Offline‑first: no outbound calls unless explicitly enabled via `.env`.

**Data & identity**

* Sidecar outputs in **`.knowledge/`**; original repo stays untouched.
* **Document identity:** SHA256 of normalized full text → `doc_id` (short hash displayed).
* **Chunk identity:** `doc_id` + zero‑padded index (`docid-0001`).
* **Moves/renames:** update path metadata only (no re‑embed) if content hash unchanged.
* **Future (pro‑style):** add *chunk‑level* content hashes (already implied by `docid-XXXX`) and optional near‑duplicate detection (MinHash/SimHash) for dedupe across repos.

**Multi‑repo & domains**

* Watcher can track **multiple repo paths**; domains default to top‑level folder under each repo.
* Optional **cross‑domain bridge** edges with threshold **`graph.cross_domain_bridge_threshold`** (default **0.75**). Toggle/adjust per your repos.
* Git submodules are supported implicitly; each path contributes to the global graph.

**Ingestion & OCR**

* Chunking: default **4k chars** with **10% overlap**; type policies for code/PDF.
* OCR: **optional** (Tesseract + `pytesseract`) controlled by `OCR_ENABLED` (default **false**). Only used for images/PDFs **without extractable text**, not “all docs”.

**Embeddings & vector index**

* Default embedder: **BAAI/bge-small-en-v1.5** (English‑optimized, fast). Easy to switch to **bge‑m3** for multilingual.
* Vector store: **hnswlib** (Windows‑friendly). Nightly/periodic **re‑ingest** recommended to rebuild and clear stale vectors (deletes can be added later).

**Graph & communities**

* Build an **entity co‑occurrence** graph (LLM‑assisted extraction can be added later).
* Community detection: **Louvain** (`python-louvain`) when available; fallback to connected components.
* Community summaries: generated and stored in `.knowledge/indexes/summaries/`.

**Retrieval & export**

* Retrieval: **hybrid** (dense via HNSW **+ BM25** via `rank-bm25`) with simple union/merge; reranker optional later.
* Graph‑aware context: preface with top community summaries; roadmap: include 1–2 hop neighbors in prompt assembly.
* Export: **`.md` by default**, token budget heuristic (\~4 chars/token). Optional **exclude PII** in exports via `export.exclude_pii`.

**Attributes & enrichment**

* Plugins = standalone Python scripts (stdin JSONL → stdout) with outputs written to `.knowledge/indexes/attributes/<plugin>/`.
* Included: `summary-20w`, `topic-tags`, **`pii-scan` (regex‑based, fast)**.
* Multiple passes allowed (e.g., `pass: "cheap"` vs `pass: "full"`), keep all outputs with version/metadata; downstream tools choose by `pass` or confidence.
* Job queue: **SQLite** (upgrade path: Redis). Background worker processes batches.

**Operations**

* **Watcher** runs continuously (file adds/edits trigger enrichment). Embeddings refresh via scheduled ingest (e.g., nightly).
* Optional **Task Scheduler** job to run `ingest_build_graph.py` nightly.
* Logs & caches live under `.knowledge/` and are safe to delete/rebuild.

**Security & PII**

* Offline by default; `.env` toggles for any cloud usage.
* `pii-scan` flags emails/phones/credit‑like sequences; set `export.exclude_pii: true` to omit flagged chunks from exports. Future: LLM‑assisted PII + patterns.

**Deliverables available**

* A ready‑to‑run **starter repo zip** accompanies this document with Windows bootstrap (`requirements-win.txt`, `setup.ps1`), code scaffold, and default configs.

---

## Folder Tree

```
kn-graphRAG-starter/
├─ README.md
├─ pyproject.toml
├─ requirements.txt
├─ .env.example
├─ .gitignore
├─ bin/
│  ├─ ingest_build_graph.py
│  ├─ watch_daemon.py
│  ├─ enrich_worker.py
│  ├─ query_rag.py
│  └─ export_monofile.py
├─ kn/
│  ├─ __init__.py
│  ├─ config.py
│  ├─ hashing.py
│  ├─ file_index.py
│  ├─ chunking.py
│  ├─ embeddings.py
│  ├─ vector_hnsw.py
│  ├─ graph_nx.py
│  ├─ retrieval.py
│  ├─ exporter.py
│  ├─ jobs.py
│  └─ utils/
│     ├─ llm_client.py
│     └─ io.py
├─ plugins/
│  ├─ attributes/
│  │  ├─ summary_20w.py
│  │  └─ topic_tags.py
│  └─ chunkers/  (placeholder)
└─ .knowledge/   (generated at runtime)
   ├─ config/
   │  ├─ models.yml
   │  └─ pipeline.yml
   ├─ indexes/
   │  ├─ chunks/
   │  ├─ embeddings/
   │  ├─ graph/
   │  ├─ summaries/
   │  ├─ attributes/
   │  ├─ manifests/
   │  ├─ queues/
   │  └─ cache/
   ├─ exports/
   │  ├─ dumps/
   │  └─ bundles/
   └─ logs/
```

---

## README.md

````md
# kn-graphRAG-starter (Windows + LM Studio)

Local GraphRAG-style pipeline with incremental sidecar indexing. Default stack:
- **LM Studio** (OpenAI-compatible endpoint) for LLM calls
- **hnswlib** for vector search (fast on Windows)
- **NetworkX** for graph & communities (Louvain if available)
- **watchdog** for file change detection

## Quickstart (Windows 10 Pro)

### 1) Install prerequisites
- Python 3.10+
- LM Studio (start local server: Settings → Local Server → Enable; default `http://localhost:1234/v1`)
- (Optional) Git for repo operations; optional Tesseract for OCR

### 2) Create venv & install
```powershell
python -m venv .venv
. .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
````

### 3) Configure

Copy `.env.example` to `.env` and set your LM Studio endpoint/model.
Adjust `.knowledge/config/models.yml` and `pipeline.yml` as needed.

### 4) Index a repo

```powershell
python .\bin\ingest_build_graph.py --repo ..\your-repo --full
```

### 5) Run watcher & enrichment in background terminals

```powershell
# Terminal A: file watcher
python .\bin\watch_daemon.py --repo ..\your-repo

# Terminal B: enrichment plugins (periodic)
python .\bin\enrich_worker.py --plugins summary-20w,topic-tags,pii-scan --watch
```

### 6) Query

```powershell
python .\bin\query_rag.py --q "Design overview of Project X" --scope projectX --topk 12
```

### 7) Export to mono-file (for big LLMs)

```powershell
python .\bin\export_monofile.py --q "Design overview of Project X" --budget 600000 --out .knowledge\exports\dumps\projectX_dump.md
```

## Incremental & Non-intrusive

* `content_hash = SHA256(normalized_text)` for doc identity; `docid-0001` style chunk IDs.
* Moves/renames only update metadata; edits re‑chunk only the changed doc; graph & summaries update incrementally.

## Swaps

* Vector: hnswlib → FAISS or Qdrant later
* Graph: NetworkX → Neo4j/Memgraph later
* LLM runtime: LM Studio → Ollama or vLLM later
* Embedder: `bge-small-en` → `bge-m3` for multilingual

````md
# kn-graphRAG-starter (Windows + LM Studio)

Local GraphRAG-style pipeline with incremental sidecar indexing. Default stack:
- **LM Studio** (OpenAI-compatible endpoint) for LLM calls
- **hnswlib** for vector search (fast CPU ANN on Windows)
- **NetworkX** for graph & communities
- **watchdog** for file change detection

## Quickstart (Windows 10 Pro)

### 1) Install prerequisites
- Python 3.10+
- LM Studio (start local server: Settings → Local Server → Enable; default `http://localhost:1234/v1`)
- (Optional) Git for repo operations

### 2) Create venv & install
```powershell
python -m venv .venv
. .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
````

### 3) Configure

Copy `.env.example` to `.env` and set your LM Studio endpoint/model.
Adjust `.knowledge/config/models.yml` and `pipeline.yml` as needed.

### 4) Index a repo

```powershell
python .\bin\ingest_build_graph.py --repo ..\your-repo --full
```

### 5) Run watcher & enrichment in background terminals

```powershell
# Terminal A: file watcher
python .\bin\watch_daemon.py --repo ..\your-repo

# Terminal B: enrichment plugins (periodic)
python .\bin\enrich_worker.py --plugins summary-20w,topic-tags --watch
```

### 6) Query

```powershell
python .\bin\query_rag.py --q "Design overview of Project X" --scope projectX --topk 12
```

### 7) Export to mono-file (for big LLMs)

```powershell
python .\bin\export_monofile.py --q "Design overview of Project X" --budget 600000 --out .knowledge\exports\dumps\projectX_dump.md
```

## Incremental & Non-intrusive

* We compute `content_hash = SHA256(normalized_text)`. Moves/renames update metadata only.
* Only changed docs are re-chunked/embedded; graph & summaries update incrementally.

## Swaps

* Vector: hnswlib → FAISS or Qdrant later (no redesign)
* Graph: NetworkX → Neo4j/Memgraph later
* LLM runtime: LM Studio → Ollama or vLLM (OpenAI‑compatible) later
* Embedder: `bge-small-en-v1.5` → `bge-m3` for multilingual

````

---

## requirements.txt
```txt
# core
pyyaml
watchdog
hnswlib
networkx
scikit-learn
numpy
pandas
rapidfuzz
python-dotenv
rank-bm25
python-louvain

# text
sentencepiece
transformers>=4.43
accelerate>=0.33
sentence-transformers>=3.0
regex
unidecode

# io + mime + pdf
python-magic-bin; sys_platform == 'win32'
PyMuPDF
chardet

# optional (comment out if not needed)
# pytesseract  # enable OCR when installed + OCR_ENABLED=true
# spacy
# spacy[transformers]
```txt
# core
pyyaml
watchdog
hnswlib
networkx
scikit-learn
numpy
pandas
rapidfuzz
python-dotenv

# text
sentencepiece
transformers
accelerate
sentence-transformers
fuzzywuzzy[speedup]
regex

# io
tqdm
rich
unidecode
python-magic-bin; sys_platform == 'win32'
PyMuPDF

# optional (comment out if not needed)
spacy
spacy[transformers]
````

---

## pyproject.toml

```toml
[project]
name = "kn-graphrag-starter"
version = "0.1.0"
description = "Local modular GraphRAG pipeline (Windows + LM Studio)"
requires-python = ">=3.10"
dependencies = []

[tool.ruff]
line-length = 100
```

---

## .env.example

````env
OPENAI_BASE_URL=http://localhost:1234/v1
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=llama-3.1-8b-instruct
EMBED_MODEL=BAAI/bge-small-en-v1.5
REPO_PATH=..\your-repo
KN_ROOT=.knowledge
OCR_ENABLED=false
```env
OPENAI_BASE_URL=http://localhost:1234/v1
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=llama-3.1-8b-instruct
EMBED_MODEL=bge-m3
REPO_PATH=..\\your-repo
KN_ROOT=.knowledge
````

---

## .knowledge/config/models.yml

````yaml
llm:
  provider: openai
  base_url: ${OPENAI_BASE_URL}
  api_key: ${OPENAI_API_KEY}
  model: ${OPENAI_MODEL}
  max_tokens: 2048
  temperature: 0.2

embeddings:
  kind: sentence-transformers
  name: ${EMBED_MODEL}
  device: auto
  normalize: true

stores:
  vector:
    kind: hnsw
    path: .knowledge/indexes/embeddings/hnsw.index
  graph:
    kind: networkx
  jobs:
    kind: sqlite
    path: .knowledge/queues/jobs.sqlite
```yaml
llm:
  provider: openai
  base_url: ${OPENAI_BASE_URL}
  api_key: ${OPENAI_API_KEY}
  model: ${OPENAI_MODEL}
  max_tokens: 2048
  temperature: 0.2

embeddings:
  kind: sentence-transformers
  name: BAAI/bge-m3
  device: auto
  normalize: true

stores:
  vector:
    kind: hnsw
    path: .knowledge/indexes/embeddings/hnsw.index
  graph:
    kind: networkx
  jobs:
    kind: sqlite
    path: .knowledge/queues/jobs.sqlite
````

---

## .knowledge/config/pipeline.yml

````yaml
watch:
  paths: ["${REPO_PATH}"]
  ignore: ["**/.git/**", "**/.knowledge/**"]

chunking:
  policies:
    default: { max_chars: 4000, overlap: 400 }
    code:    { max_chars: 2400, overlap: 200 }
    pdf:     { max_chars: 3500, overlap: 200 }

ocr:
  enabled: ${OCR_ENABLED}
  tesseract_cmd: ""

graph:
  entity_extraction: selective   # selective | all | rule_based
  relation_extraction: selective
  community_detection: louvain
  edge_conf_threshold: 0.55
  cross_domain_bridge_threshold: 0.75

retrieval:
  dense_k: 12
  bm25_k: 8
  graph_hops: 2
  rerank: false

attributes:
  plugins: ["summary-20w", "topic-tags", "pii-scan"]
  max_parallel: 4

export:
  default_budget_tokens: 600000
  strategy: hierarchy-first
  exclude_pii: false
  format: md
```yaml
watch:
  paths: ["${REPO_PATH}"]
  ignore: ["**/.git/**", "**/.knowledge/**"]

chunking:
  policies:
    default: { max_chars: 4000, overlap: 400 }
    code:    { max_chars: 2400, overlap: 200 }
    pdf:     { max_chars: 3500, overlap: 200 }

graph:
  entity_extraction: selective   # selective | all | rule_based
  relation_extraction: selective
  community_detection: louvain
  edge_conf_threshold: 0.55

retrieval:
  dense_k: 12
  bm25_k: 8
  graph_hops: 2
  rerank: true

attributes:
  plugins: ["summary-20w", "topic-tags"]
  max_parallel: 4

export:
  default_budget_tokens: 600000
  strategy: hierarchy-first
````

---

## bin/ingest\_build\_graph.py

```python
import argparse, pathlib
from kn.config import load_configs
from kn.file_index import initial_scan
from kn.chunking import chunk_repo
from kn.embeddings import embed_chunks
from kn.graph_nx import build_or_update_graph, summarize_communities

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--repo", required=True)
    ap.add_argument("--full", action="store_true")
    args = ap.parse_args()

    cfg = load_configs()
    repo = pathlib.Path(args.repo).resolve()

    print("[ingest] scanning repo…")
    docs = initial_scan(repo, cfg)
    print(f"[ingest] {len(docs)} docs found")

    print("[chunk] building chunks…")
    chunks = chunk_repo(docs, cfg)

    print("[embed] embedding chunks…")
    embed_chunks(chunks, cfg)

    print("[graph] updating graph…")
    build_or_update_graph(chunks, cfg)

    print("[summarize] community summaries…")
    summarize_communities(cfg)

    print("[done]")
```

---

## bin/watch\_daemon.py

```python
import argparse, time, pathlib
from kn.config import load_configs
from kn.file_index import watch_changes

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--repo", required=True)
    args = ap.parse_args()

    cfg = load_configs()
    repo = pathlib.Path(args.repo).resolve()
    print(f"[watch] monitoring {repo} … (Ctrl+C to quit)")
    watch_changes(repo, cfg)
```

---

## bin/enrich\_worker.py

```python
import argparse
from kn.jobs import run_enrichment_loop
from kn.config import load_configs

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--plugins", required=True, help="comma-separated plugin names")
    ap.add_argument("--watch", action="store_true")
    args = ap.parse_args()

    cfg = load_configs()
    plugins = [p.strip() for p in args.plugins.split(",") if p.strip()]
    run_enrichment_loop(plugins, cfg, watch=args.watch)
```

---

## bin/query\_rag.py

```python
import argparse
from kn.retrieval import answer_query
from kn.config import load_configs

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--q", required=True)
    ap.add_argument("--scope", default=None)
    ap.add_argument("--topk", type=int, default=12)
    args = ap.parse_args()

    cfg = load_configs()
    print(answer_query(args.q, cfg, scope=args.scope, topk=args.topk))
```

---

## bin/export\_monofile.py

```python
import argparse, pathlib
from kn.exporter import export_monofile
from kn.config import load_configs

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--q", required=True)
    ap.add_argument("--budget", type=int, default=None)
    ap.add_argument("--scope", default=None)
    ap.add_argument("--out", required=True)
    args = ap.parse_args()

    cfg = load_configs()
    out = pathlib.Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)
    export_monofile(args.q, cfg, out, budget=args.budget, scope=args.scope)
    print(f"[export] wrote {out}")
```

---

## kn/config.py

```python
import os, yaml, pathlib
from dotenv import load_dotenv

ROOT = pathlib.Path(os.getenv("KN_ROOT", ".knowledge"))
CONFIG_DIR = ROOT / "config"
MODELS_YML = CONFIG_DIR / "models.yml"
PIPELINE_YML = CONFIG_DIR / "pipeline.yml"

_defaults = {
    "llm": {
        "base_url": os.getenv("OPENAI_BASE_URL", "http://localhost:1234/v1"),
        "api_key": os.getenv("OPENAI_API_KEY", "lm-studio"),
        "model": os.getenv("OPENAI_MODEL", "llama-3.1-8b-instruct"),
        "max_tokens": 2048,
        "temperature": 0.2,
    },
    "embeddings": {"name": os.getenv("EMBED_MODEL", "BAAI/bge-m3"), "normalize": True, "device": "auto"},
    "stores": {
        "vector": {"kind": "hnsw", "path": str(ROOT/"indexes"/"embeddings"/"hnsw.index")},
        "graph": {"kind": "networkx"},
        "jobs": {"kind": "sqlite", "path": str(ROOT/"queues"/"jobs.sqlite")},
    },
}

_loaded = None

def load_configs():
    global _loaded
    if _loaded: return _loaded
    load_dotenv(override=True)
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    # load with fallbacks
    cfg = _defaults.copy()
    if MODELS_YML.exists():
        cfg = _merge(cfg, yaml.safe_load(MODELS_YML.read_text()))
    if PIPELINE_YML.exists():
        cfg = _merge(cfg, yaml.safe_load(PIPELINE_YML.read_text()))
    _loaded = cfg
    return cfg


def _merge(a, b):
    if not isinstance(b, dict): return a
    out = a.copy()
    for k, v in b.items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _merge(out[k], v)
        else:
            out[k] = v
    return out
```

---

## kn/hashing.py

```python
import hashlib

def content_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()


def short_hash(h: str, n=8) -> str:
    return h[:n]
```

---

## kn/file\_index.py

```python
import pathlib, time, json
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from .utils.io import read_text_safely, ensure_dirs
from .hashing import content_hash, short_hash

# In-memory manifest (persist minimal JSON if you like)
_manifest = {}

IGNORE_EXT = {".lock", ".tmp", ".log"}


def initial_scan(repo_path: pathlib.Path, cfg):
    docs = []
    for p in repo_path.rglob("*"):
        if p.is_dir():
            continue
        if any(part == ".git" for part in p.parts):
            continue
        if p.suffix.lower() in IGNORE_EXT:
            continue
        text = read_text_safely(p)
        if not text:
            continue
        ch = content_hash(text)
        doc_id = short_hash(ch)
        _manifest[str(p)] = {"doc_id": doc_id, "hash": ch, "mtime": p.stat().st_mtime}
        docs.append({"path": str(p), "doc_id": doc_id, "hash": ch, "text": text})
    return docs


class _Evt(FileSystemEventHandler):
    def __init__(self, repo, cfg):
        self.repo = repo
        self.cfg = cfg

    def on_any_event(self, event):
        p = pathlib.Path(event.src_path)
        if p.is_dir() or any(part == ".git" for part in p.parts):
            return
        if not p.exists():
            return
        text = read_text_safely(p)
        if text is None:
            return
        ch = content_hash(text)
        m = _manifest.get(str(p))
        if m and m["hash"] == ch:
            # move/rename or metadata change → update path mapping only
            m["mtime"] = p.stat().st_mtime
            return
        # changed/new file → enqueue lightweight downstream updates (left as TODO hook)
        _manifest[str(p)] = {"doc_id": short_hash(ch), "hash": ch, "mtime": p.stat().st_mtime}
        print(f"[watch] changed: {p}")
        # TODO: enqueue chunk/embed/graph tasks via jobs module


def watch_changes(repo_path: pathlib.Path, cfg):
    ensure_dirs(cfg)
    obs = Observer()
    h = _Evt(repo_path, cfg)
    obs.schedule(h, str(repo_path), recursive=True)
    obs.start()

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        obs.stop()
    obs.join()
```

---

## kn/chunking.py

```python
import pathlib, json, re
from .utils.io import ensure_dirs
from .hashing import short_hash

CHUNKS_DIR = ".knowledge/indexes/chunks"


def chunk_text(text: str, max_chars=4000, overlap=400):
    out = []
    i = 0
    while i < len(text):
        out.append(text[i:i+max_chars])
        i += max_chars - overlap
    return out


def detect_domain(path: pathlib.Path):
    # Domain = top-level folder under repo root (best-effort placeholder)
    parts = path.parts
    return parts[0] if parts else "root"


def chunk_repo(docs, cfg):
    ensure_dirs(cfg)
    policy = cfg.get("chunking", {}).get("policies", {}).get("default", {"max_chars": 4000, "overlap": 400})
    chunks = []
    outdir = pathlib.Path(CHUNKS_DIR)
    outdir.mkdir(parents=True, exist_ok=True)

    for d in docs:
        path = pathlib.Path(d["path"])
        ddomain = detect_domain(path.relative_to(path.anchor)) if path.is_absolute() else detect_domain(path)
        parts = chunk_text(d["text"], **policy)
        for idx, t in enumerate(parts):
            chunk_id = f"{d['doc_id']}-{idx:04d}"
            rec = {
                "doc_id": d["doc_id"],
                "chunk_id": chunk_id,
                "text": t,
                "meta": {
                    "path": str(path), "domain": ddomain, "mimetype": "text/plain"
                }
            }
            chunks.append(rec)
            (outdir / f"{chunk_id}.json").write_text(json.dumps(rec, ensure_ascii=False))
    return chunks
```

---

## kn/embeddings.py

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from .vector_hnsw import HNSWIndex
from .utils.io import ensure_dirs

_model = None
_index = None


def _get_model(name: str, device: str="auto"):
    global _model
    if _model is None:
        _model = SentenceTransformer(name, device=(None if device=="auto" else device))
    return _model


def embed_chunks(chunks, cfg):
    ensure_dirs(cfg)
    emc = cfg.get("embeddings", {})
    m = _get_model(emc.get("name", "BAAI/bge-m3"), emc.get("device", "auto"))
    texts = [c["text"] for c in chunks]
    vecs = m.encode(texts, normalize_embeddings=emc.get("normalize", True), show_progress_bar=True)
    idx = HNSWIndex.open(cfg)
    idx.add([c["chunk_id"] for c in chunks], np.asarray(vecs, dtype=np.float32))
    idx.save()
```

---

## kn/vector\_hnsw\.py

```python
import hnswlib, numpy as np, pathlib, json

class HNSWIndex:
    def __init__(self, path: pathlib.Path, dim=1024, space='cosine'):
        self.path = path
        self.meta_path = path.with_suffix('.meta.json')
        self.dim = dim
        self.space = space
        self.index = hnswlib.Index(space=space, dim=dim)
        self.inited = False
        self.ids = []

    @classmethod
    def open(cls, cfg):
        path = pathlib.Path(cfg["stores"]["vector"]["path"])
        meta = path.with_suffix('.meta.json')
        if path.exists() and meta.exists():
            m = json.loads(meta.read_text())
            dim = m.get("dim", 1024)
            obj = cls(path, dim=dim)
            obj.index.load_index(str(path))
            obj.index.set_ef(128)
            obj.inited = True
            obj.ids = m.get("ids", [])
            return obj
        # default new
        return cls(path)

    def _ensure_init(self, total=10000):
        if not self.inited:
            self.index.init_index(max_elements=total, ef_construction=200, M=16)
            self.index.set_ef(128)
            self.inited = True

    def add(self, keys, vecs: np.ndarray):
        self._ensure_init(max(10000, len(self.ids) + len(keys) + 1000))
        # map external string ids to integer labels
        labels = np.arange(len(self.ids), len(self.ids)+len(keys))
        self.index.add_items(vecs, labels)
        self.ids.extend(list(keys))

    def save(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.index.save_index(str(self.path))
        self.meta_path.write_text(json.dumps({"dim": self.dim, "ids": self.ids}))

    def search(self, vecs: np.ndarray, k=10):
        labels, dists = self.index.knn_query(vecs, k=k)
        # map back to external ids
        inv = self.ids
        mapped = [[inv[i] for i in row] for row in labels]
        return mapped, dists
```

---

## kn/graph\_nx.py

```python
import pathlib, json, networkx as nx
from collections import defaultdict
from .utils.io import ensure_dirs

GRAPH_PATH = pathlib.Path('.knowledge/indexes/graph/graph.jsonl')
COMM_PATH = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')

G = nx.Graph()

# Very lightweight placeholder extraction (extend with LLM in utils/llm_client)
def naive_entities(text: str):
    # collect simple tokens (CamelCase, path-like, #tags) — placeholder
    import re
    ents = set(re.findall(r"[A-Z][a-zA-Z0-9_]+|\b[A-Za-z0-9_]+\.\w+|#\w+", text))
    return list(ents)[:50]


def build_or_update_graph(chunks, cfg):
    ensure_dirs(cfg)
    # add nodes & co-occurrence edges (same chunk → weak link)
    for c in chunks:
        ents = naive_entities(c["text"])  # TODO swap to LLM-assisted if configured
        for e in ents:
            if not G.has_node(e):
                G.add_node(e, kind='entity')
        for i in range(len(ents)):
            for j in range(i+1, len(ents)):
                u, v = ents[i], ents[j]
                w = G[u][v]["weight"]+1 if G.has_edge(u,v) else 1
                G.add_edge(u, v, weight=w)
    # persist
    GRAPH_PATH.parent.mkdir(parents=True, exist_ok=True)
    with GRAPH_PATH.open('w', encoding='utf-8') as f:
        for u, v, d in G.edges(data=True):
            f.write(json.dumps({"src": u, "dst": v, "weight": d.get("weight",1)})+"\n")


def summarize_communities(cfg):
    # Simple connected components as communities (swap to Louvain later)
    comms = []
    for idx, comp in enumerate(nx.connected_components(G)):
        nodes = list(comp)
        # naive summary string (placeholder); you can call LLM here
        summary = ", ".join(nodes[:10])
        comms.append({"id": f"C{idx}", "size": len(nodes), "summary": summary})
    COMM_PATH.parent.mkdir(parents=True, exist_ok=True)
    with COMM_PATH.open('w', encoding='utf-8') as f:
        for c in comms:
            f.write(json.dumps(c)+"\n")
```

---

## kn/retrieval.py

```python
import json, pathlib
import numpy as np
from sentence_transformers import SentenceTransformer
from .vector_hnsw import HNSWIndex

# Simple dense-only retrieval baseline + community preface

COMM_PATH = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')

_embed_model = None

def _get_model(name):
    global _embed_model
    if _embed_model is None:
        _embed_model = SentenceTransformer(name)
    return _embed_model


def _iter_chunks():
    cdir = pathlib.Path('.knowledge/indexes/chunks')
    for p in cdir.glob('*.json'):
        yield json.loads(p.read_text(encoding='utf-8'))


def answer_query(q: str, cfg, scope=None, topk=12):
    emc = cfg.get('embeddings', {})
    m = _get_model(emc.get('name','BAAI/bge-m3'))
    qv = m.encode([q], normalize_embeddings=emc.get('normalize', True))
    idx = HNSWIndex.open(cfg)
    ids, dists = idx.search(np.asarray(qv, dtype=np.float32), k=topk)
    idset = set(ids[0])

    # Gather matched chunks
    hits = []
    for rec in _iter_chunks():
        if rec['chunk_id'] in idset:
            hits.append(rec)
    # prepend community summaries (lightweight)
    pref = []
    if COMM_PATH.exists():
        for line in COMM_PATH.read_text(encoding='utf-8').splitlines():
            c = json.loads(line)
            pref.append(f"[Community {c['id']} size={c['size']}] {c['summary']}")
    text = "\n\n".join(pref[:3]) + "\n\n" + "\n\n".join([h['text'] for h in hits])
    return text
```

---

## kn/exporter.py

```python
import json, pathlib
from .retrieval import answer_query


def export_monofile(q: str, cfg, out: pathlib.Path, budget: int|None=None, scope=None):
    body = answer_query(q, cfg, scope=scope, topk=64)
    # naïve token budget by char length (~ 4 chars/token heuristic)
    if budget:
        body = body[: int(budget*4)]
    out.write_text(body, encoding='utf-8')
```

---

## kn/jobs.py

```python
import time, subprocess, json, pathlib

QUEUE = pathlib.Path('.knowledge/queues/pending.jsonl')
QUEUE.parent.mkdir(parents=True, exist_ok=True)


def enqueue(doc_id: str, plugin: str):
    with QUEUE.open('a', encoding='utf-8') as f:
        f.write(json.dumps({"doc_id": doc_id, "plugin": plugin})+"\n")


def run_enrichment_loop(plugins, cfg, watch=False):
    # very simple loop that invokes plugins with stdin jsonl of docs (placeholder)
    while True:
        if not QUEUE.exists() or QUEUE.stat().st_size == 0:
            if not watch: break
            time.sleep(2); continue
        lines = QUEUE.read_text(encoding='utf-8').splitlines()
        QUEUE.write_text('', encoding='utf-8')
        jobs = [json.loads(l) for l in lines]
        # group by plugin
        byp = {}
        for j in jobs:
            if j['plugin'] not in plugins: continue
            byp.setdefault(j['plugin'], []).append(j)
        for plugin, items in byp.items():
            _run_plugin(plugin, items)
        if not watch:
            break


def _run_plugin(plugin, items):
    pypath = pathlib.Path(f'plugins/attributes/{plugin}.py')
    if not pypath.exists():
        print(f"[enrich] plugin not found: {plugin}")
        return
    # stream jsonl to plugin stdin
    inp = "\n".join([json.dumps(_load_doc(i['doc_id'])) for i in items])
    proc = subprocess.Popen(["python", str(pypath)], stdin=subprocess.PIPE, stdout=subprocess.PIPE, text=True)
    out, _ = proc.communicate(inp)
    print(f"[enrich] {plugin}: {len(items)} docs processed")


def _load_doc(doc_id):
    # minimal: pick first chunk for that doc
    cdir = pathlib.Path('.knowledge/indexes/chunks')
    for p in sorted(cdir.glob(f'{doc_id}-*.json')):
        return json.loads(p.read_text(encoding='utf-8'))
    return {"doc_id": doc_id, "text": ""}
```

---

## kn/utils/io.py

```python
import pathlib, chardet

ROOT = pathlib.Path('.knowledge')


def ensure_dirs(cfg):
    (ROOT/"indexes"/"chunks").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"embeddings").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"graph").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"summaries").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"attributes").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"manifests").mkdir(parents=True, exist_ok=True)
    (ROOT/"queues").mkdir(parents=True, exist_ok=True)
    (ROOT/"exports"/"dumps").mkdir(parents=True, exist_ok=True)


def read_text_safely(path: pathlib.Path):
    try:
        data = path.read_bytes()
        enc = chardet.detect(data).get('encoding') or 'utf-8'
        return data.decode(enc, errors='ignore')
    except Exception:
        return None
```

---

## kn/utils/llm\_client.py

```python
import os, requests

BASE = os.getenv('OPENAI_BASE_URL', 'http://localhost:1234/v1')
KEY  = os.getenv('OPENAI_API_KEY', 'lm-studio')
MODEL= os.getenv('OPENAI_MODEL', 'llama-3.1-8b-instruct')


def chat(prompt: str, max_tokens=512, temperature=0.2):
    url = f"{BASE}/chat/completions"
    headers = {'Authorization': f"Bearer {KEY}", 'Content-Type':'application/json'}
    payload = {
        'model': MODEL,
        'messages': [{'role':'user','content':prompt}],
        'max_tokens': max_tokens,
        'temperature': temperature,
    }
    r = requests.post(url, headers=headers, json=payload, timeout=120)
    r.raise_for_status()
    return r.json()['choices'][0]['message']['content']
```

---

## plugins/attributes/summary\_20w\.py

```python
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summary-20w')
OUTDIR.mkdir(parents=True, exist_ok=True)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','')
    if not text: continue
    prompt = f"Summarize in ~20 words, terse, factual.\n\n{text}"
    out = chat(prompt, max_tokens=100, temperature=0.1)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        "doc_id": job['doc_id'],
        "attribute": "summary-20w",
        "value": out.strip(),
        "confidence": 0.8
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({"status":"ok","doc_id":job['doc_id']}))
```

---

## plugins/attributes/topic\_tags.py

```python
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/topic-tags')
OUTDIR.mkdir(parents=True, exist_ok=True)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','')
    if not text: continue
    prompt = (
        "Extract 3-7 topical tags (comma-separated, lowercase, no spaces, use-hyphens).\n"
        "Prefer domain-relevant terms.\n\n"
        f"TEXT:\n{text}\n"
    )
    out = chat(prompt, max_tokens=64, temperature=0.2)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        "doc_id": job['doc_id'], "attribute": "topic-tags", "value": out.strip(), "confidence": 0.7
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({"status":"ok","doc_id":job['doc_id']}))
```

---

## Notes on LM Studio

* Enable Local Server in LM Studio (`http://localhost:1234/v1`).
* Download a compatible instruct model (e.g., **Llama-3.1-8B Instruct** quantized) and ensure it’s serving.
* SentenceTransformers handles embeddings locally (GPU if PyTorch CUDA available). If you later want to use LM Studio for embeddings, swap `embeddings.py` to call `/embeddings`.

## Operations playbook

* **Continuous:** `watch_daemon.py` monitors repos and enqueues enrichment.
* **Periodic:** schedule `ingest_build_graph.py --repo <path> --full` nightly to refresh embeddings & rebuild index (cleans stale vectors).
* **Ad hoc:** run `export_monofile.py` to produce big `.md` dumps with a token budget for external chats.

## Open questions (status)

* OCR for all docs? **No** — only for images/PDFs lacking text and only when `OCR_ENABLED=true`.
* Cross‑linking across repos? **Yes, optional** via `cross_domain_bridge_threshold` (default 0.75).
* Quality add‑ons now? **BM25 enabled**; lightweight reranker deferred.
* Queue backend? **SQLite** now (simple); Redis is an easy upgrade later.
* Multi‑pass attributes? **Supported**; keep both “cheap” and “full” results with pass metadata.
* Hashing scope? **Doc‑level SHA256 + per‑chunk IDs now**; roadmap: per‑chunk hashes & near‑duplicate (MinHash/SimHash) for large corpora.

## Known limitations (MVP)

* hnswlib deletions not wired; recommend nightly rebuild.
* Graph entity extraction is naive; LLM‑assisted extraction and relation typing are planned upgrades.
* Exporter does not yet include per‑chunk citations/line ranges; future enhancement.

## Future upgrades

* **Vector DB:** Qdrant/FAISS‑GPU; **Graph DB:** Neo4j/Memgraph; **Runtime:** vLLM with tensor/kv cache.
* **Graph‑aware retrieval:** 1–2 hop neighbor packing, graph‑ranked reassembly.
* **Re‑ranker:** tiny cross‑encoder for final ordering.
* **PII:** enhanced detector (LLM‑assist + rules per region).

---

### Addendum: Implementation decisions and operations log (Windows + LM Studio)

This addendum chronicles the practical design choices we made while hardening the Windows‑first GraphRAG starter into a dashboard‑driven pipeline.

#### Runtime and platform
- Default runtime: LM Studio (OpenAI‑compatible) on Windows 10 Pro; network base defaults to `http://127.0.0.1:12345/v1` with per‑plugin overrides.
- vLLM considered for higher throughput; deferred due to Windows constraints (uvloop unsupported; HF weights required, not GGUF). Dashboard retains LM Studio defaults.
- UTF‑8 and Windows console: advise `-X utf8`, `PYTHONIOENCODING`, and `chcp 65001` when needed.

#### Data model and stores
- Sidecar storage under `.knowledge/` (portable): indexes, graph, summaries, attributes, exports, queues, cache.
- Vector index: hnswlib on non‑Windows; NumPy brute‑force fallback on Windows, persisted as `.npy` + `.meta.json`.
- Graph: NetworkX with Louvain when available; communities saved to JSONL.

#### Config and environment
- `.env` and YAML configs merged at runtime with `${VAR}` expansion. Early `load_dotenv(override=True)` to ensure env is respected (fixes `${EMBED_MODEL}` issues).
- `.knowledge/config/models.yml` gains `plugins:` block for per‑plugin LLM defaults:
  - `llm`: `{ base_url, api_key, model, timeout }`
  - `process_timeout`: per‑plugin subprocess wall‑clock limit.

#### Ingestion, chunking, OCR/PDF
- Multi‑repo ingest; domains derived from top‑level folders (e.g., `G:\LOKI\papers`).
- Chunking policies tunable per type; input token pressure reduced by skeleton‑first flow.
- PDF parsing via PyMuPDF (`fitz`) for text extraction on Windows.

#### Attributes and enrichment
- Attribute plugins are standalone scripts (stdin JSONL → stdout), outputs in `.knowledge/indexes/attributes/<plugin>/`.
- Core set: `summary-20w`, `topic-tags`, `pii-scan` (regex), plus `glossary`, `requirements`, `todo-items`, `faq-pairs`.
- New performance plugins:
  - `doc-skeleton`: outline + ~100w + keyphrases snapshot per doc (token‑cheap intermediate).
  - `multi-basic`: single LLM call emits multiple attributes (summary‑short/medium/outline, keyphrases, risk‑scan).
- Existing plugins updated to prefer skeleton content when present, reducing input tokens.

#### Queue, worker, robustness
- Job queue migrated to SQLite with columns: `status`, `retries`, `completed_at`, `last_error`.
- Dequeue ordering by `(retries asc, id asc)` to avoid starvation; “any pending plugin” worker mode added.
- Error handling: timeouts/errors requeue (or mark failed for fatal issues like missing plugin).
- Global in‑flight concurrency cap across workers via SQLite counters; configurable in dashboard.

#### Dashboard (Flask + static HTML/JS)
- `/api/status` with docs, coverage, queue, LLM health, workers.
- Planner: enqueue plugins (single or list), optional JSON payload for per‑run overrides.
- Ingest: start ingest for arbitrary repo path (multi‑domain).
- Worker control: start/stop multiple workers, list active workers, set batch and max‑inflight.
- Queue panel: filter/list items, clear queue modes (non‑done/pending/all, reset running → pending).
- Coverage panel: summary rows with dropdowns per attribute → per‑doc previews and full JSON.
- Documents panel: table with per‑doc dropdowns → per‑attribute full JSON.
- Plugin defaults panel: Load/Save `plugins:` map to `.knowledge/config/models.yml` (per‑plugin model/timeout).
- LLM cache panel: show/clear prompt‑level response cache.

#### Retrieval and export
- Hybrid retrieval (dense + BM25), community preface; monofile export with token budgeting and PII exclusion.
- Graph export (GEXF/GraphML) for external tools; HTML report with D3 preview, communities section, and offline D3 fallback.

#### Performance choices
- Accept LM Studio statelessness; avoid repeated full‑doc prompts by:
  - One‑shot macro plugin (`multi-basic`) per doc.
  - Skeleton‑first flow and downstream plugins consuming the skeleton.
  - Prompt‑level response caching keyed by `(model|max_tokens|temperature|prompt)`.
  - Concurrency throttling (max‑inflight) to reduce timeouts and stabilize throughput.
  - Reasonable per‑plugin HTTP timeouts and per‑process wall‑time limits (e.g., skeleton/multi‑basic larger than others).
- Keep individual attributes runnable; treat `multi-basic` as a macro for convenience, not a replacement.

#### Known limits and next steps
- LM Studio parallelism is limited by model/runtime; increase throughput by reducing input tokens (skeleton), batching tasks, or migrating to vLLM (Linux) with HF weights and paged attention.
- Optional: small reranker; richer entity/relationship extraction; better caching/incremental invalidation; dedicated model routing per attribute class via UI.


./DESIGN_additions.md
Could you compare CorrectiveRAG to our GraphRAG design and give a high-level analysis on how to integrate the innovations from that into ours?

---

Skip to content
Our new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.
logo


Search
 
 GitHub
0.6.6
17.5k
3k
Copy page
▾
Get started
Guides
Reference
Examples
Additional resources
Table of contents
Setup
Create Index
LLMs
Web Search Tool
Create Graph
Define Graph State
Compile Graph
Use the graph
Corrective RAG (CRAG)¶
Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents.

In the paper here, a few steps are taken:

If at least one document exceeds the threshold for relevance, then it proceeds to generation
Before generation, it performs knowledge refinement
This partitions the document into "knowledge strips"
It grades each strip, and filters our irrelevant ones
If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource
It will use web search to supplement retrieval
We will implement some of these ideas from scratch using LangGraph:

Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired.
If any documents are irrelevant, let's opt to supplement retrieval with web search.
We'll use Tavily Search for web search.
Let's use query re-writing to optimize the query for web search.
Screenshot 2024-04-01 at 9.28.30 AM.png

Setup¶
First, let's download our required packages and set our API keys


! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python

import getpass
import os


def _set_env(key: str):
    if key not in os.environ:
        os.environ[key] = getpass.getpass(f"{key}:")


_set_env("OPENAI_API_KEY")
_set_env("TAVILY_API_KEY")
Set up LangSmith for LangGraph development

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here.

Create Index¶
Let's index 3 blog posts.

API Reference: RecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings


from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=250, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)

# Add to vectorDB
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name="rag-chroma",
    embedding=OpenAIEmbeddings(),
)
retriever = vectorstore.as_retriever()
LLMs¶
Using Pydantic with LangChain

This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.

API Reference: ChatPromptTemplate | ChatOpenAI


### Retrieval Grader

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from pydantic import BaseModel, Field


# Data model
class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""

    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )


# LLM with function call
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

# Prompt
system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""
grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
    ]
)

retrieval_grader = grade_prompt | structured_llm_grader
question = "agent memory"
docs = retriever.invoke(question)
doc_txt = docs[1].page_content
print(retrieval_grader.invoke({"question": question, "document": doc_txt}))

binary_score='yes'
API Reference: StrOutputParser


### Generate

from langchain import hub
from langchain_core.output_parsers import StrOutputParser

# Prompt
prompt = hub.pull("rlm/rag-prompt")

# LLM
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)


# Post-processing
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# Chain
rag_chain = prompt | llm | StrOutputParser()

# Run
generation = rag_chain.invoke({"context": docs, "question": question})
print(generation)

The design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave conditioned on past experience. Memory stream is a long-term memory module that records a comprehensive list of agents' experience in natural language. Short-term memory is utilized for in-context learning, while long-term memory allows agents to retain and recall information over extended periods.

### Question Re-writer

# LLM
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)

# Prompt
system = """You a question re-writer that converts an input question to a better version that is optimized \n 
     for web search. Look at the input and try to reason about the underlying semantic intent / meaning."""
re_write_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        (
            "human",
            "Here is the initial question: \n\n {question} \n Formulate an improved question.",
        ),
    ]
)

question_rewriter = re_write_prompt | llm | StrOutputParser()
question_rewriter.invoke({"question": question})

'What is the role of memory in artificial intelligence agents?'
Web Search Tool¶
API Reference: TavilySearchResults


### Search

from langchain_community.tools.tavily_search import TavilySearchResults

web_search_tool = TavilySearchResults(k=3)
Create Graph¶
Now let's create our graph that will use CRAG

Define Graph State¶

from typing import List

from typing_extensions import TypedDict


class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        web_search: whether to add search
        documents: list of documents
    """

    question: str
    generation: str
    web_search: str
    documents: List[str]
API Reference: Document


from langchain.schema import Document


def retrieve(state):
    """
    Retrieve documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]

    # Retrieval
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question}


def generate(state):
    """
    Generate answer

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]

    # RAG generation
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"documents": documents, "question": question, "generation": generation}


def grade_documents(state):
    """
    Determines whether the retrieved documents are relevant to the question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with only filtered relevant documents
    """

    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    # Score each doc
    filtered_docs = []
    web_search = "No"
    for d in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": d.page_content}
        )
        grade = score.binary_score
        if grade == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            web_search = "Yes"
            continue
    return {"documents": filtered_docs, "question": question, "web_search": web_search}


def transform_query(state):
    """
    Transform the query to produce a better question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates question key with a re-phrased question
    """

    print("---TRANSFORM QUERY---")
    question = state["question"]
    documents = state["documents"]

    # Re-write question
    better_question = question_rewriter.invoke({"question": question})
    return {"documents": documents, "question": better_question}


def web_search(state):
    """
    Web search based on the re-phrased question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with appended web results
    """

    print("---WEB SEARCH---")
    question = state["question"]
    documents = state["documents"]

    # Web search
    docs = web_search_tool.invoke({"query": question})
    web_results = "\n".join([d["content"] for d in docs])
    web_results = Document(page_content=web_results)
    documents.append(web_results)

    return {"documents": documents, "question": question}


### Edges


def decide_to_generate(state):
    """
    Determines whether to generate an answer, or re-generate a question.

    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """

    print("---ASSESS GRADED DOCUMENTS---")
    state["question"]
    web_search = state["web_search"]
    state["documents"]

    if web_search == "Yes":
        # All documents have been filtered check_relevance
        # We will re-generate a new query
        print(
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---"
        )
        return "transform_query"
    else:
        # We have relevant documents, so generate answer
        print("---DECISION: GENERATE---")
        return "generate"
Compile Graph¶
The just follows the flow we outlined in the figure above.

API Reference: END | StateGraph | START


from langgraph.graph import END, StateGraph, START

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("retrieve", retrieve)  # retrieve
workflow.add_node("grade_documents", grade_documents)  # grade documents
workflow.add_node("generate", generate)  # generate
workflow.add_node("transform_query", transform_query)  # transform_query
workflow.add_node("web_search_node", web_search)  # web search

# Build graph
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    },
)
workflow.add_edge("transform_query", "web_search_node")
workflow.add_edge("web_search_node", "generate")
workflow.add_edge("generate", END)

# Compile
app = workflow.compile()
Use the graph¶

from pprint import pprint

# Run
inputs = {"question": "What are the types of agent memory?"}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value["generation"])

---RETRIEVE---
"Node 'retrieve':"
'\n---\n'
---CHECK DOCUMENT RELEVANCE TO QUESTION---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
"Node 'grade_documents':"
'\n---\n'
---ASSESS GRADED DOCUMENTS---
---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---
---TRANSFORM QUERY---
"Node 'transform_query':"
'\n---\n'
---WEB SEARCH---
"Node 'web_search_node':"
'\n---\n'
---GENERATE---
"Node 'generate':"
'\n---\n'
"Node '__end__':"
'\n---\n'
('Agents possess short-term memory, which is utilized for in-context learning, '
 'and long-term memory, allowing them to retain and recall vast amounts of '
 'information over extended periods. Some experts also classify working memory '
 'as a distinct type, although it can be considered a part of short-term '
 'memory in many cases.')

from pprint import pprint

# Run
inputs = {"question": "How does the AlphaCodium paper work?"}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value["generation"])

---RETRIEVE---
"Node 'retrieve':"
'\n---\n'
---CHECK DOCUMENT RELEVANCE TO QUESTION---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
"Node 'grade_documents':"
'\n---\n'
---ASSESS GRADED DOCUMENTS---
---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---
---TRANSFORM QUERY---
"Node 'transform_query':"
'\n---\n'
---WEB SEARCH---
"Node 'web_search_node':"
'\n---\n'
---GENERATE---
"Node 'generate':"
'\n---\n'
"Node '__end__':"
'\n---\n'
('The AlphaCodium paper functions by proposing a code-oriented iterative flow '
 'that involves repeatedly running and fixing generated code against '
 'input-output tests. Its key mechanisms include generating additional data '
 'like problem reflection and test reasoning to aid the iterative process, as '
 'well as enriching the code generation process. AlphaCodium aims to improve '
 'the performance of Large Language Models on code problems by following a '
 'test-based, multi-stage approach.')
LangSmith Traces -
https://smith.langchain.com/public/f6b1716c-e842-4282-9112-1026b93e246b/r

https://smith.langchain.com/public/497c8ed9-d9e2-429e-8ada-e64de3ec26c9/r

 Back to top
Copyright © 2025 LangChain, Inc | Consent Preferences
Made with Material for MkDocs
ChatGPT said:
Thought for 1m 16s

Here’s a crisp compare-and-integrate pass: what CorrectiveRAG (CRAG) adds on top of “plain” GraphRAG, and exactly how to fold those ideas into our Windows/LM-Studio starter.

What CRAG is (in one paragraph)

CRAG adds a lightweight retrieval evaluator that self-grades the initial context, then routes the query through one of three actions: Correct (keep and refine), Incorrect (discard and expand with external search), or Ambiguous (combine both). On “Correct,” it decomposes documents into fine-grained “knowledge strips,” grades strips, filters, and recomposes. On “Incorrect,” it rewrites the query and brings in web search results (then also strips/filters). Ablations show each piece helps; a small T5-based evaluator outperformed ChatGPT for relevance grading, and the method boosts both vanilla RAG and Self-RAG across short/long-form tasks with modest overhead. 

2401.15884v3

GraphRAG vs. CRAG (high-level)
Dimension	Our GraphRAG starter	CRAG additions (what we should borrow)
Global structure	Entity co-occurrence graph + Louvain communities; hybrid dense+BM25; community summaries	Not graph-centric; per-query quality control gate on retrieved chunks
Quality control	None yet (just hybrid merge)	Self-grading evaluator → route to Correct / Incorrect / Ambiguous
Granularity	Chunk-level	Strip-level refinement (sentence/mini-span filtering) before generation
Fallbacks	Expand k; walk graph neighbors (planned)	Web search (with query rewrite), then same refinement
Overhead	Low	Small extra pass (grader + strip scoring); paper shows modest cost for clear gains 

2401.15884v3


Robustness	Sensitive to bad chunks	Designed to be robust to bad retrieval; ablations confirm each action helps 

2401.15884v3

Integration plan for our starter (minimal moving parts)
1) Add a local Relevance Grader (no fine-tuning required to start)

Use a fast cross-encoder as a plug-in scorer (local, GPU): e.g., cross-encoder/ms-marco-MiniLM-L-6-v2 (very fast) or BAAI/bge-reranker-base (higher quality). Both run via sentence-transformers we already ship.

Score each (question, chunk_text) → scalar in [0,1] (normalize if model outputs logits).

Config (new, in .knowledge/config/models.yml):

crag:
  enabled: true
  grader_model: cross-encoder/ms-marco-MiniLM-L-6-v2
  upper: 0.60      # ≥ upper ⇒ Correct
  lower: 0.30      # < lower ⇒ Incorrect
  topk_grade: 24   # how many retrieved chunks to grade


Why cross-encoder first? The CRAG paper trains a small T5 evaluator and finds it beats prompting LLMs for grading; a cross-encoder is the closest off-the-shelf analogue that’s local and fast. 

2401.15884v3

2) Implement action policy (Correct / Incorrect / Ambiguous)

After our hybrid retrieval returns N chunks, grade them:

Correct if any grade ≥ upper → keep only graded-relevant chunks and go to refinement.

Incorrect if all < lower → skip internal chunks and go to fallback retrieval (next section).

Ambiguous otherwise → keep best few internal chunks and run fallback; merge.

3) Add strip-level refinement (the “knowledge strips”)

For chunks that survive:

Split each chunk into sentences (or ~150–300 char spans).

Grade each strip with the same grader; keep top m per chunk (e.g., 2–3).

Reassemble in source order.

This mirrors CRAG’s decompose→filter→recompose, which drove gains in ablations. 

2401.15884v3

Config knobs (new in pipeline.yml):

crag_refine:
  strip_chars: 300
  max_strips_per_chunk: 3

4) Add fallback retrieval without breaking “offline-first”

Default (offline) fallback when Incorrect (or Ambiguous):

Expand scope: raise dense k and relax BM25; include graph-neighbor chunks (1–2 hops from any entity in the top few chunks, if any).

Query rewrite (local LLM via LM Studio) to enrich keywords; then re-retrieve internally.

Optional (opt-in) web search module (Tavily/Bing/Serper) if you ever add API keys. Paper prefers authoritative pages (e.g., Wikipedia) then applies the same strip-refine; we’ll mirror that when online is enabled. 

2401.15884v3

Config:

crag_fallback:
  use_web: false         # stays offline by default
  expand_dense_k: 48
  graph_hops: 2
  rewrite_with_llm: true

5) Wire it into our retrieval path (surgical changes)

New module kn/crag.py:

grade_chunks(question, chunks) -> List[(chunk, score)]

decide_action(scores, upper, lower) -> {"Correct"|"Incorrect"|"Ambiguous"}

refine_strips(question, chunks) -> refined_text

fallback_retrieve(question, cfg) -> chunks (expand k, graph hops; optional web)

Modify kn/retrieval.py:answer_query(...):

Do current hybrid retrieval (dense+BM25).

If crag.enabled, call grader → action.

Build final context:

Correct → refine_strips(internal_hits)

Incorrect → refine_strips(fallback_hits)

Ambiguous → refine_strips(internal_top) + refine_strips(fallback_hits)

Prepend our community snippets (unchanged), then hand off to generator or exporter.

Cache: write per-(query_hash, chunk_id) grades to .knowledge/indexes/grades/ to avoid rescoring on repeated queries.

6) Export & safety

Exporter can note the CRAG path taken:

e.g., CRAG: Correct (6 strips kept from 3 chunks); or CRAG: Ambiguous (internal+fallback).

If optional web fallback is enabled, pass those strips through the existing PII plugin before inclusion (paper calls out reliability concerns for web content; we’ll add our own guardrails). 

2401.15884v3

Why this is worth it (evidence from CRAG)

Self-grading matters: Their small evaluator correctly judged retrieval quality far better than ChatGPT in their test (PopQA) and drove consistent gains. 

2401.15884v3

All three actions help: Removing Correct / Incorrect / Ambiguous each hurt performance; the “Ambiguous” middle path mitigates evaluator uncertainty. 

2401.15884v3

Strip-level refinement helps: Dropping refinement, rewrite, or selection each reduced accuracy—so do the decomposition/filter/recompose step, not just bigger k. 

2401.15884v3

Overhead is modest relative to gains; CRAG keeps compute close to RAG in their measurements. 

2401.15884v3

Defaults I’d set for you now

Grader model: cross-encoder/ms-marco-MiniLM-L-6-v2 (fastest good baseline; upgrade to BAAI/bge-reranker-base if you want more headroom).

Thresholds: upper=0.60, lower=0.30.

topk to grade: 24 (grade fewer than you retrieve; still plenty to decide and filter).

Strip size ~300 chars, keep 2–3 per chunk.

Fallback: expand dense k to 48, include 1–2 graph hops, rewrite query via local LLM (LM Studio), keep use_web=false.

Where CRAG + GraphRAG complement each other

CRAG gives per-query robustness, GraphRAG gives global structure. Combining them: when Incorrect, instead of jumping straight to the web, we first walk our graph (neighbors & communities) and only go online if you’ve opted in. That keeps us fast, offline, and aligned with your multi-repo setup—while inheriting CRAG’s resilience when initial hits are noisy.

If you want, I can produce the small code patch list (new kn/crag.py, few edits in kn/retrieval.py, and the two config blocks) next.

Sources
You said:
2508.11386v1.pdf
PDF
Could you also incorporate the lessons learned by this paper, if they're replicable to our environment (or explain why not, if not)
ChatGPT said:
Thought for 1m 9s

Absolutely—here’s how we’d fold the key, replicable ideas from “Retrieval-Augmented Reasoning with Lean Language Models” into our GraphRAG starter, plus what isn’t a great fit (and why). I’ll keep this focused on changes you can actually run on your Windows + LM Studio setup.

What to adopt (and where it lands in our repo)
1) Summary-first indexing (big win, easy to add)

Lesson: They index summarized versions of documents (≈85% shorter) and see higher p@k than indexing full pages; then recover full content only when needed. This shrinks context, speeds everything, and improves first-hit relevance. 

2508.11386v1

How we add it

New attribute plugin: plugins/attributes/doc-summary.py that creates a compact per-doc summary (static, offline) into .knowledge/indexes/summaries/doc/.

New “summary index”: build a second HNSW index from summaries alongside the full-chunk index.

Retrieval path: query summary-index → select top docs → rehydrate by pulling either (a) the doc’s best chunks from the full index or (b) the entire doc if you prefer (see next item).

Config (new):

summarize_index:
  enabled: true
  target_ratio: 0.15     # ~85% reduction
  model: ${OPENAI_MODEL} # via LM Studio locally
  use_for_retrieval: true
  rehydrate: full_doc    # or: best_chunks


Why it fits your constraints: offline, uses your local LLM, keeps original files untouched.

2) “Promote to full document” (source rehydration)

Lesson: If any chunk from a doc hits, they feed the whole doc to the reasoner so local context isn’t lost. In our pipeline that’s a controlled “rehydration” step. 

2508.11386v1

How we add it

After we pick top-k summaries, rehydrate either to (a) entire sources or (b) N nearest chunks around those sources.

Export path also respects this, so mono-file includes full supporting text (optionally trimmed).

3) Retrieval as a “tool” (don’t always retrieve)

Lesson: Treat retrieval as callable: the model decides when to retrieve vs. answer directly; also rewrite the query when needed. This matters in chat flows and follow-ups. 

2508.11386v1

How we add it (without fancy tool-calling)

Keep it offline and model-agnostic: add a small “should_retrieve” gate (cross-encoder or logistic classifier over the last user turn + short history). If score < τ, skip retrieval.

If documents look weak (see §5 CRAG below), rewrite the query via local LLM (brief keyword-boosted rewrite) then re-retrieve.

4) CRAG-style self-grading + strip filtering (pairs perfectly)

Lesson: Their results underscore that quality control before generation and fine-grain filtering lift accuracy with small overhead. Combine with our CRAG plan:

Grade retrieved chunks with a local cross-encoder (e.g., ms-marco-MiniLM-L-6-v2) → route to Correct / Incorrect / Ambiguous.

Strip-level refinement: split surviving chunks into 150–300-char strips, grade again, keep only the best few per chunk, and recompose the context. (This mirrors the paper’s “knowledge refinement” benefit.) 

2508.11386v1

Config we’ll ship

crag:
  enabled: true
  grader_model: cross-encoder/ms-marco-MiniLM-L-6-v2
  upper: 0.60
  lower: 0.30
  topk_grade: 24
crag_refine:
  strip_chars: 300
  max_strips_per_chunk: 3
crag_fallback:
  use_web: false           # stays offline
  expand_dense_k: 48
  graph_hops: 2
  rewrite_with_llm: true

5) Two-tier retrieval budget (pick k smartly)

Lesson: They explicitly choose k by measuring p@k trade-offs; retrieval of summaries let them run k=5 with manageable context (and ~0.76 upper-bound hit-rate in their domain). 

2508.11386v1

How we add it

Small CLI to log p@k on your corpora (using source-of-truth tags or quick heuristics).

Config preset: k_small=5 for fast passes, auto-bump to k_large=30..50 inside fallback only when needed.

6) Test-time scaling for lean models (no fine-tuning required)

Lesson: They show big gains by combining retrieval with lean reasoning and test-time tricks (self-consistency/majority vote, budget forcing). You want offline + cheap → we use the tricks, skip heavy SFT. 

2508.11386v1

Add self-consistency: do n=3–5 short generations and vote (string-match or cross-encoder evidence score over answers). Gate behind --fast vs --best.

Add “budget forcing” lite: set high max_tokens but cap visible reasoning; instruct the model to keep internal notes concise (we’ll only surface the answer + citations).

7) Conversation assembly that won’t bloat history

Lesson: Put retrieved context in the system message for the current turn vs. appending into history (keeps history small and stable). 

2508.11386v1

How we add it

Our chat runner (optional) will build a new system message per turn with the refined strips + short community preface. History retains only user/assistant turns, not past contexts.

8) Evaluation knobs you’ll appreciate

Lesson: They instrument retrieval p@k and task accuracy to pick k, model size, and summarization ratio; they also tried reranking and query rewriting. We’ll mirror the easy bits. 

2508.11386v1

New CLI: bin/eval_retrieval.py (p@k over a labeled subset).

Optional query rewriter (local LLM) gated by the CRAG decision.

Reranker: leave as optional (they saw mixed results). We already have a cross-encoder on hand; flip on if you want.

What not to copy (and our substitutes)

Frontier-model synthetic data & reasoning-trace SFT
They generate synthetic queries with GPT-4o and distill DeepSeek-R1 reasoning into Qwen2.5-32B via long-context SFT (block size 32k), then deploy that tuned model. Great for research; heavy for a local, offline workflow. We won’t do this by default. Instead we:

Use summary-first + CRAG + strip filtering + self-consistency to emulate much of the gain at inference time.

If you ever want SFT later, we can prepare datasets locally and fine-tune a smaller model (e.g., 7B) as an add-on. 

2508.11386v1

Always retrieving full docs
Works for their NHS domain because pages are bounded and curated. On arbitrary repos, full-doc payloads can explode. We support rehydrate=best_chunks (default) and let you toggle full_doc when files are small. 

2508.11386v1

Query-aware summarization at retrieval time
They note it’s higher quality but slower (an extra LLM step per query). We keep it optional; static summaries are the fast default. 

2508.11386v1

Concrete patch list for our starter

New: plugins/attributes/doc-summary.py (offline summarizer; writes .knowledge/indexes/summaries/doc/<doc_id>.md).

New: kn/summary_index.py (build/read HNSW over summaries).

New: kn/crag.py (grader, action policy, strip refinement, fallback).

Update: kn/retrieval.py

Step 1: search summary index (if enabled).

Step 2: CRAG grade → Correct / Incorrect / Ambiguous.

Step 3: refine strips; on Incorrect/Ambiguous, expand k and walk graph neighbors before any (optional) web fallback.

Step 4: assemble system prompt with refined strips (not appended to history).

New: bin/eval_retrieval.py (reports p@k across a labeled CSV/JSONL).

Config additions: summarize_index, crag, crag_refine, crag_fallback blocks shown above.

Defaults I’d ship (tuned for your box)

Summary ratio 0.15, k_small=8, fallback k_large=48, graph_hops=2.

Grader = ms-marco-MiniLM-L-6-v2, thresholds upper=0.60, lower=0.30.

Strip size 300, keep 3 per chunk.

Self-consistency: off by default; --best enables n=3 votes.

Why these changes are justified

Summary-first + rehydrate improves retrieval and slashes context without losing fidelity; they measured better p@k with summaries than with full pages. 

2508.11386v1

CRAG + strip filtering boosts robustness to bad hits; their ablations and narrative emphasize the value of pre-generation filtering/correction and query rewrite. 

2508.11386v1

Agentic retrieval (only when useful) and system-slot context keep chats snappy and memory-efficient. 

2508.11386v1

Test-time scaling (self-consistency, budget control) is the practical path for lean, local models when SFT isn’t desired. 

2508.11386v1

If you want, I can generate the four new files (summary plugin, summary index, crag module, eval CLI) tailored to the scaffold you already downloaded and wire the config blocks exactly where they go.

Sources
You said:
so, RAG is nice to have but I want to be able to get as close to being able to ingest a list of every saved post on reddit and feed them into a single AI session in the most information-rich form we can despite compression.  Alternatively if that means segmenting them into multiple sessions and reducing them down with summaries/sorting that might be necessary too.  RAG by itself seems to be criticized as losing the nuance and deeper understanding of the data when digested, so im not sure if that's sufficient.

What is the breadth of other methods available here?  what do we have as tools already and what might we still want to be able to pushthis system as close to a general-purpose one we can simply toss piles of data into ajd trust that it will process it about as well as any known method out there - dynamically picking methods for different data if it needs to, even.
ChatGPT said:
Thought for 40s

You’re right: plain RAG is great for lookup, but it tends to sand off nuance when you’re trying to ingest a whole personal corpus (like every Reddit save) and think across it. Here’s a compact playbook of methods beyond vanilla RAG, what we already have in your starter, and what I’d add so you can “toss piles of data in” and get rich, trustworthy synthesis.

What we already have (good base)

Sidecar ingestion (non-intrusive): incremental hashing, chunking, embeddings (HNSW), BM25, NetworkX graph + Louvain, community summaries, export to big .md.

Enrichment plugins: terse summary, topic tags, PII scan.

CRAG-style robustness (design ready): relevance grader + strip filtering + query rewrite + fallback scope expansion (stays offline).

Summary-first retrieval (design ready): build a summary index to retrieve against; then rehydrate to full content.

What else is on the table (breadth of methods)

Think of these as composable capabilities you can mix per data & question type.

1) Hierarchical / map-reduce summarization (retains nuance)

Why: Single-pass summaries lose rare but important details.

How: Summarize per post → per cluster → global synthesis, keeping quotes & citations (“evidence packs”) at each level. Use chain-of-density style prompts to add detail and “don’t drop facts” instructions.

Where: New bin/hsum_build.py and an attributes/doc-summary.py (long-form) that writes layered summaries; exporter can assemble top-down digests.

2) “Knowledge strips” + selective recomposition (CRAG refinement)

Why: Passages are long; keep only the salient spans tied to the question/theme.

How: Sentence/150–300-char strips → score via a local cross-encoder → keep the top N per doc → recombine in source order.

Where: kn/crag.py with refine_strips(), already planned.

3) Topic discovery & clustering (multi-view navigation)

Why: You’ll have many domains with sparse links; you need lenses.

How: Embed posts, run KMeans/Agglomerative (Windows-friendly), detect keyphrases per cluster, and surface diversity (not just upvoted).

Where: New kn/topics.py → outputs .knowledge/indexes/topics/ + an “index.md” nav. (We can add HDBSCAN later if you want.)

4) Argument/claim mining (preserve positions)

Why: Reddit nuance = claims + counterclaims. Pure summaries blur this.

How: Lightweight claim/stance extraction with a small local model or prompt; store (claim, support, oppose, quotes, links) per post.

Where: Plugin attributes/claims.py → .knowledge/indexes/attributes/claims/.

5) Timeline & event extraction (when posts talk about evolving stuff)

Why: Helps the model reason causally and temporally.

How: Extract timestamps (post time + in-text dates), build event graph; cluster by entity/topic; generate timeline cards.

Where: plugins/attributes/events.py + kn/timeline.py.

6) Contextual compression & dedup (keep signal, drop repeats)

Why: Saved posts often repeat the same ideas.

How: Near-duplicate detection (MinHash/SimHash), MMR/novelty sampling when assembling contexts, “quota per cluster” to keep variety.

Where: kn/dedup.py + novelty sampler in exporter.

7) Long-context & multi-session packing (monofile, but smarter)

Why: Sometimes you do want to “just feed it all in.”

How: Pack by topic buckets, sorted by importance×novelty, then spill into sequenced sessions with a state summary passed between sessions.

Where: Extend exporter with --pack-mode {topic,chronological,claims} and --sessions N.

8) Self-consistency & self-verification (better final answers)

Why: For global synthesis, single-shot reasoning can miss edges.

How: Generate n=3 independent answers (fast local model), vote/rerank via cross-encoder vs. evidence strips; run a verification pass that asks: “Which claims are weak? What’s missing?”

Where: kn/verify.py + flag in query_rag.py --best.

9) Schema induction to a lightweight KB (when structure helps)

Why: Some subdomains want structured queries later.

How: Opportunistically extract entities/relations (“subreddit, author, topic, claim, evidence_url, time”) into Parquet/SQLite—no heavy graph DB needed yet.

Where: kn/struct_extract.py + data/warehouse.sqlite.

10) Dynamic policy engine (pick the right method per query)

Why: Different questions need different pipelines.

How: A tiny orchestrator decides: lookup (RAG) vs synthesize (HSum+strips) vs explore (topics/timeline) vs argument (claims). Uses cheap signals: query intent, hit quality, corpus size.

Where: kn/orchestrator.py + rules in pipeline.yml:

orchestrator:
  rules:
    - if: intent in ["why","compare","nuance"] or scope=="global"
      then: ["hsum", "crag_refine", "verify"]
    - if: intent in ["fact","where","who"] and good_hits
      then: ["rag_basic"]
    - if: intent=="trend" or "timeline"
      then: ["timeline","hsum_cluster"]

Reddit-specific ingest to preserve nuance

Loader: bin/ingest_reddit_saved.py that reads your export/API, normalizes:

post_id, subreddit, author, title, url, flair, score, created_utc, body_md, comments[]

Thread shaping: Build “post + highlighted comments” docs (comments scored by upvotes, diversity, novelty, and author OP replies).

Quote harvesting: Keep top quotes with permalinks; tag with stance if available.

Attribution: Every summary/claim keeps a back-link to source.

How these pieces interplay (a typical end-to-end)

Ingest & normalize Reddit saves → dedup → chunk → embed.

Run doc summaries (long-form) + topic clustering + claims + events.

For global synthesis: Orchestrator picks HSum → CRAG strips → self-consistency → verify; exporter builds an evidence-rich monofile (or multi-session pack).

For specific questions: Orchestrator tries hybrid RAG → CRAG grading; if weak, expands to graph neighbors; if still weak (and you allow), query rewrite + optional web (off by default).

For exploration: Topic index + timeline + quote booklets.

What this buys you (vs RAG-only)

Nuance preserved via quotes/claims/timelines and strip-level selection.

Coverage via hierarchical map-reduce + topic diversity controls.

Robustness via CRAG grading, fallback scope, and self-verification.

Scalability via dedup/novelty sampling and multi-session packing.

Flexibility via the orchestrator (dynamic method selection).

Minimal add-ons to your starter to get there

New modules: kn/topics.py, kn/dedup.py, kn/orchestrator.py, kn/verify.py, kn/summary_index.py, kn/crag.py (we already scoped CRAG), kn/timeline.py, kn/struct_extract.py.

New plugins: attributes/doc-summary.py (long form), attributes/claims.py, attributes/events.py.

New CLI: bin/ingest_reddit_saved.py, bin/hsum_build.py, bin/eval_retrieval.py.

Config blocks: orchestrator, summarize_index, crag*, pack_mode, sessions, dedup.

If you want, I can generate stubs for those modules and wire the orchestrator rules you prefer (e.g., prioritize HSum for “why/compare/nuance,” RAG for “fact/where,” timeline for “when/how over time”), plus a Reddit ingest script that works with your exported JSON.

You said:
Similarly we want this same system to be able to just easily digest:  github repos (readmes at least), twitter dumps, entire subreddits, scientific papers, various .txt writings, signal chats, discord dumps, lists of comfyui node repos, kiwix data dumps, academictorrents dumps, arxiv paper dumps, huggingface model pages, news articles, hackernews,  bluesky, previous chatgpt and gemini conversations, a messy local disk full of files, etc etc - a whole bunch of different sources with varying degrees of truth and trustworthiness.  Ideally we want to be able to have it just churn on these and find the most useful things it can, for various priorities, and index them generally in preparation to start.  How does that menu change things
ChatGPT said:
Thought for 19s

Love that menu. The good news: we don’t need a different “system” — we need a source-adapter layer, trust/utility scoring, and a slightly smarter orchestrator. Your GraphRAG+CRAG starter becomes a general “throw piles of data at it” engine by adding adapters for each dump format, normalizing to a common schema, running enrichment, then letting the orchestrator pick the right workflow per source and query.

Here’s the high-level delta.

1) Architecture changes (small but powerful)

Adapters → Canonical docs. Add sources/* modules that turn each dump (Reddit, GitHub, X/Twitter, PDFs, Kiwix, etc.) into a Doc with the same fields:

doc_id, source_type, source_id, title, text, html_md, url, author, created_at,
metrics{score,stars,retweets,points…}, tags[], provenance[], media[], 
trust_signals{domain_rank?, venue?, arxiv_ver?, ocr_conf?, repo_age…}


Trust & utility scoring. New kn/trust.py computes two numbers per doc:

TrustScore (0–1): source-specific reliability prior.

UtilityScore (0–1): novelty × recency × engagement × length quality.

Priority queue. kn/prioritizer.py orders work: cheap enrichments → embeddings → clustering → heavy long-form summaries, weighted by UtilityScore and your “priority policy” (e.g., papers > news > social).

Method router (orchestrator). Extend rules so source_type + trust band + user intent select the pipeline:

Fact lookup → hybrid RAG → CRAG grade → maybe expand.

Deep synthesis/nuance → hierarchical summarization + strip-refine + self-consistency.

Trend/timeline → event extraction + timeline pack.

Code repos → code-aware chunking + readme/issue synthesis.

2) Adapters you’ll want (all offline-friendly)

(Each is a small parser that yields Canonical Doc objects and writes to .knowledge/indexes/chunks/ via the existing chunker.)

Reddit saved JSON (we already outlined): posts + top comments, quote harvesting, subreddit/author/time, score.

GitHub repos (local clones or zips): README, docs/, LICENSE, *.md, top *.py/js/ts/go with code-aware chunking; heuristics from .git (commit cadence, stars if present in dump).

Twitter/X exports (zip/JSON): tweets, retweets, likes; author verification flag if present; threads reconstructed.

Discord/Signal exports: threads → conversation turns; per-speaker attribution; timestamps.

Subreddit dumps (Pushshift/JSONL): similar to Reddit saved.

arXiv/academic PDFs: PyMuPDF text; metadata from the dump (title, authors, categories, vN); optional OCR if no text layer.

Hugging Face model pages: model card (README.md), tags, downloads, last update; treat like docs.

Kiwix (ZIM): via libzim python binding; title, URL, text; provenance domain in trust signals.

AcademicTorrents: dataset/paper metadata from the dump; link back for provenance.

Hacker News dumps: item text, points, comments.

News articles (if you have local HTML/Markdown): boilerplate removal → text, domain from URL.

Bluesky (ATProto export): posts/threads like Twitter.

ChatGPT/Gemini transcripts: role-tagged turns; mark as internal notes (low trust externally).

ComfyUI node repos: from list → fetch local READMEs (you supply zip/clone), tag authors/names.

We won’t fetch anything from the web; we only parse your dumps. That keeps it offline and ToS-friendly.

3) Trust bands & utility (how we “believe” and “prioritize”)

TrustScore by source (defaults; you can tune):

Peer-reviewed venue / well-known docs: 0.85–0.95

arXiv/self-published research: 0.65–0.8 (bonus if many versions & citations present in dump)

Major news: 0.55–0.75 (domain allowlist optional)

GitHub READMEs/code: 0.5–0.7 (up with stars/age/consistency)

HN/Reddit/Twitter/Discord: 0.25–0.55

Personal chats/notes: 0.2–0.4

UtilityScore (per doc):
Utility = w1*Novelty + w2*Recency + w3*Engagement + w4*LengthQuality + w5*TopicMatch

Novelty via near-dup detection (MinHash/SimHash) and MMR at assembly time.

Engagement from upvotes/retweets/stars (normalized).

TopicMatch = cosine to your interest profile (optional).

We store bands: High (≥0.7), Medium (0.4–0.7), Low (<0.4), which the orchestrator can use (e.g., avoid low-trust in final unless corroborated).

4) Enrichments that preserve nuance (beyond RAG)

Hierarchical summaries (per-doc → per-cluster → global) with quotes + citations kept.

CRAG “knowledge strips”: sentence/short-span scoring via a local cross-encoder; keep top strips.

Topic clustering (KMeans or Agglomerative) + diversity sampler to avoid echo chambers.

Claim/stance mining (argument structure) for social/news/papers: (claim, support_evidence[], oppose_evidence[]).

Event extraction & timelines for evolving topics.

Dedup/near-dup + novelty sampling.

PII & license guardrails (already in place for PII; add a lightweight license/usage flag for code/repos).

5) How the orchestrator picks methods (examples)

Rules in pipeline.yml:

orchestrator:
  rules:
    - if: intent in ["why","compare","synthesize","opinionated"] or scope=="global"
      then: ["hsum", "crag_refine", "verify"]
    - if: intent in ["fact","where","who"] and good_hits
      then: ["rag_basic","crag_grade"]
    - if: source_mix includes ["news","social"] and question mentions "timeline" or "trend"
      then: ["events","timeline","hsum_cluster"]
    - if: source_type in ["github","code"]
      then: ["code_chunking","hsum_repo","claims_code?"]


Where:

hsum = hierarchical summarization

crag_refine = strip-level recomposition

verify = self-consistency + evidence check

events/timeline = temporal lens

code_chunking = smaller chunks + symbol-aware tagging

6) Packing “single session” mega-contexts (when you want it)

Pack by topic buckets, sorted by Utility × Trust × Novelty, include evidence quotes and source links.

If it won’t fit: multi-session chain — we export N sequential files; each session ends with a state summary the next session consumes. (Export flags: --pack-mode {topic,chronological,claims} --sessions N.)

7) What we add to your repo (small, modular)

New modules:

sources/ — adapters per dump: reddit.py, github.py, twitter.py, discord.py, arxiv.py, zim.py, hn.py, news.py, chats.py, hf.py.

kn/trust.py — TrustScore + domain/venue heuristics; pluggable bands.

kn/prioritizer.py — schedules work by UtilityScore.

kn/topics.py, kn/dedup.py, kn/timeline.py, kn/summary_index.py, kn/crag.py (as planned), kn/orchestrator.py, kn/verify.py.
New plugins:

attributes/doc-summary.py (long-form), attributes/claims.py, attributes/events.py.
New CLIs:

bin/ingest_<source>.py for each adapter,

bin/hsum_build.py, bin/eval_retrieval.py.

Config additions (minimal):

# .knowledge/config/pipeline.yml
sources:
  reddit: { enabled: true }
  github: { enabled: true }
  twitter: { enabled: true }
  # ... others toggled on/off

trust:
  domain_allowlist: []   # optional for news
  arxiv_bonus_per_version: 0.02
  github: { star_breakpoints: [10,100,1000] }

prioritizer:
  weights: { novelty: 0.35, recency: 0.2, engagement: 0.15, length_quality: 0.1, topic: 0.2 }
  max_parallel: 4

packing:
  mode: topic
  sessions: 3
  evidence_quotes: true

8) What changes because of your bigger menu?

Nothing fundamental breaks. We add adapters and trust/utility so the same pipelines (HSum, CRAG, timeline, RAG) operate across heterogeneous sources.

Quality control improves. Low-trust sources don’t dominate final synthesis unless corroborated. High-trust sources get preferential weight.

Throughput stays high. Cheap passes (doc summaries, dedup, embeddings) run first; heavier steps are batched and scheduled.

You keep optionality. Everything remains offline-first, file-system driven, and modular.

If you want, I can generate stubs for the top 4 adapters you’ll hit first (Reddit saved, GitHub repo, Twitter export, arXiv PDFs) and wire Trust/Utility + orchestrator rules so you can drop dumps into a folder and let it churn.

You said:
How about capturing and preserving the context of what matters to me (one human user among many potentially) for the various data sources?  e.g. my personal notes, my reddit comments and saved posts, tweets, twitter favorites, etc etc - those are different than the more general data of what's fed in.  though even the general data has somewhat more meaning than anything random due to my selection bias too.  how would this system account for all that, while also being general enough to be able to do the same for more users in the future and/or to make its own objective selections minimizing influence
ChatGPT said:
Thought for 17s

Totally—this is where you add a personalization layer on top of the global GraphRAG/CRAG engine, but keep it switchable so you can run in Personal, Balanced, or Objective modes. Here’s a tight plan that captures your signals, preserves your intent, and stays multi-user friendly.

Goals

Preserve “what matters to me” (saves, likes, comments, notes, chats) as first-class signal.

Stay general so multiple users can coexist later (or the system can run “objective”).

Remain offline-first and modular; everything is explainable & auditable.

Layers (what we add)

User Profile Layer (per user):

A canonical UserEvent log: “I saved/liked/replied/highlighted/read” across Reddit, X, Discord/Signal, GH stars, HN upvotes, notes, etc.

Derived interest embeddings (topic vectors) from your interactions (centroids with time decay).

A light preference schema: topics, sources, styles (e.g., longform, tutorials), recency bias, and “avoid” lists.

Personal Graph Overlay:

Over the global graph, build an ego-graph: nodes and edges you’ve touched, plus nearest neighbors.

Keep weights for “touched by user,” “often co-saved,” “same author/subreddit,” etc.

Scoring & Modes:

Rank items by a transparent formula, then pick a mode:

Score = α·Trust + β·Utility + γ·PersonalRelevance + δ·DiversityBoost − ε·DupPenalty


Personal: γ high (e.g., 0.45), α/β moderate

Balanced: γ moderate (0.2) with stronger diversity

Objective: γ=0; α (trust) and diversity up; also “counter-bias” sampling

Always expose why an item was chosen: top contributing terms, source type, trust band, personal signals.

What to capture (signals)

Normalize everything into a single UserEvent schema (JSONL):

user_id, timestamp, source_type, source_id, event_type, weight,
meta{ subreddit|repo|handle, url, author, title, tags[], device?... }


event_type: saved, upvoted, liked, starred, replied, quoted, highlighted, read_dwell, note_edit

weight: default 1.0; upweight comments you wrote; time-decay older events

We store this per user at:

.knowledge/users/<user_id>/events/events.jsonl

Modeling (simple, effective)

Interest vectors: for each user, maintain one or more centroids (topic clusters) as EMA over embeddings of interacted docs/quotes. Keep:

Global interest vector

Per-domain vectors (reddit, code, papers)

Recency-biased vector for “current focus”

PersonalRelevance(doc) = max cosine(doc_emb, interest_vectors) with decay for stale topics.

DiversityBoost = MMR/novelty w.r.t. what you’ve recently read/seen.

Counter-bias meter: KL divergence between your interest distribution and the corpus distribution; we can surface “areas you under-sample.”

How retrieval & synthesis use it

Two-stage retrieval:

Global hybrid (dense+BM25), then re-rank by PersonalRelevance unless mode=Objective.

If CRAG says “Incorrect/Ambiguous,” expand scope via graph neighbors prioritized by ego-graph proximity (Personal/Balanced) or by trust (Objective).

Context budget allocation:

Personal: allocate, say, 60% budget to high PersonalRelevance across clusters; reserve 40% for high-Trust/novel content.

Balanced: 40/60 split.

Objective: 0/100; plus “anti-filter bubble” slot that forces in counter-examples.

Generation prompts:

Add a persona frame: “Weight sources you have engaged with, but report dissenting high-trust evidence.”

Objective mode prompt removes the persona and asks for “minimize personalization; maximize trust & diversity.”

Multi-user & generality

Per-user dirs:

.knowledge/users/<user_id>/
  profile.yml            # declared prefs (optional)
  events/events.jsonl    # raw interaction log
  vectors/*.npy          # interest vectors (global/domain/recency)
  overlays/ego_graph.*   # user overlay on the global graph
  caches/…               # cached re-ranks & summaries


The global corpus lives as it does now. User overlays don’t mutate it.

Switching users is just switching the user_id in CLI flags or env.

Objective selections (minimize influence)

Mode = Objective:

γ=0; stronger α (Trust) and δ (Diversity).

Disallow user-specific boosts; use domain allowlist and venue priors only.

Report exposure audit: distribution of sources and stances used.

Optionally dual runs: produce personalized and objective answers side-by-side with different evidence packs.

Where it fits in your repo (concrete)

New modules:

kn/profile.py – load/save profile.yml, manage user_id

kn/personalizer.py – compute/update interest vectors; compute PersonalRelevance

kn/ego_graph.py – build/update user overlay graph

kn/trust.py – (already planned) trust bands per source; expose α defaults per mode

kn/orchestrator.py – add mode-aware routing & budget allocation

kn/mmr.py – novelty/diversity utilities

Config additions:

# .knowledge/config/pipeline.yml
users:
  default_user: "me"
  modes: ["personal","balanced","objective"]
  personal:
    alpha: 0.25   # Trust
    beta: 0.25    # Utility
    gamma: 0.45   # PersonalRelevance
    delta: 0.15   # Diversity
  balanced:
    alpha: 0.30
    beta: 0.30
    gamma: 0.20
    delta: 0.25
  objective:
    alpha: 0.45
    beta: 0.35
    gamma: 0.00
    delta: 0.25
  interest:
    decay_half_life_days: 45
    per_domain_vectors: true
    recent_window_days: 14
retrieval_budget:
  personal: { personal_pct: 0.6, trust_pct: 0.4 }
  balanced: { personal_pct: 0.4, trust_pct: 0.6 }
  objective:{ personal_pct: 0.0, trust_pct: 1.0 }


CLI knobs (examples):

# Use my profile in personal mode
python bin/query_rag.py --q "What should I read next about X?" --user me --mode personal

# Balanced synthesis over my Reddit saves + papers
python bin/export_monofile.py --q "State of topic Y" --user me --mode balanced --pack-mode topic

# Objective brief (no personalization) from papers+news only
python bin/export_monofile.py --q "Objective summary of Z" --mode objective --sources papers,news

Privacy & audit

Personal events never leave disk; PII plugin runs on both global and user artifacts.

Every export includes an evidence ledger (sources + why they were included).

Optional encryption-at-rest for .knowledge/users/<user_id> if you want it.

Defaults I’d start with

Half-life 45 days for interest decay; 3 interest vectors (global/domain/recent).

Personal mode: 60/40 budget split; Balanced: 40/60; Objective: 0/100.

Dual-answer option for high-stakes queries: return personalized + objective with evidence lists.

If you like, I can draft the profile.yml format, the UserEvent JSONL schema, and a tiny bin/user_events_import.py (for Reddit saves/Twitter likes/Discord exports) so your profile builds itself the moment you drop those files in.

## Hierarchical Summarization & Metadata Roadmap

### 1. Chunking Foundations (v1)
- Upgrade `kn/chunking.py` to capture structure: detect headings/TOC, bullet hierarchies; store `section_id`, `parent_section_id`, `level`, `title`, breadcrumbs.
- Persist per-document structure files (e.g., `.knowledge/indexes/structure/<doc_id>.json`) to describe the tree and link to chunk IDs.
- Tailor chunking policies by document type (code vs. prose vs. scanned PDFs) with pluggable detectors.

### 2. Map-Reduce Pipeline (v1)
- Ensure `chunk-summary` runs automatically on ingest/update; extend to chunk-level attributes (keywords, glossary, risk flags).
- Generalize `doc_reduce` into reusable reducers per attribute type; each reducer consumes ordered chunk outputs, emits doc-level JSON, records provenance & timestamps.
- Add dashboard controls to trigger map-reduce batches, display chunk coverage, and surface doc-level aggregates.

### 3. Multi-Level Summaries (v2)
- Introduce section-level reducers (chunk → section → doc). Support recursive summarization for deep documents.
- Implement corpus-level reducers to summarize domains/repos using doc summaries (e.g., domain overview, changelog).
- Auto-refresh stale sections by comparing chunk/doc mtimes; trigger incremental rebuilds.

### 4. Attribute Aggregation (v2)
- Refactor doc-level attributes to operate over chunk outputs (merge per-section topic tags, glossary terms, requirements, etc.).
- Support incremental updates: when a subset of chunks change, recompute affected sections/doc aggregates only.
- Store merge heuristics (frequency, confidence, section coverage) so outputs remain explainable.

### 5. Repository-Level Hierarchies (v3)
- Maintain multiple virtual hierarchies: original file tree, logical domains, priority tiers, temporal views.
- Compute rollups for each hierarchy node (summary, highlights, metrics) using section/doc outputs.
- Allow dashboard/agents to switch hierarchies and retrieve the corresponding summaries/metadata.

### 6. Retrieval & Storage (v3)
- Index chunk/section summaries alongside embeddings; include hierarchy metadata so retrieval can surface context at the right depth.
- Provide APIs for hierarchy traversal (e.g., `/api/hierarchy/<name>` returning nodes, summaries, children).
- Cache LLM inputs/outputs for chunk & section stages to avoid redundant calls; record latency/stats for tuning.

### 7. Guardrails & Tooling (ongoing)
- Enforce strict token budgets per stage; re-chunk or flag when limits are exceeded.
- Add CLI utilities for hierarchy builds/status (`bin/hierarchy_build.py`, `bin/hierarchy_status.py`) and regression tests.
- Monitor coverage metrics (chunk summaries present, doc reduces up-to-date, hierarchy rollups fresh).

### 8. Future Directions
- Multi-perspective hierarchies (importance, recency, author ownership, user-defined playlists).
- Integration with training/finetuning pipelines (export structured summaries, provide grounding traces).
- Agent planner that uses hierarchy metadata to decide reading order, recommend updates, or trigger targeted re-summarization.

./setup.ps1
param(
  [string]$RepoPath = ".\FAE",
  [string]$Model = "llama-3.1-8b-instruct"
)

if (-not (Test-Path ".venv\Scripts\python.exe")) {
  python -m venv .venv
} else {
  Write-Host "Virtual environment already exists; skipping creation."
}

if (-not (Test-Path ".venv\Scripts\Activate.ps1")) {
  throw "Virtual environment missing activation script; rerun without workers or recreate manually."
}

. .\.venv\Scripts\Activate.ps1

if (Test-Path "requirements-win.txt") {
  python -m pip install -r requirements-win.txt
} else {
  python -m pip install -r requirements.txt
}
$env:PYTHONPATH = $PSScriptRoot

try {
  $resp = Invoke-RestMethod -Method Get -Uri "http://localhost:1234/v1/models" -TimeoutSec 5
} catch { Write-Warning "LM Studio local server not reachable." }

@"
OPENAI_BASE_URL=http://10.5.0.2:12345/v1
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=$Model
EMBED_MODEL=BAAI/bge-small-en-v1.5
REPO_PATH=$RepoPath
KN_ROOT=.knowledge
OCR_ENABLED=false
"@ | Set-Content .env -Encoding UTF8

python .\bin\ingest_build_graph.py --repo $RepoPath --full

./bin\ingest_build_graph.py
import argparse, pathlib
from kn.config import load_configs
from kn.file_index import initial_scan
from kn.chunking import chunk_repo
from kn.embeddings import embed_chunks
from kn.graph_nx import build_or_update_graph, summarize_communities

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--repo", required=True)
    ap.add_argument("--full", action="store_true")
    args = ap.parse_args()
    cfg = load_configs()
    repo = pathlib.Path(args.repo).resolve()
    print("[ingest] scanning repo…")
    docs = initial_scan(repo, cfg)
    print(f"[ingest] {len(docs)} docs found")
    print("[chunk] building chunks…")
    chunks = chunk_repo(docs, cfg, repo_root=repo)
    print("[embed] embedding chunks…")
    embed_chunks(chunks, cfg)
    print("[graph] updating graph…")
    build_or_update_graph(chunks, cfg)
    print("[summarize] community summaries…")
    summarize_communities(cfg)
    print("[done]")

./bin\watch_daemon.py
import argparse, pathlib
from kn.config import load_configs
from kn.file_index import watch_changes

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--repo", required=True, help="Path to watch")
    args = ap.parse_args()
    cfg = load_configs()
    repo = pathlib.Path(args.repo).resolve()
    print(f"[watch] monitoring {repo} … (Ctrl+C to quit)")
    watch_changes(repo, cfg)

./bin\enrich_worker.py
import argparse, time, json, threading, pathlib
from kn.config import load_configs
from kn.jobs_sqlite import (
    ensure_db,
    dequeue_batch,
    ack_job,
    iter_docs_for_jobs,
    fail_and_requeue_job,
    list_pending_plugins,
    try_acquire,
    release,
    set_limit,
    reset_status,
    reset_running_jobs,
    reset_counter,
)


ATTR_ROOT = pathlib.Path('.knowledge/indexes/attributes')


def _job_output_exists(plugin: str, job: dict) -> bool:
    payload = job.get('payload') or {}
    if payload.get('overwrite') or payload.get('force'):
        return False
    doc_id = job.get('doc_id')
    plugin_dir = ATTR_ROOT / plugin
    if plugin == 'chunk-summary':
        chunk_id = payload.get('chunk_id')
        if not chunk_id:
            return False
        return plugin_dir.joinpath(f"{chunk_id}.json").exists()
    if plugin == 'summaries':
        mode = payload.get('mode')
        if not mode or not doc_id:
            return False
        return plugin_dir.joinpath(f"{doc_id}_{mode}.json").exists()
    if not doc_id:
        return False
    return plugin_dir.joinpath(f"{doc_id}.json").exists()

def run_once(plugins, cfg, batch_size=16):
    ensure_db(cfg)
    # If no jobs for requested plugins, peek pending list and suggest available
    jobs = dequeue_batch(cfg, wanted_plugins=plugins, limit=batch_size)
    if not jobs:
        # try to broaden to any pending plugins if requested plugins empty
        pend = list_pending_plugins(cfg)
        avail = [p for p in pend if p in plugins]
        if not avail:
            return 0
        jobs = dequeue_batch(cfg, wanted_plugins=avail, limit=batch_size)
        if not jobs:
            return 0
    docs = iter_docs_for_jobs(jobs)
    def _load_chunk_by_id(chunk_id: str):
        import json as _json, pathlib as _pathlib
        p = _pathlib.Path('.knowledge/indexes/chunks')/f"{chunk_id}.json"
        if p.exists():
            try:
                return _json.loads(p.read_text(encoding='utf-8'))
            except Exception:
                return None
        return None
    import subprocess, pathlib, sys
    by_plugin = {}
    for j in jobs:
        by_plugin.setdefault(j["plugin"], []).append(j)
    for plugin, items in by_plugin.items():
        remaining = []
        for j in items:
            if _job_output_exists(plugin, j):
                ack_job(cfg, j["id"])
                print(f"[enrich] {plugin}: doc {j.get('doc_id')} already has output; skipping (no overwrite)")
            else:
                remaining.append(j)
        if not remaining:
            continue
        acquired = try_acquire(cfg, 'llm_concurrency')
        if not acquired:
            print(f"[enrich] {plugin}: concurrency limit reached, skipping batch")
            continue
        fs_name = plugin.replace('-', '_')
        pypath = pathlib.Path(f"plugins/attributes/{fs_name}.py")
        if not pypath.exists():
            print(f"[enrich] plugin not found: {plugin}")
            for j in items:
                fail_and_requeue_job(cfg, j["id"], error_message="plugin not found", back_to_pending=False)
            continue
        inp_lines = []
        doc_ids = []
        for j in remaining:
            payload = j.get("payload") or {}
            doc = None
            if payload.get('chunk_id'):
                doc = _load_chunk_by_id(payload['chunk_id'])
            if doc is None:
                doc = docs.get(j["doc_id"])  # fallback
            if doc:
                merged = dict(doc)
                merged["payload"] = payload
                inp_lines.append(json.dumps(merged, ensure_ascii=False))
                doc_ids.append(j["doc_id"])
        if not inp_lines:
            for j in remaining:
                fail_and_requeue_job(cfg, j["id"], error_message="no input doc", back_to_pending=False)
            continue
        try:
            # per-plugin process timeout (seconds)
            pcfg = (cfg.get('plugins') or {}).get(plugin) or {}
            proc_timeout = pcfg.get('process_timeout') if isinstance(pcfg, dict) else None
            if not isinstance(proc_timeout, (int, float)):
                proc_timeout = 600 if plugin in ('multi-basic','doc-skeleton') else 300
            summary_docs = ",".join(doc_ids[:4])
            if len(doc_ids) > 4:
                summary_docs += ",…"
            if summary_docs:
                print(f"[worker-current] plugin={plugin} docs={summary_docs}")
            proc = subprocess.Popen([sys.executable, str(pypath)], stdin=subprocess.PIPE,
                                    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)

            def _pump_stdout():
                try:
                    for raw_line in proc.stdout:
                        print(f"[plugin:{plugin}] {raw_line.rstrip()}" )
                except Exception:
                    pass

            reader = threading.Thread(target=_pump_stdout, daemon=True)
            reader.start()

            input_blob = "\n".join(inp_lines) + "\n"
            try:
                proc.stdin.write(input_blob)
                proc.stdin.close()
            except Exception:
                pass

            try:
                proc.wait(timeout=proc_timeout)
            except subprocess.TimeoutExpired:
                print(f"[enrich] {plugin}: timeout, requeueing batch")
                try:
                    proc.kill()
                except Exception:
                    pass
                try:
                    proc.communicate(timeout=1)
                except Exception:
                    pass
                for j in remaining:
                    fail_and_requeue_job(cfg, j["id"], error_message="timeout", back_to_pending=True)
                continue
            finally:
                reader.join(timeout=0.5)

            if proc.returncode and proc.returncode != 0:
                msg = f"exit code {proc.returncode}"
                print(f"[enrich] {plugin}: {msg}, requeueing batch")
                for j in remaining:
                    fail_and_requeue_job(cfg, j["id"], error_message=msg, back_to_pending=True)
            else:
                print(f"[enrich] {plugin}: {len(remaining)} docs processed")
                for j in remaining:
                    ack_job(cfg, j["id"])
        except Exception as e:
            msg = str(e)[:500]
            print(f"[enrich] {plugin}: error {msg}")
            back_to_pending = True
            if 'Context too large' in msg or 'context too large' in msg:
                back_to_pending = False
            for j in remaining:
                fail_and_requeue_job(cfg, j["id"], error_message=msg, back_to_pending=back_to_pending)
        finally:
            if acquired:
                release(cfg, 'llm_concurrency')
            print(f"[worker-current-clear] plugin={plugin}")
    return len(jobs)

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--plugins", required=True, help="comma-separated plugin names or '*' for any pending")
    ap.add_argument("--watch", action="store_true")
    ap.add_argument("--batch", type=int, default=16)
    ap.add_argument("--any-pending", action="store_true", dest="any_pending")
    ap.add_argument("--max-inflight", type=int, default=2, help="global concurrent LLM calls across workers")
    args = ap.parse_args()
    cfg = load_configs()
    plugins = [p.strip() for p in args.plugins.split(",") if p.strip()]
    any_pending = args.any_pending or (len(plugins)==1 and plugins[0] in ("*","any"))
    # set concurrency limit at start
    set_limit(cfg, 'llm_concurrency', max(1, int(args.max_inflight)))
    reset_running_jobs(cfg, status='pending')
    reset_counter(cfg, 'llm_concurrency')
    while True:
        use_plugins = plugins
        if any_pending:
            dyn = list_pending_plugins(cfg)
            use_plugins = dyn if dyn else []
        n = run_once(use_plugins, cfg, batch_size=args.batch)
        if not args.watch:
            break
        if n == 0:
            time.sleep(2)

./bin\query_rag.py
import argparse
from kn.retrieval import answer_query
from kn.config import load_configs

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--q", required=True)
    ap.add_argument("--scope", default=None)
    ap.add_argument("--topk", type=int, default=12)
    args = ap.parse_args()
    cfg = load_configs()
    print(answer_query(args.q, cfg, scope=args.scope, topk=args.topk))

./bin\export_monofile.py
import argparse, pathlib
from kn.exporter import export_monofile
from kn.config import load_configs

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--q", required=True)
    ap.add_argument("--budget", type=int, default=None)
    ap.add_argument("--scope", default=None)
    ap.add_argument("--no-meta", action="store_true", help="Exclude per-chunk headers/citations")
    ap.add_argument("--out", required=True)
    args = ap.parse_args()
    cfg = load_configs()
    out = pathlib.Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)
    export_monofile(args.q, cfg, out, budget=args.budget, scope=args.scope, include_meta=not args.no_meta)
    print(f"[export] wrote {out}")

./bin\report_attrs.py
﻿import json, csv, pathlib

def main():
    base = pathlib.Path('.knowledge/indexes/attributes')
    out = pathlib.Path('.knowledge/exports/reports')
    out.mkdir(parents=True, exist_ok=True)

    def load(name: str):
        result = {}
        p = base / name
        if p.exists():
            for f in p.glob('*.json'):
                try:
                    rec = json.loads(f.read_text(encoding='utf-8'))
                    result[rec.get('doc_id')] = rec
                except Exception:
                    continue
        return result

    summaries = load('summary-20w')
    tags = load('topic-tags')
    pii = load('pii-scan')
    ids = sorted(set(summaries) | set(tags) | set(pii))

    rows = []
    for i in ids:
        rows.append({
            'doc_id': i,
            'summary': summaries.get(i, {}).get('value', ''),
            'tags': tags.get(i, {}).get('value', ''),
            'pii_risk': pii.get(i, {}).get('risk', ''),
        })

    csv_path = out / 'attributes_catalog.csv'
    with csv_path.open('w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['doc_id','summary','tags','pii_risk'])
        writer.writeheader()
        writer.writerows(rows)
    print(f"[report] wrote {csv_path}")

if __name__ == '__main__':
    main()




./bin\report_communities.py
import json, pathlib

def main():
    src = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')
    out_dir = pathlib.Path('.knowledge/exports/reports')
    out_dir.mkdir(parents=True, exist_ok=True)
    lines = src.read_text(encoding='utf-8').splitlines() if src.exists() else []
    md = ['# Communities', '']
    for l in lines[:200]:
        try:
            c = json.loads(l)
            md.append(f"- {c['id']} (size={c['size']}): {c['summary']}")
        except Exception:
            continue
    out = out_dir / 'communities.md'
    out.write_text('\n'.join(md), encoding='utf-8')
    print(f"[report] wrote {out}")

if __name__ == '__main__':
    main()



./bin\export_graph.py
import json, pathlib, networkx as nx

def main():
    graph_path = pathlib.Path('.knowledge/indexes/graph/graph.jsonl')
    out_dir = pathlib.Path('.knowledge/exports/graph')
    out_dir.mkdir(parents=True, exist_ok=True)
    G = nx.Graph()
    if graph_path.exists():
        for l in graph_path.read_text(encoding='utf-8').splitlines():
            try:
                d = json.loads(l)
                G.add_edge(d['src'], d['dst'], weight=d.get('weight', 1))
            except Exception:
                continue
    nx.write_gexf(G, str(out_dir / 'graph.gexf'))
    nx.write_graphml(G, str(out_dir / 'graph.graphml'))
    print(f"[graph] wrote {out_dir / 'graph.gexf'} and {out_dir / 'graph.graphml'}")

if __name__ == '__main__':
    main()



./bin\planner_llm.py
import argparse, json, pathlib, collections
from kn.config import load_configs
from kn.jobs_sqlite import ensure_db, enqueue
from kn.utils.llm_client import chat

CHUNK_DIR = pathlib.Path('.knowledge/indexes/chunks')
COMM_PATH = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')

def collect_docs_and_domains():
    doc_to_domain: dict[str, str] = {}
    domain_counts = collections.Counter()
    for p in CHUNK_DIR.glob('*.json'):
        try:
            rec = json.loads(p.read_text(encoding='utf-8'))
        except Exception:
            continue
        doc_id = rec.get('doc_id')
        meta = rec.get('meta', {})
        domain = meta.get('domain', 'root') or 'root'
        if doc_id not in doc_to_domain:
            doc_to_domain[doc_id] = domain
            domain_counts[domain] += 1
    return doc_to_domain, domain_counts

def load_community_summaries(limit=8):
    out = []
    if COMM_PATH.exists():
        for line in COMM_PATH.read_text(encoding='utf-8').splitlines()[:limit]:
            try:
                c = json.loads(line)
                out.append(f"{c.get('id')} size={c.get('size')}: {c.get('summary')}")
            except Exception:
                continue
    return out

SYS_PROMPT = (
    "You are a planning agent for repository analysis. Based on the context, propose high-value attribute jobs.\n"
    "Available plugins: ['summary-20w','topic-tags','pii-scan','glossary','requirements','todo-items','faq-pairs'].\n"
    "Scopes you may use: 'all' or a list of domains from the provided list.\n"
    "Return STRICT JSON: {\"jobs\": [{\"plugin\": str, \"scope\": str, \"limit\": int|null}...]}.\n"
    "Choose at most 6 jobs. Prefer targeted scopes over 'all'."
)

def build_planner_context():
    doc_to_domain, domain_counts = collect_docs_and_domains()
    domains_list = ', '.join([f"{d}({n})" for d, n in domain_counts.most_common()]) or 'root(0)'
    comms = load_community_summaries()
    ctx = [
        "Context:",
        f"Domains: {domains_list}",
        "Top communities:",
    ] + [f"- {c}" for c in comms]
    return "\n".join(ctx), doc_to_domain

def expand_scope(scope: str, doc_to_domain: dict[str,str]) -> list[str]:
    if scope.strip().lower() == 'all':
        return list(doc_to_domain.keys())
    if scope.startswith('domain:'):
        want = scope.split(':',1)[1].strip().lower()
        return [doc for doc, dom in doc_to_domain.items() if (dom or '').lower() == want]
    # Unknown scope → no docs
    return []

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--max-jobs', type=int, default=64)
    args = ap.parse_args()

    cfg = load_configs()
    ensure_db(cfg)
    context, doc_to_domain = build_planner_context()

    prompt = SYS_PROMPT + "\n\n" + context + "\n\nReturn JSON now."
    try:
        raw = chat(prompt, max_tokens=400, temperature=0.2)
        plan = json.loads(raw)
        jobs = plan.get('jobs', []) if isinstance(plan, dict) else []
    except Exception:
        jobs = []

    enq = 0
    for job in jobs:
        plugin = str(job.get('plugin','')).strip()
        scope = str(job.get('scope','all')).strip()
        limit = job.get('limit')
        docs = expand_scope(scope, doc_to_domain)
        if limit and isinstance(limit, int):
            docs = docs[:max(0, limit)]
        for doc_id in docs:
            enqueue(cfg, plugin, doc_id, payload={})
            enq += 1
            if enq >= args.max_jobs:
                break
        if enq >= args.max_jobs:
            break
    print(f"[planner] enqueued {enq} jobs from LLM plan")

if __name__ == '__main__':
    main()



./bin\jobs_status.py
import argparse
import collections
import datetime as dt
import json
import os
import pathlib
import sqlite3
import sys

# Ensure project root is on sys.path when invoked from subdirectories
ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from dotenv import load_dotenv
import requests


CHUNKS_DIR = pathlib.Path('.knowledge/indexes/chunks')
ATTR_DIR = pathlib.Path('.knowledge/indexes/attributes')
DB_PATH = pathlib.Path('.knowledge/queues/jobs.sqlite')


def _unique_doc_ids():
    seen = set()
    if not CHUNKS_DIR.exists():
        return seen
    for p in CHUNKS_DIR.glob('*.json'):
        try:
            doc_id = p.stem.split('-')[0]
            seen.add(doc_id)
        except Exception:
            continue
    return seen


def _attribute_coverage(plugins, doc_ids):
    coverage = {}
    for plugin in plugins:
        p = ATTR_DIR / plugin
        have = 0
        missing = []
        if p.exists():
            existing = {f.stem for f in p.glob('*.json')}
            for d in doc_ids:
                if d in existing:
                    have += 1
                else:
                    missing.append(d)
        else:
            missing = list(doc_ids)
        total = len(doc_ids)
        coverage[plugin] = {
            'total_docs': total,
            'have': have,
            'pct': round(100.0 * (have / total), 1) if total else 0.0,
            'missing_examples': missing[:5],
        }
    return coverage


def _domains_breakdown():
    from json import loads
    dom = collections.Counter()
    for p in CHUNKS_DIR.glob('*.json'):
        try:
            rec = loads(p.read_text(encoding='utf-8'))
            d = (rec.get('meta') or {}).get('domain') or 'root'
            doc_id = rec.get('doc_id')
            # count per-doc by domain (first chunk only)
            dom[(doc_id, d)] += 1
        except Exception:
            continue
    out = collections.Counter()
    for (_, d), _ in dom.items():
        out[d] += 1
    return [{'domain': k, 'docs': v} for k, v in out.most_common()]


def _db_summary(minutes_recent=60):
    if not DB_PATH.exists():
        return {
            'present': False,
            'total': 0,
            'by_status': {},
            'by_plugin_status': {},
            'oldest_pending_min': None,
            'recent_created': {},
        }
    con = sqlite3.connect(DB_PATH)
    con.row_factory = sqlite3.Row
    cur = con.cursor()
    out = {'present': True}
    try:
        by_status = {k: v for k, v in cur.execute(
            "select status, count(*) from jobs group by status").fetchall()}
        out['by_status'] = by_status
        total = sum(by_status.values()) if by_status else 0
        out['total'] = total

        # by plugin, status
        plugin_status = {}
        for row in cur.execute(
            "select plugin, status, count(*) as n from jobs group by plugin, status"):
            plugin_status.setdefault(row['plugin'], {})[row['status']] = row['n']
        out['by_plugin_status'] = plugin_status

        # durations per plugin (avg seconds)
        try:
            durations = {}
            for row in cur.execute(
                "select plugin, avg(strftime('%s',coalesce(completed_at, created_at)) - strftime('%s',created_at)) as avg_s, count(*) as n "
                "from jobs where status='done' and completed_at is not null group by plugin"):
                durations[row['plugin']] = {'avg_s': round(row['avg_s'] or 0, 1), 'n': row['n']}
        except Exception:
            durations = {}
        out['durations'] = durations

        # oldest pending age (minutes)
        row = cur.execute(
            "select created_at from jobs where status='pending' order by id asc limit 1").fetchone()
        if row and row['created_at']:
            try:
                created = dt.datetime.fromisoformat(str(row['created_at']))
                age_min = (dt.datetime.now() - created).total_seconds() / 60.0
            except Exception:
                age_min = None
        else:
            age_min = None
        out['oldest_pending_min'] = round(age_min, 1) if age_min is not None else None

        # recent created by plugin within window
        recent = {}
        try:
            cutoff = dt.datetime.now() - dt.timedelta(minutes=minutes_recent)
            cutoff_iso = cutoff.isoformat(sep=' ')
            for row in cur.execute(
                "select plugin, count(*) as n from jobs where created_at >= ? group by plugin",
                (cutoff_iso,)):
                recent[row['plugin']] = row['n']
        except Exception:
            recent = {}
        out['recent_created'] = recent

        # recent throughput and ETA
        try:
            cutoff = dt.datetime.now() - dt.timedelta(minutes=minutes_recent)
            cutoff_iso = cutoff.isoformat(sep=' ')
            row = cur.execute(
                "select count(*) as n from jobs where status='done' and completed_at >= ?",
                (cutoff_iso,)
            ).fetchone()
            recent_done_total = row[0] if row else 0
            window_sec = float(minutes_recent) * 60.0 if minutes_recent else 1.0
            overall_dps = recent_done_total / window_sec
            plugin_dps = {}
            for row in cur.execute(
                "select plugin, count(*) as n from jobs where status='done' and completed_at >= ? group by plugin",
                (cutoff_iso,)
            ):
                plugin_dps[row['plugin']] = row['n'] / window_sec
            out['throughput'] = {
                'window_min': minutes_recent,
                'overall_dps': round(overall_dps, 4),
                'plugin_dps': {k: round(v, 4) for k, v in plugin_dps.items()},
            }
            pending_total = sum((plugin_status.get(k, {}).get('pending', 0) for k in plugin_status.keys()))
            eta_overall = (pending_total / overall_dps) if overall_dps > 0 else None
            durations = out.get('durations', {})
            plugin_eta = {}
            for plug, stat in plugin_status.items():
                pend = stat.get('pending', 0)
                dps = plugin_dps.get(plug, 0.0)
                if dps > 0:
                    plugin_eta[plug] = pend / dps
                else:
                    avg_s = (durations.get(plug) or {}).get('avg_s') or 0
                    plugin_eta[plug] = (pend * avg_s) if avg_s > 0 else None
            out['eta'] = {
                'overall_sec': round(eta_overall, 1) if eta_overall is not None else None,
                'plugin_sec': {k: (round(v, 1) if v is not None else None) for k, v in plugin_eta.items()},
            }
        except Exception:
            out['throughput'] = {'window_min': minutes_recent, 'overall_dps': 0, 'plugin_dps': {}}
            out['eta'] = {'overall_sec': None, 'plugin_sec': {}}
    finally:
        con.close()
    return out


def _llm_health():
    load_dotenv(override=False)
    base = os.getenv('OPENAI_BASE_URL', 'http://localhost:1234/v1')
    try:
        r = requests.get(base.rstrip('/') + '/models', timeout=5)
        ok = r.status_code == 200
        models = r.json().get('data', []) if ok else []
        return {'reachable': ok, 'endpoint': base, 'models': [m.get('id') for m in models[:5]]}
    except Exception:
        return {'reachable': False, 'endpoint': base, 'models': []}


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--recent-mins', type=int, default=60)
    ap.add_argument('--json', action='store_true')
    args = ap.parse_args()

    plugins = ['summary-20w','topic-tags','pii-scan','glossary','requirements','todo-items','faq-pairs']
    doc_ids = _unique_doc_ids()
    data = {
        'docs_total': len(doc_ids),
        'domains': _domains_breakdown(),
        'attributes_coverage': _attribute_coverage(plugins, doc_ids),
        'queue': _db_summary(args.recent_mins),
        'llm': _llm_health(),
    }

    if args.json:
        print(json.dumps(data, ensure_ascii=False, indent=2))
        return

    # Pretty print
    print(f"docs_total: {data['docs_total']}")
    print("domains:")
    for d in data['domains'][:10]:
        print(f"  - {d['domain']}: {d['docs']}")
    print("attributes coverage:")
    for k, v in data['attributes_coverage'].items():
        print(f"  - {k}: {v['have']}/{v['total_docs']} ({v['pct']}%)")
    print("queue:")
    q = data['queue']
    print(f"  present: {q['present']}")
    if q['present']:
        print(f"  total: {q.get('total',0)}  by_status: {q.get('by_status',{})}")
        print(f"  oldest_pending_min: {q.get('oldest_pending_min')}")
        if q.get('recent_created'):
            print(f"  recent_created({args.recent_mins}m): {q['recent_created']}")
        print("  by_plugin_status:")
        for plug, m in q.get('by_plugin_status',{}).items():
            print(f"    - {plug}: {m}")
    print("llm:")
    llm = data['llm']
    print(f"  reachable: {llm['reachable']}  endpoint: {llm['endpoint']}")
    if llm['models']:
        print(f"  models: {', '.join(llm['models'])}")


if __name__ == '__main__':
    main()



./bin\render_report.py
import json, pathlib, html, datetime as dt, sqlite3

CHUNKS_DIR = pathlib.Path('.knowledge/indexes/chunks')
ATTR_DIR   = pathlib.Path('.knowledge/indexes/attributes')
COMM_PATH  = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')
OUT_HTML   = pathlib.Path('.knowledge/exports/reports/index.html')
GRAPH_PATH = pathlib.Path('.knowledge/indexes/graph/graph.jsonl')

PLUGINS = ['summary-20w','topic-tags','pii-scan','glossary','requirements','todo-items','faq-pairs']

def _iter_docs():
    seen = {}
    for p in CHUNKS_DIR.glob('*.json'):
        try:
            rec = json.loads(p.read_text(encoding='utf-8'))
        except Exception:
            continue
        doc_id = rec.get('doc_id')
        if doc_id in seen:
            continue
        meta = rec.get('meta') or {}
        seen[doc_id] = {
            'doc_id': doc_id,
            'rel': meta.get('rel') or meta.get('path') or '',
            'domain': meta.get('domain') or 'root',
        }
    return list(seen.values())

def _load_attr(plugin, doc_id):
    p = ATTR_DIR / plugin / f'{doc_id}.json'
    if not p.exists():
        return None
    try:
        return json.loads(p.read_text(encoding='utf-8'))
    except Exception:
        return None

def _load_comms(max_items=100):
    out = []
    if COMM_PATH.exists():
        for i, line in enumerate(COMM_PATH.read_text(encoding='utf-8').splitlines()):
            if i >= max_items: break
            try:
                c = json.loads(line)
                out.append(c)
            except Exception:
                continue
    return out

def _esc(s):
    return html.escape(str(s or ''))

def _render_doc_card(d):
    doc_id = d['doc_id']
    parts = [f"<div class='card' id='doc-{doc_id}' data-doc='{doc_id}' data-domain='{_esc(d['domain'])}'><div class='hdr'><span class='rel'>{_esc(d['rel'])}</span><span class='meta'>doc:{doc_id} · domain:{_esc(d['domain'])}</span></div>"]
    # attributes
    def sec(title, txt):
        if not txt: return
        parts.append(f"<div class='sec'><div class='stit'>{_esc(title)}</div><div class='sbody'><pre>{_esc(txt)}</pre></div></div>")

    # summary
    s = _load_attr('summary-20w', doc_id); sec('Summary (20w)', s and s.get('value'))
    t = _load_attr('topic-tags', doc_id);  sec('Tags', t and t.get('value'))
    g = _load_attr('glossary', doc_id);    sec('Glossary', g and g.get('value'))
    r = _load_attr('requirements', doc_id);sec('Requirements', r and r.get('value'))
    f = _load_attr('faq-pairs', doc_id);   sec('FAQ', f and f.get('value'))
    td = _load_attr('todo-items', doc_id); sec('TODOs', json.dumps(td.get('value'), ensure_ascii=False, indent=2) if td else '')
    pii = _load_attr('pii-scan', doc_id)
    if pii:
        sec('PII risk', f"risk={pii.get('risk')} findings={len(pii.get('value') or [])}")
    parts.append("</div>")
    return "\n".join(parts)

def main():
    docs = sorted(_iter_docs(), key=lambda x: x['rel'].lower())
    comms = _load_comms()
    OUT_HTML.parent.mkdir(parents=True, exist_ok=True)
    now = dt.datetime.now().strftime('%Y-%m-%d %H:%M')
    # attribute coverage & rollups
    doc_ids = {d['doc_id'] for d in docs}
    attr_cov = {}
    attr_examples_missing = {}
    for plugin in PLUGINS:
        have = set()
        p = ATTR_DIR / plugin
        if p.exists():
            for f in p.glob('*.json'):
                have.add(f.stem)
        attr_cov[plugin] = {'have': len(have & doc_ids), 'total': len(doc_ids)}
        missing = list(doc_ids - have)
        attr_examples_missing[plugin] = missing[:5]
    # PII rollup
    pii_counts = {'high':0,'any':0}
    pii_dir = ATTR_DIR/'pii-scan'
    if pii_dir.exists():
        for f in pii_dir.glob('*.json'):
            try:
                d = json.loads(f.read_text(encoding='utf-8'))
                r = float(d.get('risk') or 0)
                if r>0: pii_counts['any']+=1
                if r>=0.8: pii_counts['high']+=1
            except Exception:
                continue
    # TODO rollup
    todo_nonempty = 0
    todo_dir = ATTR_DIR/'todo-items'
    if todo_dir.exists():
        for f in todo_dir.glob('*.json'):
            try:
                d = json.loads(f.read_text(encoding='utf-8'))
                if d.get('value'): todo_nonempty += 1
            except Exception:
                continue
    # requirements present/missing
    req_dir = ATTR_DIR/'requirements'
    req_have = set()
    if req_dir.exists():
        req_have = {f.stem for f in req_dir.glob('*.json')}
    req_missing = list((doc_ids - req_have))
    # queue summary (best effort)
    queue = {'present': (pathlib.Path('.knowledge/queues/jobs.sqlite').exists())}
    if queue['present']:
        try:
            con = sqlite3.connect('.knowledge/queues/jobs.sqlite')
            cur = con.cursor()
            by_status = {k: v for k, v in cur.execute("select status,count(*) from jobs group by status")}
            queue['by_status']=by_status; queue['total']=sum(by_status.values())
            con.close()
        except Exception:
            pass
    # prepare graph data (nodes/links), limited for performance
    graph_nodes = []
    graph_links = []
    if GRAPH_PATH.exists():
        # Build nodes/links
        name_to_idx = {}
        degree = {}
        edges = []
        for i, line in enumerate(GRAPH_PATH.read_text(encoding='utf-8').splitlines()):
            try:
                e = json.loads(line)
            except Exception:
                continue
            edges.append((e.get('src'), e.get('dst'), int(e.get('weight', 1))))
            if len(edges) >= 1500:  # cap edges to keep UI responsive
                break
        for u, v, w in edges:
            for n in (u, v):
                if n not in name_to_idx:
                    idx = len(graph_nodes)
                    name_to_idx[n] = idx
                    graph_nodes.append({'id': n, 'deg': 0, 'idx': idx})
            degree[u] = degree.get(u, 0) + 1
            degree[v] = degree.get(v, 0) + 1
            graph_links.append({'source': name_to_idx[u], 'target': name_to_idx[v], 'weight': w})
        for n in graph_nodes:
            n['deg'] = degree.get(n['id'], 0)
    # simple HTML
    graph_json = json.dumps({'nodes': graph_nodes, 'links': graph_links}, ensure_ascii=False)
    head = f"""
<!doctype html>
<html><head><meta charset='utf-8'>
<title>Knowledge Report</title>
<style>
body{{font-family:Segoe UI,Arial,sans-serif;margin:0;background:#fafafa;color:#222}}
header{{background:#0f62fe;color:#fff;padding:12px 16px;display:flex;align-items:center;gap:16px}}
header h1{{font-size:18px;margin:0}}
.sub{{opacity:.9;font-size:12px}}
.wrap{{padding:14px 16px}}
.row{{display:flex;gap:16px;flex-wrap:wrap}}
.col{{flex:1 1 380px;min-width:320px}}
.card{{background:#fff;border:1px solid #eee;border-radius:8px;margin:10px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}}
.hdr{{display:flex;justify-content:space-between;gap:12px;padding:10px 12px;border-bottom:1px solid #f0f0f0}}
.rel{{font-weight:600}}
.meta{{font-size:12px;color:#666}}
.sec{{padding:10px 12px;border-top:1px dashed #f2f2f2}}
.stit{{font-size:12px;color:#555;margin-bottom:6px;text-transform:uppercase;letter-spacing:.02em}}
.sbody pre{{white-space:pre-wrap;margin:0;font-family:ui-monospace,Consolas,monospace;font-size:13px;line-height:1.35}}
.pill{{display:inline-block;background:#eef5ff;color:#0f62fe;border-radius:999px;padding:2px 8px;margin:2px;font-size:12px}}
#q{{width:100%;padding:10px;border:1px solid #ddd;border-radius:8px;margin:10px 0}}
</style>
</head><body>
<script>window.GRAPH_DATA = {graph_json};</script>
<header><h1>Knowledge Report</h1><div class='sub'>generated {now}</div></header>
<div class='wrap'>
<input id='q' placeholder='Filter by file, domain or text... (client-side)'>
<div class='row'>
  <div class='col'>
    <div class='card'>
      <div class='hdr'><span class='rel'>Communities</span><span class='meta'>{len(comms)} items</span></div>
      <div class='sec'>
        {''.join(f"<div class='pill'>"+_esc(c.get('id'))+f" size={_esc(c.get('size'))}</div>" for c in comms)}
      </div>
      <div class='sec'>
        <div class='stit'>Top community keywords</div>
        <div class='sbody'>
          <ul>
          {''.join(f"<li>- { _esc(c.get('id')) } (size={ _esc(c.get('size')) }): { _esc(c.get('summary')) }</li>" for c in comms)}
          </ul>
        </div>
      </div>
    </div>
  </div>
  <div class='col'>
    <div class='card'>
      <div class='hdr'><span class='rel'>Coverage</span><span class='meta'>{len(doc_ids)} docs</span></div>
      <div class='sec'>
        {''.join(f"<div style='margin:6px 0'><div class='stit'>{_esc(k)}</div><div style='background:#eee;border-radius:6px;overflow:hidden'><div style='height:8px;width:{ (attr_cov[k]['have']*100/max(1,attr_cov[k]['total'])):.1f}%;background:#0f62fe'></div></div><div style='font-size:12px;color:#666'>{attr_cov[k]['have']}/{attr_cov[k]['total']}" + (" · missing: "+', '.join(f"<a href=#doc-{_esc(m)}>#{_esc(m)}</a>" for m in attr_examples_missing[k]) if attr_examples_missing[k] else '') + "</div></div>" for k in PLUGINS)}
      </div>
      <div class='sec'>
        <div class='stit'>PII & TODO</div>
        <div class='sbody' style='font-size:12px;color:#444'>PII high-risk: {pii_counts['high']} · PII any: {pii_counts['any']} · Files with TODOs: {todo_nonempty}</div>
      </div>
      <div class='sec'>
        <div class='stit'>Requirements</div>
        <div class='sbody' style='font-size:12px;color:#444'>present: {len(req_have)} · missing: {len(req_missing)} {('· e.g. '+', '.join(f"<a href=#doc-{_esc(m)}>#{_esc(m)}</a>" for m in req_missing[:5])) if req_missing else ''}</div>
      </div>
      <div class='sec'>
        <div class='stit'>Queue</div>
        <div class='sbody' style='font-size:12px;color:#444'>present: {queue.get('present')} · total: {queue.get('total','-')} · by_status: { _esc(queue.get('by_status','-')) }</div>
      </div>
    </div>
  </div>
  <div class='col'>
    <div class='card'>
      <div class='hdr'><span class='rel'>Graph (preview)</span><span class='meta'>{len(graph_nodes)} nodes · {len(graph_links)} edges</span></div>
      <div class='sec'>
        <div id='graphwrap' style='width:100%;height:380px;'>
          <svg id='graph' width='100%' height='360'></svg>
        </div>
        <div style='font-size:12px;color:#666'>Drag to move. Hover to highlight neighborhood. Filter: min degree <input type='range' id='mindeg' min='0' max='10' value='0'/> <span id='mindegv'>0</span></div>
      </div>
    </div>
  </div>
</div>
<div id='docs'>
"""
    cards = [ _render_doc_card(d) for d in docs ]
    tail = """
</div></div>
<script>
// Try local d3 first, then CDN; if all fail, render static graph
(function(){
  function onReady(){ if (typeof d3!=='undefined') { initGraph(); } else { renderStaticGraph(); } }
  function tryCdn(){
    var s=document.createElement('script'); s.src='https://cdnjs.cloudflare.com/ajax/libs/d3/7.9.0/d3.min.js';
    s.onload=onReady;
    s.onerror=function(){
      var s2=document.createElement('script'); s2.src='https://unpkg.com/d3@7/dist/d3.min.js'; s2.onload=onReady; s2.onerror=onReady; document.head.appendChild(s2);
    };
    document.head.appendChild(s);
  }
  var sl=document.createElement('script'); sl.src='d3.min.js'; sl.onload=onReady; sl.onerror=tryCdn; document.head.appendChild(sl);
  window.addEventListener('load', onReady);
})();

// guard flags to prevent double rendering
window.__graphInited = window.__graphInited || false;
window.__staticRendered = window.__staticRendered || false;

const GRAPH_DATA = (window.GRAPH_DATA && window.GRAPH_DATA.nodes) ? window.GRAPH_DATA : {nodes:[], links:[]};

function initGraph(){
  if (window.__graphInited) return; window.__graphInited = true;
  const svg = d3.select('#graph');
  if (!svg.node()) return;
  if (GRAPH_DATA.nodes.length===0){
    const w=document.getElementById('graphwrap');
    if(w){ w.innerHTML = "<div style='padding:8px;color:#666;font-size:12px'>No graph data available.</div>"; }
    return;
  }
  // clear any prior content (e.g., static fallback)
  svg.selectAll('*').remove();
  const wrap = document.getElementById('graphwrap');
  const width = (wrap && wrap.clientWidth) ? wrap.clientWidth : 640;
  const height = 360;
  svg.attr('viewBox', `0 0 ${width} ${height}`).attr('width', width).attr('height', height);
  const g = svg.append('g');
  const zoom = d3.zoom().on('zoom', (ev)=> g.attr('transform', ev.transform));
  svg.call(zoom);

  const color = d3.scaleOrdinal(d3.schemeCategory10);
  const deg = GRAPH_DATA.nodes.map(n=>n.deg);
  const maxDeg = Math.max(1, d3.max(deg));
  const rscale = d3.scaleSqrt().domain([0, maxDeg]).range([3, 14]);

  const nodes = GRAPH_DATA.nodes.map(d=>Object.assign({}, d));
  const rawLinks = GRAPH_DATA.links.map(d=>Object.assign({}, d));
  // Build link objects referencing node objects by index to avoid id mismatches
  const linkData = rawLinks.map(l=>({source: nodes[l.source], target: nodes[l.target], weight: l.weight}));

  // Seed positions in a circle for visibility before forces settle
  const R = Math.min(width, height) * 0.45;
  nodes.forEach((d,i)=>{ const a = (2*Math.PI*i)/nodes.length; d.x = width/2 + R*Math.cos(a); d.y = height/2 + R*Math.sin(a); });

  const link = g.append('g').attr('stroke', '#999').attr('stroke-opacity', 0.35)
      .selectAll('line').data(linkData).join('line').attr('stroke-width', d=>Math.max(1, Math.log(1+(d.weight||1))));
  const node = g.append('g').attr('stroke', '#fff').attr('stroke-width', 1)
      .selectAll('circle').data(nodes).join('circle')
      .attr('r', d=>rscale(d.deg)).attr('fill', (d,i)=> color(i%10)).call(drag(sim()));
  const label = g.append('g').selectAll('text').data(nodes).join('text')
      .text(d=>d.id).attr('font-size', '10px').attr('fill', '#444').attr('pointer-events', 'none');

  function sim(){
    const simulation = d3.forceSimulation(nodes)
      .force('link', d3.forceLink(linkData).distance(40).strength(0.2))
      .force('charge', d3.forceManyBody().strength(-80))
      .force('center', d3.forceCenter(width/2, height/2))
      .alpha(1).on('tick', ticked);
    function ticked(){
      link.attr('x1', d=>d.source.x).attr('y1', d=>d.source.y).attr('x2', d=>d.target.x).attr('y2', d=>d.target.y);
      node.attr('cx', d=>d.x).attr('cy', d=>d.y);
      label.attr('x', d=>d.x+6).attr('y', d=>d.y+3);
    }
    return simulation;
  }

  function drag(simulation){
    function dragstarted(event, d){ if (!event.active) simulation.alphaTarget(0.3).restart(); d.fx=d.x; d.fy=d.y; }
    function dragged(event, d){ d.fx=event.x; d.fy=event.y; }
    function dragended(event, d){ if (!event.active) simulation.alphaTarget(0); d.fx=null; d.fy=null; }
    return d3.drag().on('start', dragstarted).on('drag', dragged).on('end', dragended);
  }

  node.on('mouseover', (_, d)=>{
    const neigh = new Set([d.index]);
    linkData.forEach(l=>{ if(l.source.index===d.index) neigh.add(l.target.index); if(l.target.index===d.index) neigh.add(l.source.index); });
    node.attr('opacity', n=> neigh.has(n.index)?1:0.15);
    link.attr('opacity', l=> l.source.index===d.index||l.target.index===d.index?0.8:0.1);
    label.attr('opacity', n=> neigh.has(n.index)?1:0.1);
  }).on('mouseout', ()=>{
    node.attr('opacity', 1); link.attr('opacity', 0.35); label.attr('opacity', 1);
  });

  const slider = document.getElementById('mindeg');
  const sv = document.getElementById('mindegv');
  if (slider){
    slider.max = String(Math.max(10, Math.floor(maxDeg)));
    slider.addEventListener('input', ()=>{
      sv.textContent = slider.value;
      const minDeg = +slider.value;
      node.attr('display', d=> d.deg>=minDeg? null : 'none');
      label.attr('display', d=> d.deg>=minDeg? null : 'none');
      link.attr('display', l=> (l.source.deg>=minDeg && l.target.deg>=minDeg)? null : 'none');
    });
  }
  // Fallback: if nothing rendered for any reason, draw static
  // do not auto-render static here; static is reserved for no-d3 case
}

function renderStaticGraph(){
  if (window.__graphInited || window.__staticRendered) return; window.__staticRendered = true;
  const svg = document.getElementById('graph');
  const wrap = document.getElementById('graphwrap');
  if (!svg || !wrap) return;
  const width = (wrap && wrap.clientWidth) ? wrap.clientWidth : 640;
  const height = 360;
  svg.setAttribute('viewBox', `0 0 ${width} ${height}`);
  const ns = 'http://www.w3.org/2000/svg';
  // Clear
  while (svg.firstChild) svg.removeChild(svg.firstChild);
  const nodes = GRAPH_DATA.nodes.slice(0, 500); // cap
  const links = GRAPH_DATA.links.slice(0, 1500);
  const R = Math.min(width, height) * 0.42;
  nodes.forEach((d,i)=>{ const a = (2*Math.PI*i)/nodes.length; d.x = width/2 + R*Math.cos(a); d.y = height/2 + R*Math.sin(a); });
  // draw links
  links.forEach(l=>{
    const s = nodes[l.source]; const t = nodes[l.target]; if(!s||!t) return;
    const line = document.createElementNS(ns, 'line');
    line.setAttribute('x1', s.x); line.setAttribute('y1', s.y);
    line.setAttribute('x2', t.x); line.setAttribute('y2', t.y);
    line.setAttribute('stroke', '#bbb'); line.setAttribute('stroke-opacity', '0.5'); line.setAttribute('stroke-width', String(Math.max(1, Math.log(1+(l.weight||1)))));
    svg.appendChild(line);
  });
  // draw nodes + labels
  nodes.forEach((d,i)=>{
    const circ = document.createElementNS(ns, 'circle'); circ.setAttribute('cx', d.x); circ.setAttribute('cy', d.y); circ.setAttribute('r', String(3 + Math.min(12, (d.deg||0)/2))); circ.setAttribute('fill', '#0f62fe'); circ.setAttribute('stroke', '#fff'); circ.setAttribute('stroke-width','1'); svg.appendChild(circ);
    if (i < 400){ const tx = document.createElementNS(ns, 'text'); tx.setAttribute('x', d.x+6); tx.setAttribute('y', d.y+3); tx.setAttribute('font-size','10'); tx.setAttribute('fill','#444'); tx.textContent = d.id; svg.appendChild(tx); }
  });
}

const q = document.getElementById('q');
q.addEventListener('input', () => {
  const term = q.value.toLowerCase();
  document.querySelectorAll('#docs .card').forEach(card => {
    card.style.display = card.innerText.toLowerCase().includes(term) ? '' : 'none';
  });
});
</script>
</body></html>
"""
    html_out = head.replace('__NODES__', json.dumps(graph_nodes, ensure_ascii=False)) \
                  .replace('__LINKS__', json.dumps(graph_links, ensure_ascii=False)) \
               + "\n".join(cards) + tail
    OUT_HTML.write_text(html_out, encoding='utf-8')
    print(f"[report] wrote {OUT_HTML}")

if __name__ == '__main__':
    main()



./bin\test_lmstudio.py
import argparse
import os
import sys
import textwrap
from typing import Iterable

import requests

try:
    from kn.config import load_configs
except Exception:  # noqa: BLE001
    load_configs = None  # type: ignore

DEFAULT_PROMPT = "This is a connectivity test. Reply with a short acknowledgement."


def _print_header(title: str) -> None:
    print("\n" + "=" * len(title))
    print(title)
    print("=" * len(title))


def list_models(base_url: str, timeout: int = 5) -> list[str]:
    url = base_url.rstrip('/') + '/models'
    resp = requests.get(url, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    models = [m.get('id') for m in data.get('data', []) if m.get('id')]
    return models


def ping_model(base_url: str, model: str, prompt: str, timeout: int = 30, max_tokens: int = 32,
               temperature: float | None = None) -> tuple[int, str | None]:
    url = base_url.rstrip('/') + '/chat/completions'
    payload = {
        'model': model,
        'messages': [{'role': 'user', 'content': prompt}],
        'max_tokens': max_tokens,
    }
    if temperature is not None:
        payload['temperature'] = temperature
    resp = requests.post(url, json=payload, timeout=timeout)
    status = resp.status_code
    content = None
    if resp.ok:
        try:
            content = resp.json()['choices'][0]['message']['content']
        except Exception as exc:  # noqa: BLE001
            content = f"Failed to parse response: {exc}"  # type: ignore[assignment]
    else:
        try:
            content = resp.text
        except Exception:  # noqa: BLE001
            content = None
    return status, content


def check_dashboard(base_url: str, timeout: int = 5) -> dict[str, tuple[int, str]]:
    """Ping key dashboard endpoints and return {endpoint: (status, details)}."""
    results: dict[str, tuple[int, str]] = {}
    base = base_url.rstrip('/')

    def _record(name: str, response: requests.Response, body_preview: str | None = None) -> None:
        snippet = ""
        if body_preview is not None:
            snippet = body_preview
        else:
            try:
                snippet = textwrap.shorten(response.text or '', width=120, placeholder='…')
            except Exception:  # noqa: BLE001
                snippet = "<no body>"
        results[name] = (response.status_code, snippet)

    try:
        resp = requests.get(f"{base}/api/status", timeout=timeout)
        if resp.ok:
            body_preview = textwrap.shorten(resp.text, width=120, placeholder='…')
        else:
            body_preview = resp.text
        _record('GET /api/status', resp, body_preview)
    except Exception as exc:  # noqa: BLE001
        results['GET /api/status'] = (0, f"{exc}")

    try:
        resp = requests.get(f"{base}/api/queue/list?limit=5", timeout=timeout)
        _record('GET /api/queue/list', resp)
    except Exception as exc:  # noqa: BLE001
        results['GET /api/queue/list'] = (0, f"{exc}")

    try:
        payload = {'mode': 'reset-running'}
        resp = requests.post(f"{base}/api/queue/clear", json=payload, timeout=timeout)
        _record('POST /api/queue/clear', resp)
    except Exception as exc:  # noqa: BLE001
        results['POST /api/queue/clear'] = (0, f"{exc}")

    try:
        resp = requests.get(f"{base}/api/workers", timeout=timeout)
        _record('GET /api/workers', resp)
    except Exception as exc:  # noqa: BLE001
        results['GET /api/workers'] = (0, f"{exc}")

    return results


def summarize_results(title: str, items: Iterable[tuple[str, tuple[int, str]]]) -> None:
    _print_header(title)
    for name, (status, detail) in items:
        prefix = 'OK' if status == 200 else 'ERR'
        print(f"[{prefix}] {name} -> status={status} detail={detail}")


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Check LM Studio availability and dashboard endpoints.")
    parser.add_argument('--base-url', default=os.getenv('OPENAI_BASE_URL', 'http://127.0.0.1:12345/v1'),
                        help='LM Studio base URL (default: %(default)s or OPENAI_BASE_URL env)')
    parser.add_argument('--dashboard-url', default='http://127.0.0.1:5051',
                        help='Dashboard base URL (default: %(default)s)')
    parser.add_argument('--models', nargs='*', default=None,
                        help='Specific model IDs to probe. If absent, lists models only.')
    parser.add_argument('--prompt', default=DEFAULT_PROMPT, help='Prompt text for chat check.')
    parser.add_argument('--max-tokens', type=int, default=32, help='max_tokens for chat check (default: %(default)s)')
    parser.add_argument('--temperature', type=float, default=None,
                        help='Optional temperature value for chat checks.')
    parser.add_argument('--timeout', type=int, default=30, help='HTTP timeout in seconds (default: %(default)s)')
    parser.add_argument('--skip-dashboard', action='store_true', help='Skip dashboard API checks.')
    parser.add_argument('--plugins', nargs='*', default=None,
                        help='Plugin names (from config) to resolve into model IDs for chat checks.')
    parser.add_argument('--show-response', action='store_true', help='Print the full model response instead of a preview.')
    args = parser.parse_args(argv)

    base = args.base_url.rstrip('/')
    _print_header(f"LM Studio Diagnostics @ {base}")

    try:
        models = list_models(base, timeout=args.timeout)
    except requests.RequestException as exc:
        print(f"[ERROR] Could not reach {base + '/models'}: {exc}")
        return 1

    if not models:
        print("[WARN] No models returned by the server.")
    else:
        print("Available models:")
        for model in models:
            print(f"  - {model}")

    plugin_models: list[str] = []
    if args.plugins:
        if load_configs is None:
            print("[WARN] kn.config.load_configs not available; skipping plugin resolution.")
        else:
            cfg = load_configs()
            for plugin in args.plugins:
                p = cfg.get('plugins', {}).get(plugin)
                if not isinstance(p, dict):
                    print(f"[WARN] Plugin '{plugin}' not found in config; skipping")
                    continue
                llm_cfg = p.get('llm') if isinstance(p.get('llm'), dict) else {}
                model_id = llm_cfg.get('model') or cfg.get('llm', {}).get('model')
                if not model_id:
                    print(f"[WARN] Plugin '{plugin}' has no model configured; skipping")
                    continue
                plugin_models.append(model_id)
                print(f"Resolved plugin '{plugin}' -> model '{model_id}'")

    models_to_check = []
    seen = set()
    for model in [*(args.models or []), *plugin_models]:
        m = model.strip()
        if not m or m in seen:
            continue
        seen.add(m)
        models_to_check.append(m)

    if models_to_check:
        _print_header("Chat Checks")
        for model in models_to_check:
            model = model.strip()
            if not model:
                continue
            print(f"Testing model: {model}")
            try:
                status, content = ping_model(base, model, prompt=args.prompt,
                                             timeout=args.timeout, max_tokens=args.max_tokens,
                                             temperature=args.temperature)
            except requests.RequestException as exc:
                print(f"  [ERROR] Request failed: {exc}")
                continue

            if status == 200:
                if args.show_response:
                    print(f"  [OK] status=200 response={content!r}")
                else:
                    preview = textwrap.shorten(content or '', width=120, placeholder='…')
                    print(f"  [OK] status=200 response={preview!r} (use --show-response to display full text)")
            else:
                snippet = textwrap.shorten(content or '', width=120, placeholder='…')
                print(f"  [FAIL] status={status} body={snippet!r}")
    else:
        print("\nNo specific models requested for chat check. Use --models or --plugins to verify.")

    if not args.skip_dashboard:
        dashboard = args.dashboard_url.rstrip('/')
        results = check_dashboard(dashboard, timeout=args.timeout)
        summarize_results(f"Dashboard Diagnostics @ {dashboard}", sorted(results.items()))

    return 0


if __name__ == '__main__':  # pragma: no cover
    raise SystemExit(main())


./bin\test_llm_gateway.py
import argparse
import threading
import time
import sys

from kn.config import load_configs
from kn.llm_gateway.runner import run_gateway_service
from kn.llm_gateway.client import submit_chat_request
from kn.llm_gateway.errors import GatewayError, QueueTimeoutError, RequestRejectedError


def _run_gateway(service: str, cfg):
    run_gateway_service(service, loop_forever=True, cfg=cfg)


def main(argv=None):
    parser = argparse.ArgumentParser(description="Smoke-test the LLM gateway by issuing a simple chat request.")
    parser.add_argument("prompt", nargs="?", default="Reply with OK.", help="Prompt to send through the gateway")
    parser.add_argument("--service", default=None, help="Service name to target (defaults to llm.service from config)")
    parser.add_argument("--model", default=None, help="Override model name for the request")
    parser.add_argument("--boot-delay", type=float, default=0.5, help="Seconds to wait after starting the gateway thread")
    args = parser.parse_args(argv)

    cfg = load_configs()
    service = args.service or cfg.get("llm", {}).get("service", "lmstudio")

    gateway_thread = threading.Thread(target=_run_gateway, args=(service, cfg), daemon=True)
    gateway_thread.start()
    time.sleep(max(args.boot_delay, 0))

    overrides = {"model": args.model} if args.model else None
    try:
        response = submit_chat_request(
            args.prompt,
            overrides=overrides,
            cfg=cfg,
        )
    except (QueueTimeoutError, RequestRejectedError, GatewayError) as err:
        print(f"Gateway call failed: {err}")
        return 1
    print(response)
    return 0


if __name__ == "__main__":
    sys.exit(main())








./bin\llm_gateway_service.py
import argparse
import logging
import sys

from kn.config import load_configs
from kn.llm_gateway.runner import run_gateway_service


def main(argv=None):
    parser = argparse.ArgumentParser(description="Run the LLM gateway service loop")
    parser.add_argument("--service", default=None, help="Service name (defaults to llm.service from config)")
    parser.add_argument("--log-level", default="INFO", help="Logging level (INFO, DEBUG, ...)")
    parser.add_argument("--once", action="store_true", help="Process a single request and exit")
    args = parser.parse_args(argv)

    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO), format="[%(asctime)s] %(levelname)s %(name)s: %(message)s")

    cfg = load_configs()
    service = args.service or cfg.get("llm", {}).get("service", "lmstudio")
    run_gateway_service(service_name=service, loop_forever=not args.once, cfg=cfg)


if __name__ == "__main__":
    sys.exit(main())


./bin\jobs_reset.py
import argparse
import sys

from kn.config import load_configs
from kn.jobs_sqlite import reset_running_jobs, reset_counter, get_counter


def main(argv=None):
    parser = argparse.ArgumentParser(description="Reset job status and counters for the enrichment queue")
    parser.add_argument("--status", default="pending", help="Status to set for running jobs (default: pending)")
    parser.add_argument("--counter", default="llm_concurrency", help="Counter name to reset (default: llm_concurrency)")
    parser.add_argument("--no-counter", action="store_true", help="Skip resetting the counter")
    args = parser.parse_args(argv)

    cfg = load_configs()

    ids = reset_running_jobs(cfg, status=args.status)
    print(f"Reset {len(ids)} jobs from 'running' to '{args.status}'")

    if not args.no_counter:
        before = get_counter(cfg, args.counter)
        reset_counter(cfg, args.counter)
        after = get_counter(cfg, args.counter)
        print(f"Counter '{args.counter}' reset (was {before}, now {after})")


if __name__ == "__main__":
    sys.exit(main())


./bin\plan_enqueue.py
import argparse, pathlib, time, json
from kn.config import load_configs
from kn.jobs_sqlite import ensure_db, enqueue

ATTR_ROOT = pathlib.Path('.knowledge/indexes/attributes')
CHUNKS_DIR = pathlib.Path('.knowledge/indexes/chunks')

def _plugin_dir_name(plugin: str) -> str:
    if plugin == 'summaries':
        return 'summaries'
    return plugin.replace('_', '-')


def already_has(plugin: str, doc_id: str) -> bool:
    dir_name = _plugin_dir_name(plugin)
    if dir_name == 'summaries':
        pdir = ATTR_ROOT / 'summaries'
        if not pdir.exists():
            return False
        return any(pp.name.startswith(f"{doc_id}_") for pp in pdir.glob(f"{doc_id}_*.json"))
    p = ATTR_ROOT / dir_name / f"{doc_id}.json"
    return p.exists()

def iter_doc_ids():
    seen = set()
    for p in CHUNKS_DIR.glob('*.json'):
        doc_id = p.stem.split('-')[0]
        if doc_id not in seen:
            seen.add(doc_id)
            yield doc_id

def latest_mtime_for_doc(doc_id: str) -> float:
    mt = 0.0
    for p in CHUNKS_DIR.glob(f"{doc_id}-*.json"):
        mt = max(mt, p.stat().st_mtime)
    return mt

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--plugins', required=False, default='glossary,requirements,todo-items,faq-pairs', help='comma-separated plugin names')
    ap.add_argument('--limit', type=int, default=0, help='max jobs to enqueue in total (0 = unlimited)')
    ap.add_argument('--only-missing', action='store_true', help='enqueue only if attribute output file missing')
    ap.add_argument('--changed-since-min', type=int, default=0, help='only enqueue docs whose chunks changed in last N minutes (0 = ignore)')
    ap.add_argument('--summaries-modes', default='short,medium', help='for plugin "summaries", comma list of modes (short,medium,long,outline)')
    ap.add_argument('--payload-json', default=None, help='optional JSON to attach as payload to each job (e.g., {"llm":{"model":"qwen2.5-7b-instruct","timeout":60}})')
    ap.add_argument('--map-reduce', action='store_true', help='enqueue chunk map (chunk-summary) then doc reduce (doc-reduce)')
    ap.add_argument('--doc-ids', default=None, help='comma-separated list of doc ids; if provided, only these docs are enqueued')
    args = ap.parse_args()

    cfg = load_configs()
    ensure_db(cfg)
    plugins = [p.strip() for p in args.plugins.split(',') if p.strip()]
    sum_modes = [m.strip() for m in args.summaries_modes.split(',') if m.strip()] if 'summaries' in plugins else []
    try:
        base_payload = json.loads(args.payload_json) if args.payload_json else {}
    except Exception:
        base_payload = {}

    cutoff = None
    if args.changed_since_min and args.changed_since_min > 0:
        cutoff = time.time() - args.changed_since_min * 60

    if args.doc_ids:
        doc_list = list({d.strip(): None for d in args.doc_ids.split(',') if d.strip()}.keys())
    else:
        doc_list = list(iter_doc_ids())

    count = 0
    for doc_id in doc_list:
        if cutoff is not None and latest_mtime_for_doc(doc_id) < cutoff:
            continue
        for plugin in plugins:
            if args.only_missing and already_has(plugin, doc_id):
                continue
            if plugin == 'summaries' and sum_modes:
                for mode in sum_modes:
                    pl = dict(base_payload); pl['mode'] = mode
                    enqueue(cfg, plugin, doc_id, payload=pl)
                    count += 1
            else:
                enqueue(cfg, plugin, doc_id, payload=base_payload)
                count += 1
        if args.map_reduce:
            # enqueue chunk map jobs for all chunks of this doc, then doc reduce
            from pathlib import Path
            cdir = Path('.knowledge/indexes/chunks')
            chunks = sorted(cdir.glob(f"{doc_id}-*.json"))
            for cp in chunks:
                chk_id = cp.stem
                pl = dict(base_payload); pl['chunk_id'] = chk_id
                enqueue(cfg, 'chunk-summary', doc_id, payload=pl)
                count += 1
            enqueue(cfg, 'doc-reduce', doc_id, payload=base_payload)
            count += 1
            if args.limit and count >= args.limit:
                print(f"[plan] enqueued {count} jobs (limit reached)")
                return
    print(f"[plan] enqueued {count} jobs for plugins: {', '.join(plugins)}")

if __name__ == '__main__':
    main()



./bin\dashboard_server.py
import copy
import json, pathlib, threading, time, os, sqlite3, collections, datetime as dt, subprocess, sys
import yaml
from flask import Flask, send_from_directory, jsonify, request

from kn.config import load_configs
from kn.jobs_sqlite import ensure_db, enqueue
from kn.llm_gateway.client import submit_generic_request
from kn.llm_gateway.storage import QueueStorage

ROOT = pathlib.Path('.knowledge')
BASE_DIR = pathlib.Path(__file__).resolve().parent
STATIC_DIR = BASE_DIR / 'dashboard_static'

app = Flask(__name__, static_folder=str(STATIC_DIR))


from collections import deque
import itertools, datetime
_WORKER_ID_GEN = itertools.count(1)
WORKERS: dict[int, dict] = {}
GATEWAY = {
    'proc': None,
    'log': deque(maxlen=500),
    'service': None,
    'started_at': None,
}


def _gateway_running() -> bool:
    proc = GATEWAY.get('proc')
    return proc is not None and proc.poll() is None


def _spawn_gateway(service: str | None = None, log_level: str = 'INFO') -> bool:
    if _gateway_running():
        return False
    cfg = load_configs()
    service_name = service or cfg.get('llm', {}).get('service', 'lmstudio')
    args = [sys.executable, '-X', 'utf8', 'bin/llm_gateway_service.py']
    if log_level:
        args.extend(['--log-level', log_level])
    if service_name:
        args.extend(['--service', service_name])
    env = os.environ.copy()
    env['PYTHONPATH'] = str(pathlib.Path.cwd())
    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)
    GATEWAY['proc'] = proc
    GATEWAY['service'] = service_name
    GATEWAY['started_at'] = datetime.datetime.now().isoformat(timespec='seconds')
    GATEWAY['log'].clear()

    def _reader():
        try:
            for line in proc.stdout:
                GATEWAY['log'].append(line.rstrip())
        except Exception:
            pass

    threading.Thread(target=_reader, daemon=True).start()
    return True


def _stop_gateway() -> bool:
    proc = GATEWAY.get('proc')
    if not proc:
        return False
    if proc.poll() is None:
        try:
            proc.terminate()
        except Exception:
            pass
    GATEWAY['proc'] = None
    GATEWAY['started_at'] = None
    return True


def _gateway_status() -> dict:
    cfg = load_configs()
    service_name = GATEWAY.get('service') or cfg.get('llm', {}).get('service', 'lmstudio')
    queue_cfg = cfg.get('llm_gateway', {})
    queue_path = pathlib.Path(queue_cfg.get('queue_path', str(ROOT / 'queues' / 'llm_gateway.sqlite')))
    stats = {}
    try:
        storage = QueueStorage(queue_path)
        stats = storage.stats(service_name if service_name else None)
    except Exception:
        stats = {}
    return {
        'running': _gateway_running(),
        'service': service_name,
        'started_at': GATEWAY.get('started_at'),
        'queue': stats,
        'log_tail': list(GATEWAY['log'])[-50:],
    }

CHUNKS_DIR = pathlib.Path('.knowledge/indexes/chunks')
ATTR_DIR = pathlib.Path('.knowledge/indexes/attributes')
DB_PATH = pathlib.Path('.knowledge/queues/jobs.sqlite')
DOC_ATTR_PLUGINS = [
    'summary-20w','topic-tags','pii-scan','glossary','requirements',
    'todo-items','faq-pairs','keyphrases','bridge-candidates','risk-scan',
    'recent-summary','summary-short','summary-medium','summary-long','summary-outline'
]


def _handle_worker_line(wid: int, line: str):
    w = WORKERS.get(wid)
    if not w:
        return
    if line.startswith('[worker-current]'):
        # expected format: [worker-current] plugin=foo docs=doc1,doc2,…
        try:
            rest = line.split(']', 1)[1].strip()
            parts = {}
            for part in rest.split():
                if '=' in part:
                    k, v = part.split('=', 1)
                    parts[k] = v
            w['current'] = {
                'plugin': parts.get('plugin'),
                'docs': parts.get('docs'),
                'since': datetime.datetime.now().isoformat(timespec='seconds'),
            }
        except Exception:
            w['current'] = {'raw': line}
    elif line.startswith('[worker-current-clear]'):
        w['current'] = None

def _unique_doc_ids():
    seen = set()
    if not CHUNKS_DIR.exists():
        return seen
    for p in CHUNKS_DIR.glob('*.json'):
        try:
            doc_id = p.stem.split('-')[0]
            seen.add(doc_id)
        except Exception:
            continue
    return seen

def _attribute_coverage(plugins, doc_ids):
    coverage = {}
    for plugin in plugins:
        p = ATTR_DIR / plugin
        have = 0
        missing = []
        failed = 0
        failed_examples = []
        if p.exists():
            existing = {f.stem for f in p.glob('*.json')}
            for d in doc_ids:
                if d in existing:
                    fpath = p / f"{d}.json"
                    try:
                        data = json.loads(fpath.read_text(encoding='utf-8'))
                    except Exception:
                        failed += 1
                        if len(failed_examples) < 5:
                            failed_examples.append({'doc_id': d, 'error': 'invalid json'})
                        continue
                    value = data.get('value')
                    error = data.get('error')
                    unavailable = isinstance(value, str) and 'unavailable' in value.lower()
                    if error or unavailable:
                        failed += 1
                        if len(failed_examples) < 5:
                            failed_examples.append({'doc_id': d, 'error': error or value})
                    else:
                        have += 1
                else:
                    missing.append(d)
        else:
            missing = list(doc_ids)
        total = len(doc_ids)
        coverage[plugin] = {
            'total_docs': total,
            'have': have,
            'failed': failed,
            'pct': round(100.0 * (have / total), 1) if total else 0.0,
            'missing_examples': missing[:5],
            'failed_examples': failed_examples[:5],
        }
    return coverage

def _domains_breakdown():
    from json import loads
    dom = collections.Counter()
    for p in CHUNKS_DIR.glob('*.json'):
        try:
            rec = loads(p.read_text(encoding='utf-8'))
            d = (rec.get('meta') or {}).get('domain') or 'root'
            doc_id = rec.get('doc_id')
            dom[(doc_id, d)] += 1
        except Exception:
            continue
    out = collections.Counter()
    for (_, d), _ in dom.items():
        out[d] += 1
    return [{'domain': k, 'docs': v} for k, v in out.most_common()]

def _db_summary(minutes_recent=60):
    if not DB_PATH.exists():
        return {
            'present': False,
            'total': 0,
            'by_status': {},
            'by_plugin_status': {},
            'oldest_pending_min': None,
            'recent_created': {},
            'recent_failed': {},
        }
    con = sqlite3.connect(DB_PATH)
    con.row_factory = sqlite3.Row
    cur = con.cursor()
    out = {'present': True}
    try:
        by_status = {k: v for k, v in cur.execute(
            "select status, count(*) from jobs group by status").fetchall()}
        out['by_status'] = by_status
        total = sum(by_status.values()) if by_status else 0
        out['total'] = total
        plugin_status = {}
        for row in cur.execute(
            "select plugin, status, count(*) as n from jobs group by plugin, status"):
            plugin_status.setdefault(row['plugin'], {})[row['status']] = row['n']
        out['by_plugin_status'] = plugin_status
        row = cur.execute(
            "select created_at from jobs where status='pending' order by id asc limit 1").fetchone()
        if row and row['created_at']:
            try:
                created = dt.datetime.fromisoformat(str(row['created_at']))
                age_min = (dt.datetime.now() - created).total_seconds() / 60.0
            except Exception:
                age_min = None
        else:
            age_min = None
        out['oldest_pending_min'] = round(age_min, 1) if age_min is not None else None
        recent = {}
        try:
            cutoff = dt.datetime.now() - dt.timedelta(minutes=minutes_recent)
            cutoff_iso = cutoff.isoformat(sep=' ')
            for row in cur.execute(
                "select plugin, count(*) as n from jobs where created_at >= ? group by plugin",
                (cutoff_iso,)):
                recent[row['plugin']] = row['n']
        except Exception:
            recent = {}
        out['recent_created'] = recent

        # recent failed per plugin
        recent_failed = {}
        try:
            cutoff = dt.datetime.now() - dt.timedelta(minutes=minutes_recent)
            cutoff_iso = cutoff.isoformat(sep=' ')
            for row in cur.execute(
                "select plugin, count(*) as n from jobs where status='failed' and completed_at >= ? group by plugin",
                (cutoff_iso,)):
                recent_failed[row['plugin']] = row['n']
        except Exception:
            recent_failed = {}
        out['recent_failed'] = recent_failed
    finally:
        con.close()
    return out

LLM_MODELS_CACHE = {'data': None, 'ts': 0}

def _llm_health(force: bool=False, ttl_sec: int=600):
    now = time.time()
    if not force and LLM_MODELS_CACHE['data'] and (now - LLM_MODELS_CACHE['ts'] < ttl_sec):
        return copy.deepcopy(LLM_MODELS_CACHE['data'])

    cfg = load_configs()
    service = cfg.get('llm', {}).get('service', 'lmstudio')
    service_cfg = (cfg.get('llm_services') or {}).get(service, {})
    model = service_cfg.get('default_model') or cfg.get('llm', {}).get('model')
    try:
        wait_timeout = float((cfg.get('llm_gateway') or {}).get('health_timeout_sec', 15))
        resp = submit_generic_request(
            service=service,
            payload={'action': 'health', 'model': model},
            metadata={'source': 'dashboard.health'},
            cfg=cfg,
            wait=True,
            timeout_override=wait_timeout,
        )
        raw = resp.raw or {}
        data = {
            'reachable': resp.success,
            'endpoint': service_cfg.get('base_url') or cfg.get('llm', {}).get('base_url'),
            'models': (raw.get('available_models') or [])[:5],
        }
        if not resp.success:
            data['error'] = resp.error or raw.get('error')
        LLM_MODELS_CACHE['data'] = data
        LLM_MODELS_CACHE['ts'] = now
        return data
    except Exception as exc:
        data = {
            'reachable': False,
            'endpoint': service_cfg.get('base_url') or cfg.get('llm', {}).get('base_url'),
            'models': [],
            'error': str(exc),
        }
        LLM_MODELS_CACHE['data'] = data
        LLM_MODELS_CACHE['ts'] = now
        return data

def _read_jobs_status():
    plugins = DOC_ATTR_PLUGINS
    doc_ids = _unique_doc_ids()
    data = {
        'docs_total': len(doc_ids),
        'domains': _domains_breakdown(),
        'attributes_coverage': _attribute_coverage(plugins, doc_ids),
        'queue': _db_summary(60),
        'llm': _llm_health(),
        'gateway': _gateway_status(),
    }
    return data


@app.post('/api/enqueue')
def api_enqueue():
    body = request.get_json(force=True, silent=True) or {}
    plugins = body.get('plugins') or []
    doc_ids = body.get('doc_ids') or []
    payload = body.get('payload')
    force = body.get('force')
    overwrite = body.get('overwrite')
    if payload is None:
        payload = {}
    if force is True:
        payload.setdefault('force', True)
    if overwrite is True:
        payload['overwrite'] = True
    cfg = load_configs(); ensure_db(cfg)
    count = 0
    for d in doc_ids:
        for p in plugins:
            enqueue(cfg, p, d, dict(payload))
            count += 1
    return jsonify({'enqueued': count})

@app.post('/api/plan')
def api_plan():
    # spawn plan_enqueue.py as a subprocess to avoid import/package issues
    args = request.get_json(force=True, silent=True) or {}
    cmd = [sys.executable, '-X', 'utf8', 'bin/plan_enqueue.py']
    if 'plugins' in args:
        cmd += ['--plugins', ','.join(args['plugins'])]
    if args.get('only_missing'):
        cmd += ['--only-missing']
    if args.get('limit'):
        cmd += ['--limit', str(args['limit'])]
    if args.get('changed_since_min'):
        cmd += ['--changed-since-min', str(args['changed_since_min'])]
    if args.get('summaries_modes'):
        cmd += ['--summaries-modes', ','.join(args['summaries_modes'])]
    if args.get('map_reduce'):
        cmd += ['--map-reduce']
    if args.get('doc_ids'):
        cmd += ['--doc-ids', ','.join(args['doc_ids'])]
    if args.get('payload_json'):
        cmd += ['--payload-json', args['payload_json']]
    env = os.environ.copy(); env['PYTHONPATH'] = str(pathlib.Path.cwd())
    subprocess.run(cmd, env=env)
    return jsonify({'ok': True})

def _spawn_worker(plugins: list[str], batch: int, watch: bool=True):
    args = [sys.executable, '-X', 'utf8', 'bin/enrich_worker.py', '--plugins', ','.join(plugins), '--batch', str(batch)]
    if watch:
        args.append('--watch')
    env = os.environ.copy(); env['PYTHONPATH'] = str(pathlib.Path.cwd())
    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)
    wid = next(_WORKER_ID_GEN)
    wlog = deque(maxlen=500)
    WORKERS[wid] = {
        'proc': proc,
        'plugins': plugins,
        'batch': batch,
        'watch': watch,
        'started_at': datetime.datetime.now().isoformat(timespec='seconds'),
        'log': wlog,
        'current': None,
    }
    def _reader():
        try:
            for line in proc.stdout:
                clean = line.rstrip()
                wlog.append(clean)
                _handle_worker_line(wid, clean)
        except Exception:
            pass
    threading.Thread(target=_reader, daemon=True).start()
    return wid

@app.post('/api/worker/start')
def api_worker_start():
    body = request.get_json(force=True, silent=True) or {}
    plugins = body.get('plugins') or ['summaries','keyphrases','bridge-candidates','risk-scan','recent-summary']
    batch = int(body.get('batch') or 32)
    wid = _spawn_worker(plugins, batch, watch=True)
    return jsonify({'started': True, 'worker_id': wid})

@app.post('/api/worker/stop')
def api_worker_stop():
    body = request.get_json(force=True, silent=True) or {}
    wid = body.get('id') or body.get('worker_id')
    if wid is None:
        # stop all
        for wid2 in list(WORKERS.keys()):
            _stop_worker_id(wid2)
        return jsonify({'stopped_all': True})
    ok = _stop_worker_id(int(wid))
    return jsonify({'stopped': ok, 'worker_id': wid})


@app.post('/api/gateway/start')
def api_gateway_start():
    body = request.get_json(force=True, silent=True) or {}
    service = body.get('service')
    log_level = body.get('log_level', 'INFO')
    started = _spawn_gateway(service=service, log_level=log_level)
    status = _gateway_status()
    status['started'] = started
    return jsonify(status)


@app.post('/api/gateway/stop')
def api_gateway_stop():
    stopped = _stop_gateway()
    status = _gateway_status()
    status['stopped'] = stopped
    return jsonify(status)

@app.get('/api/workers')
def api_workers():
    out = []
    for wid, w in WORKERS.items():
        proc = w.get('proc')
        out.append({
            'id': wid,
            'pid': proc.pid if proc else None,
            'running': (proc is not None and proc.poll() is None),
            'plugins': w.get('plugins'),
            'batch': w.get('batch'),
            'watch': w.get('watch'),
            'started_at': w.get('started_at'),
        })
    return jsonify({'workers': out})

@app.get('/api/queue/list')
def api_queue_list():
    try:
        limit = int(request.args.get('limit', '200'))
    except Exception:
        limit = 200
    status = request.args.get('status')  # optional: pending|running|done
    plugin = request.args.get('plugin')  # optional
    if not DB_PATH.exists():
        return jsonify({'items': []})
    con = sqlite3.connect(DB_PATH)
    con.row_factory = sqlite3.Row
    cur = con.cursor()
    q = "select id, plugin, doc_id, status, created_at from jobs"
    cond = []
    args = []
    if status:
        cond.append("status=?"); args.append(status)
    if plugin:
        cond.append("plugin=?"); args.append(plugin)
    if cond:
        q += " where " + " and ".join(cond)
    q += " order by id asc limit ?"; args.append(limit)
    rows = [dict(r) for r in cur.execute(q, args).fetchall()]
    con.close()
    return jsonify({'items': rows})


@app.get('/api/queue/job/<int:job_id>')
def api_queue_job(job_id: int):
    if not DB_PATH.exists():
        return jsonify({'error': 'queue database not found'}), 404
    con = sqlite3.connect(DB_PATH)
    con.row_factory = sqlite3.Row
    cur = con.cursor()
    row = cur.execute(
        "select id, plugin, doc_id, status, payload, retries, last_error, created_at, completed_at from jobs where id=?",
        (job_id,),
    ).fetchone()
    con.close()
    if not row:
        return jsonify({'error': 'job not found', 'id': job_id}), 404
    payload = row['payload']
    try:
        payload = json.loads(payload) if payload else None
    except Exception:
        payload = {'raw': payload}
    return jsonify({
        'id': row['id'],
        'plugin': row['plugin'],
        'doc_id': row['doc_id'],
        'status': row['status'],
        'payload': payload,
        'retries': row['retries'],
        'last_error': row['last_error'],
        'created_at': row['created_at'],
        'completed_at': row['completed_at'],
    })

@app.post('/api/queue/clear')
def api_queue_clear():
    mode = (request.get_json(force=True, silent=True) or {}).get('mode') or 'non-done'
    app.logger.info('[queue-clear] mode=%s', mode)
    if not DB_PATH.exists():
        return jsonify({'ok': True, 'cleared': 0, 'mode': mode})
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cleared = 0
    try:
        if mode == 'all':
            cur.execute('delete from jobs')
            cleared = cur.rowcount
        elif mode in ('reset-running', 'reset-running-counter'):
            cur.execute("update jobs set status='pending', retries=coalesce(retries,0)+1 where status='running'")
            cleared = cur.rowcount
            if mode == 'reset-running-counter':
                from kn.jobs_sqlite import reset_counter
                reset_counter(load_configs(), 'llm_concurrency')
        elif mode == 'pending':
            cur.execute("delete from jobs where status='pending'")
            cleared = cur.rowcount
        elif mode == 'non-done':
            cur.execute("delete from jobs where status!='done'")
            cleared = cur.rowcount
        else:
            return jsonify({'error': 'unknown mode', 'mode': mode}), 400
        con.commit()
    finally:
        con.close()
    return jsonify({'ok': True, 'cleared': int(cleared), 'mode': mode})


# Plugin defaults (persisted to .knowledge/config/models.yml under 'plugins')

# Cache management
CACHE_DIR = pathlib.Path('.knowledge/cache/llm')

@app.get('/api/cache/llm/stats')
def api_cache_stats():
    total = 0; count = 0
    if CACHE_DIR.exists():
        for p in CACHE_DIR.glob('*.json'):
            try:
                total += p.stat().st_size
                count += 1
            except Exception:
                continue
    return jsonify({'count': count, 'total_bytes': total})

@app.get('/api/cache/llm/list')
def api_cache_list():
    try:
        limit = int(request.args.get('limit', '200'))
    except Exception:
        limit = 200
    items = []
    if CACHE_DIR.exists():
        files = sorted(CACHE_DIR.glob('*.json'), key=lambda p: p.stat().st_mtime, reverse=True)
        for p in files[:limit]:
            try:
                st = p.stat()
                items.append({'name': p.name, 'size': st.st_size, 'mtime': dt.datetime.fromtimestamp(st.st_mtime).isoformat(sep=' ')})
            except Exception:
                continue
    return jsonify({'items': items})

@app.post('/api/cache/llm/clear')
def api_cache_clear():
    cleared = 0
    if CACHE_DIR.exists():
        for p in CACHE_DIR.glob('*.json'):
            try:
                p.unlink(); cleared += 1
            except Exception:
                continue
    return jsonify({'ok': True, 'cleared': cleared})


# --------------
# Document + attribute browsing APIs
# --------------

def _first_chunk_for_doc(doc_id: str):
    for p in CHUNKS_DIR.glob(f"{doc_id}-*.json"):
        try:
            return json.loads(p.read_text(encoding='utf-8'))
        except Exception:
            continue
    return None

@app.get('/api/docs')
def api_docs():
    docs = []
    total = len(DOC_ATTR_PLUGINS)
    for d in sorted(_unique_doc_ids()):
        rec = _first_chunk_for_doc(d) or {}
        meta = rec.get('meta') or {}
        attrs = _attr_paths_for_doc(d)
        have = sum(1 for plugin in DOC_ATTR_PLUGINS if isinstance(attrs.get(plugin), dict))
        docs.append({
            'doc_id': d,
            'domain': meta.get('domain','root'),
            'path': meta.get('path',''),
            'coverage': {'have': have, 'total': total}
        })
    return jsonify({'docs': docs, 'coverage_total': total, 'runnable_plugins': DOC_ATTR_PLUGINS})

@app.post('/api/ingest')
def api_ingest():
    body = request.get_json(force=True, silent=True) or {}
    repo = body.get('repo') or body.get('path')
    full = bool(body.get('full', True))
    if not repo:
        return jsonify({'error':'missing repo path'}), 400
    # spawn ingest_build_graph.py
    env = os.environ.copy(); env['PYTHONPATH'] = str(pathlib.Path.cwd())
    args = [sys.executable, '-X', 'utf8', 'bin/ingest_build_graph.py', '--repo', repo]
    if full:
        args.append('--full')
    subprocess.Popen(args, env=env)
    return jsonify({'ok': True, 'started': True, 'repo': repo, 'full': full})

def _attr_paths_for_doc(doc_id: str):
    out = {}
    if ATTR_DIR.exists():
        for plugin_dir in ATTR_DIR.iterdir():
            if not plugin_dir.is_dir():
                continue
            if plugin_dir.name == 'summaries':
                modes = []
                for f in plugin_dir.glob(f"{doc_id}_*.json"):
                    try:
                        mode = f.stem.split('_', 1)[1]
                    except Exception:
                        continue
                    try:
                        mt = f.stat().st_mtime
                    except Exception:
                        mt = None
                    modes.append({'mode': mode, 'path': str(f), 'mtime': mt})
                if modes:
                    out['summaries'] = {'modes': modes, 'runnable': False}
            else:
                f = plugin_dir / f"{doc_id}.json"
                if f.exists():
                    try:
                        mt = f.stat().st_mtime
                    except Exception:
                        mt = None
                    out[plugin_dir.name] = {
                        'path': str(f),
                        'mtime': mt,
                        'runnable': plugin_dir.name in DOC_ATTR_PLUGINS,
                    }
    return out

@app.get('/api/doc/<doc_id>')
def api_doc(doc_id):
    rec = _first_chunk_for_doc(doc_id) or {}
    meta = rec.get('meta') or {}
    attrs = _attr_paths_for_doc(doc_id)
    return jsonify({'doc_id': doc_id, 'meta': meta, 'attributes': attrs})

@app.get('/api/doc/<doc_id>/attr/<plugin>')
def api_doc_attr(doc_id, plugin):
    if plugin == 'summaries':
        pdir = ATTR_DIR / 'summaries'
        items = []
        for f in sorted(pdir.glob(f"{doc_id}_*.json")):
            try:
                data = json.loads(f.read_text(encoding='utf-8'))
                items.append(data)
            except Exception:
                continue
        return jsonify({'items': items})
    p = ATTR_DIR / plugin / f"{doc_id}.json"
    if not p.exists():
        return jsonify({'error':'not found'}), 404
    try:
        data = json.loads(p.read_text(encoding='utf-8'))
    except Exception:
        data = {'raw': p.read_text(errors='ignore')}
    return jsonify({'item': data})

# Chunk-level attributes per document
def _list_chunk_attrs_for_doc(doc_id: str, plugin: str):
    items = []
    pdir = ATTR_DIR / plugin
    if not pdir.exists():
        return []
    for f in sorted(pdir.glob('*.json')):
        try:
            data = json.loads(f.read_text(encoding='utf-8'))
        except Exception:
            continue
        if data.get('doc_id') != doc_id:
            continue
        chunk_id = data.get('chunk_id')
        seq = None
        try:
            if chunk_id:
                cpath = CHUNKS_DIR / f"{chunk_id}.json"
                if cpath.exists():
                    crec = json.loads(cpath.read_text(encoding='utf-8'))
                    seq = ((crec.get('meta') or {}).get('seq'))
        except Exception:
            pass
        val = data.get('value')
        preview = None
        if isinstance(val, str):
            preview = val.strip().replace('\n',' ')[:200]
        items.append({'chunk_id': chunk_id, 'seq': seq, 'path': str(f), 'preview': preview})
    # order by seq when available, else by chunk_id
    def _key(it):
        if isinstance(it.get('seq'), int):
            return (0, it['seq'])
        return (1, str(it.get('chunk_id') or ''))
    items.sort(key=_key)
    return items

@app.get('/api/doc/<doc_id>/chunks/attr/<plugin>')
def api_doc_chunk_attrs(doc_id, plugin):
    items = _list_chunk_attrs_for_doc(doc_id, plugin)
    return jsonify({'doc_id': doc_id, 'plugin': plugin, 'items': items})

# Serve existing report (if present) under /report
REPORT_DIR = ROOT / 'exports' / 'reports'

@app.get('/report')
def report_index():
    idx = REPORT_DIR / 'index.html'
    if not idx.exists():
        return jsonify({'error': 'report not found', 'path': str(idx)}), 404
    return send_from_directory(str(REPORT_DIR), 'index.html')

@app.get('/report/<path:path>')
def report_static(path):
    if not REPORT_DIR.exists():
        return jsonify({'error': 'report dir not found'}), 404
    return send_from_directory(str(REPORT_DIR), path)

# Attribute-wise listing across documents
@app.get('/api/attr/<plugin>/docs')
def api_attr_docs(plugin):
    pdir = ATTR_DIR / plugin
    present = []
    if pdir.exists() and pdir.is_dir():
        for f in sorted(pdir.glob('*.json')):
            try:
                data = json.loads(f.read_text(encoding='utf-8'))
                doc_id = data.get('doc_id') or f.stem
                val = data.get('value')
                preview = None
                if isinstance(val, str):
                    preview = val.strip().replace('\n',' ')[:160]
                present.append({'doc_id': doc_id, 'path': str(f), 'preview': preview})
            except Exception:
                doc_id = f.stem
                present.append({'doc_id': doc_id, 'path': str(f)})
    all_docs = _unique_doc_ids()
    have = {i['doc_id'] for i in present}
    missing = sorted([d for d in all_docs if d not in have])
    return jsonify({'plugin': plugin, 'present': present, 'missing': missing})

def _stop_worker_id(wid: int) -> bool:
    w = WORKERS.get(wid)
    if not w:
        return False
    proc = w.get('proc')
    if proc and (proc.poll() is None):
        try:
            proc.terminate()
        except Exception:
            pass
    WORKERS.pop(wid, None)
    return True

@app.get('/api/status')
def api_status():
    force_models = bool(request.args.get('force_models'))
    data = _read_jobs_status()
    data['gateway'] = _gateway_status()
    workers = []
    for wid, w in WORKERS.items():
        proc = w.get('proc')
        workers.append({
            'id': wid,
            'pid': proc.pid if proc else None,
            'running': (proc is not None and proc.poll() is None),
            'plugins': w.get('plugins'),
            'batch': w.get('batch'),
            'watch': w.get('watch'),
            'started_at': w.get('started_at'),
            'log_tail': list(w.get('log', deque()))[-200:],
            'current': w.get('current'),
        })
    data['workers'] = workers
    if workers:
        data['worker'] = workers[0]
    try:
        cfg = load_configs()
        data['plugin_defaults'] = cfg.get('plugins', {})
    except Exception:
        data['plugin_defaults'] = {}
    data['llm'] = _llm_health(force=force_models)
    return jsonify(data)

@app.get('/')
def root():
    index_path = pathlib.Path(app.static_folder) / 'index.html'
    if not index_path.exists():
        return jsonify({'error':'index not found', 'path': str(index_path)}), 500
    return send_from_directory(app.static_folder, 'index.html')

@app.get('/<path:path>')
def static_proxy(path):
    return send_from_directory(app.static_folder, path)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5051, debug=False, threaded=True)



./bin\dashboard_static\index.html
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GraphRAG Dashboard</title>
  <style>
    :root {
      --bg: #0b111b;
      --header-bg: #0f172a;
      --panel: #161f2e;
      --panel-alt: #1e2737;
      --border: #263342;
      --text: #e6edf7;
      --subtle: #94a3b8;
      --accent: #3b82f6;
      --accent-strong: #60a5fa;
      --danger: #f87171;
      --success: #34d399;
      --input-bg: #0f172a;
      --input-border: #1f2937;
      --table-row: #101827;
      --table-row-alt: #141d2c;
    }
    * { box-sizing: border-box; }
    body { font-family: system-ui, sans-serif; margin: 0; padding: 0; background: var(--bg); color: var(--text); }
    header { padding: 12px 16px; background: var(--header-bg); color: var(--text); display: flex; align-items: center; gap: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.3); }
    header h1 { margin: 0; font-size: 16px; font-weight: 600; }
    main { padding: 16px; display: grid; grid-template-columns: 320px 1fr; gap: 16px; }
    .panel { border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: var(--panel); box-shadow: 0 2px 12px rgba(0,0,0,0.25); }
    .row { display: flex; gap: 8px; align-items: center; flex-wrap: wrap; }
    .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(160px, 1fr)); gap: 8px; }
    .btn { padding: 6px 10px; border: 1px solid var(--input-border); border-radius: 6px; background: var(--input-bg); color: var(--text); cursor: pointer; transition: background 0.15s ease, border-color 0.15s ease; }
    .btn.small { padding: 4px 6px; font-size: 12px; }
    .btn:hover { background: var(--panel-alt); border-color: var(--accent); }
    .btn.primary { background: var(--accent); color: #0b1220; border-color: var(--accent); }
    .btn.primary:hover { background: var(--accent-strong); border-color: var(--accent-strong); }
    table { width: 100%; border-collapse: collapse; }
    th, td { padding: 6px 8px; border-bottom: 1px solid rgba(148, 163, 184, 0.2); text-align: left; font-size: 13px; }
    thead th { color: var(--subtle); font-weight: 500; text-transform: uppercase; letter-spacing: 0.02em; background: rgba(15, 23, 42, 0.6); }
    tbody tr { background: var(--table-row); }
    tbody tr:nth-child(2n) { background: var(--table-row-alt); }
    h3 { margin: 0 0 8px 0; font-size: 14px; color: var(--text); }
    small { color: var(--subtle); }
    ul { margin: 6px 0 0 18px; }
    .badge { background: rgba(59, 130, 246, 0.2); color: var(--accent-strong); padding:2px 6px; border-radius:10px; font-size:11px; }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }
    .ok { color: var(--success); }
    .err { color: var(--danger); }
    input, select, textarea { background: var(--input-bg); color: var(--text); border: 1px solid var(--input-border); border-radius: 6px; padding: 4px 6px; }
    input::placeholder, textarea::placeholder { color: rgba(148, 163, 184, 0.6); }
    details { background: var(--panel-alt); border-radius: 8px; border: 1px solid var(--border); padding: 8px; }
    details summary { cursor: pointer; color: var(--text); }
    pre { background: var(--panel-alt); color: var(--text); border-radius: 6px; padding: 8px; }
    select { min-height: 28px; }
    option { background: var(--input-bg); color: var(--text); }
    select option[selected], select option:checked {
      background-color: rgba(148, 163, 184, 0.28) !important;
      color: var(--text) !important;
    }
    select:focus option[selected], select:focus option:checked {
      background-color: rgba(59, 130, 246, 0.45) !important;
      color: var(--text) !important;
    }
    a { color: var(--accent-strong); }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <header>
    <h1>GraphRAG Dashboard</h1>
    <button class="btn" onclick="refresh()">Refresh</button>
  </header>
  <main>
    <section class="panel" id="controls">
      <h3>Planner</h3>
      <div class="row" style="margin-bottom:4px; align-items:center;">
        <label>Plugins</label>
        <span style="flex:1"></span>
        <div class="row" style="gap:4px;">
          <button class="btn" type="button" onclick="multiSelectAll('plugins')">All</button>
          <button class="btn" type="button" onclick="multiSelectNone('plugins')">None</button>
        </div>
      </div>
      <div class="row" style="margin-bottom:8px;">
        <select id="plugins" multiple size="10" style="width:100%">
          <option value="multi-basic">multi-basic (batch)</option>
          <option value="doc-skeleton">doc-skeleton</option>
          <option value="summary-20w">summary-20w</option>
          <option value="summary-short">summary-short</option>
          <option value="summary-medium">summary-medium</option>
          <option value="summary-long">summary-long</option>
          <option value="summary-outline">summary-outline</option>
          <option value="topic-tags">topic-tags</option>
          <option value="keyphrases">keyphrases</option>
          <option value="glossary">glossary</option>
          <option value="requirements">requirements</option>
          <option value="todo-items">todo-items</option>
          <option value="faq-pairs">faq-pairs</option>
          <option value="bridge-candidates">bridge-candidates</option>
          <option value="risk-scan">risk-scan</option>
          <option value="recent-summary">recent-summary</option>
          <option value="pii-scan">pii-scan</option>
        </select>
      </div>
      <div class="row" style="margin-bottom:4px; align-items:center;">
        <label>Docs</label>
        <span style="flex:1"></span>
        <div class="row" style="gap:4px;">
          <button class="btn" type="button" onclick="multiSelectAll('planner_docs')">All</button>
          <button class="btn" type="button" onclick="multiSelectNone('planner_docs')">None</button>
        </div>
      </div>
      <div class="row" style="margin-bottom:8px;">
        <select id="planner_docs" multiple size="10" style="width:100%"></select>
      </div>
      <div class="row" style="margin-bottom:8px;">
        <label>Payload JSON</label>
        <input id="payload_json" placeholder='{"llm":{"model":"qwen2.5-7b-instruct","timeout":60}}' style="flex:1" />
      </div>
      <div class="row" style="margin-bottom:8px;">
        <label>Changed since (min)</label>
        <input id="changed_min" type="number" min="0" value="0" style="width:100px" />
        <label>Limit</label>
        <input id="limit" type="number" min="0" value="0" style="width:100px" />
        <label><input id="map_reduce" type="checkbox" /> map-reduce</label>
      </div>
      <div class="row">
        <label><input id="only_missing" type="checkbox" checked /> only-missing</label>
        <button class="btn primary" onclick="planEnqueue()">Plan + Enqueue</button>
      </div>

      <hr/>
      <h3>Ingest</h3>
      <div class="row" style="margin-bottom:8px;">
        <input id="ingest_path" placeholder="repo path (e.g., G:\\LOKI\\papers)" style="flex:1" />
        <label><input id="ingest_full" type="checkbox" checked /> full</label>
      </div>
      <div class="row">
        <button class="btn" onclick="startIngest()">Start Ingest</button>
      </div>
      <hr/>
      <h3>Direct Enqueue</h3>
      <div class="row" style="margin-bottom:8px;">
        <input id="doc_ids" placeholder="doc ids comma-separated (leave empty = all)" style="flex:1" />
      </div>
      <div class="row">
        <button class="btn" onclick="enqueueSelected()">Enqueue for selected plugins</button>
      </div>
      <hr/>
      <h3>Worker</h3>
      <div class="row" style="margin-bottom:8px;">
        <input id="worker_plugins" value="*" style="flex:1" />
        <label>batch</label>
        <input id="worker_batch" type="number" value="8" style="width:90px" />
        <label>max-inflight</label>
        <input id="worker_max_inflight" type="number" value="2" style="width:90px" />
      </div>
      <div class="row">
        <button class="btn" onclick="startWorker()">Start Worker</button>
        <button class="btn" onclick="stopWorkerAll()">Stop All</button>
        <button class="btn" onclick="testWorker()">Test Connection</button>
      </div>
      <hr/>
      <h3>Gateway</h3>
      <div class="row" style="margin-bottom:8px;">
        <input id="gateway_service" placeholder="service (default lmstudio)" style="flex:1" />
        <label>log level</label>
        <input id="gateway_loglevel" value="INFO" style="width:90px" />
      </div>
      <div class="row">
        <button class="btn" onclick="startGateway()">Start Gateway</button>
        <button class="btn" onclick="stopGateway()">Stop Gateway</button>
      </div>
      <pre id="gateway_status" class="mono" style="margin-top:8px;background:var(--panel-alt);color:var(--text);padding:8px;border-radius:6px;max-height:160px;overflow:auto;border:1px solid var(--border);"></pre>
      <pre id="worker_test_result" class="mono" style="margin-top:8px;background:var(--panel-alt);color:var(--text);padding:8px;border-radius:6px;max-height:160px;overflow:auto;border:1px solid var(--border);"></pre>
      <div id="workers_list" style="margin-top:8px;"></div>
    </section>

    <section class="panel" id="status">
      <h3>Status</h3>
      <div id="llm"></div>
      <div class="grid" id="queue"></div>
      <details style="margin:8px 0;">
        <summary><b>Plugin defaults</b> <small>(per-plugin model/timeout)</small></summary>
        <div class="row" style="margin:6px 0;">
          <button class="btn" onclick="loadPluginDefaults()">Load</button>
          <span><small>Use JSON: { "summary-short": { "llm": { "model": "qwen2.5-7b-instruct", "timeout": 60 } } }</small></span>
        </div>
        <textarea id="plugin_defaults_json" style="width:100%;height:120px"></textarea>
        <div class="row" style="margin-top:6px;">
          <button class="btn" onclick="savePluginDefaults()">Save</button>
        </div>
      </details>
      <details style="margin:8px 0;">
        <summary><b>Queue listing</b> <small>(ordered by id asc)</small></summary>
      <div class="row" style="margin:6px 0;">
        <label>Status</label>
        <select id="queue_status"><option value="">(any)</option><option>pending</option><option>running</option><option>done</option></select>
        <label>Plugin</label>
        <input id="queue_plugin" placeholder="(any)" />
        <label>Limit</label>
        <input id="queue_limit" type="number" value="200" style="width:90px" />
        <button class="btn" onclick="loadQueueList()">Load</button>
        <span style="flex:1"></span>
        <label>Clear</label>
        <select id="queue_clear_mode"><option value="non-done">non-done</option><option value="pending">pending</option><option value="reset-running">reset running→pending</option><option value="reset-running-counter">reset running→pending + reset counter</option><option value="all">all</option></select>
        <button class="btn" onclick="clearQueue()">Clear Queue</button>
      </div>
        <div id="queue_list" style="max-height:240px; overflow:auto; border:1px solid var(--border); border-radius:6px; padding:6px; background:var(--panel-alt);"></div>
      </details>
      <details style="margin:8px 0;">
        <summary><b>LLM cache</b> <small>(prompt-level)</small></summary>
        <div class="row" style="margin:6px 0;">
          <button class="btn" onclick="loadCacheStats()">Refresh</button>
          <div id="cache_stats" style="margin-left:8px"></div>
          <span style="flex:1"></span>
          <button class="btn" onclick="clearCache()">Clear cache</button>
        </div>
        <div class="row" style="margin:6px 0;">
          <label>Show</label>
          <input id="cache_limit" type="number" value="200" style="width:90px" />
          <button class="btn" onclick="loadCacheList()">List</button>
        </div>
        <div id="cache_list" style="max-height:240px; overflow:auto; border:1px solid var(--border); border-radius:6px; padding:6px; background:var(--panel-alt);"></div>
      </details>
      <details style="margin:8px 0;">
        <summary><b>Failed Jobs</b></summary>
        <button class="btn" onclick="loadFailedJobs()">Refresh Failed</button>
        <div id="failed_jobs" style="margin-top:8px; max-height:200px; overflow:auto; border:1px solid var(--border); border-radius:6px; padding:6px; background:var(--panel-alt);"></div>
      </details>
      <div class="row" style="justify-content:space-between;align-items:center;">
        <h3>Attributes Coverage</h3>
        <div><label><input id="auto_refresh" type="checkbox" checked /> auto-refresh</label>
        <select id="refresh_sec"><option>3</option><option selected>5</option><option>10</option><option>30</option></select></div>
      </div>
      <div id="coverage_overall"></div>
      <div id="coverage"></div>
      <h3 style="margin-top:16px;">Domains</h3>
      <div id="domains"></div>
      <h3 style="margin-top:16px;">Worker Log (tail)</h3>
      <pre id="worker_log" style="background:var(--panel-alt);color:var(--text);padding:8px;border-radius:6px;max-height:200px;overflow:auto;border:1px solid var(--border);"></pre>

      <h3 style="margin-top:16px;">Documents</h3>
      <div class="row" style="margin-bottom:8px;">
        <input id="doc_filter" placeholder="filter by doc_id or domain" oninput="renderDocs()" style="flex:1" />
      </div>
      <div id="docs"></div>
    </section>
  </main>

  <script>
    // coverage open state persists across refreshes
    window.__COV_OPEN__ = window.__COV_OPEN__ || new Set();
    window.__COV_DOC_OPEN__ = window.__COV_DOC_OPEN__ || {}; // plugin -> Set(doc_id)
    window.__COV_DATA__ = window.__COV_DATA__ || {}; // plugin -> {present, missing}
    window.__ATTR_ITEM_CACHE__ = window.__ATTR_ITEM_CACHE__ || {}; // key: plugin|doc -> json
    window.__PLANNER_SELECTED_PLUGINS__ = window.__PLANNER_SELECTED_PLUGINS__ || [];
    window.__PLANNER_SELECTED_DOCS__ = window.__PLANNER_SELECTED_DOCS__ || [];
    window.__RUNNABLE_PLUGINS__ = window.__RUNNABLE_PLUGINS__ || [];
    window.__DOCS_COVERAGE_TOTAL__ = window.__DOCS_COVERAGE_TOTAL__ || 0;

    async function refresh(){
      const r = await fetch('/api/status');
      const data = await r.json();
      render(data);
      populatePlannerDocs();
      ensurePlannerBindings();
      restorePlannerSelections();
    }
    function selValues(id){
      const s = document.getElementById(id);
      return Array.from(s.selectedOptions).map(o=>o.value);
    }
    function rememberSelection(id, key){
      const sel = document.getElementById(id);
      if(!sel){ return; }
      window[key] = Array.from(sel.selectedOptions).map(o=>o.value);
    }
    function applySelection(id, key){
      const sel = document.getElementById(id);
      if(!sel){ return; }
      const desired = new Set(window[key] || []);
      Array.from(sel.options).forEach(opt=>{ opt.selected = desired.has(opt.value); });
    }
    function restorePlannerSelections(){
      applySelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
      applySelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
    }
    function populatePlannerDocs(){
      const select = document.getElementById('planner_docs');
      if(!select){ return; }
      const docs = window.__DOCS__ || [];
      const scroll = select.scrollTop;
      select.innerHTML = '';
      docs.slice().sort((a,b)=>a.doc_id.localeCompare(b.doc_id)).forEach(doc=>{
        const opt = document.createElement('option');
        opt.value = doc.doc_id;
        const domain = doc.domain ? ` (${doc.domain})` : '';
        opt.textContent = `${doc.doc_id}${domain}`;
        select.appendChild(opt);
      });
      applySelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
      select.scrollTop = scroll;
      rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
    }
    function multiSelectAll(id){
      const sel = document.getElementById(id);
      if(!sel){ return; }
      Array.from(sel.options).forEach(opt=>{ opt.selected = true; });
      rememberSelection(id, id === 'plugins' ? '__PLANNER_SELECTED_PLUGINS__' : '__PLANNER_SELECTED_DOCS__');
      restorePlannerSelections();
    }
    function multiSelectNone(id){
      const sel = document.getElementById(id);
      if(!sel){ return; }
      Array.from(sel.options).forEach(opt=>{ opt.selected = false; });
      rememberSelection(id, id === 'plugins' ? '__PLANNER_SELECTED_PLUGINS__' : '__PLANNER_SELECTED_DOCS__');
      restorePlannerSelections();
    }
    function ensurePlannerBindings(){
      const pluginSel = document.getElementById('plugins');
      if(pluginSel && pluginSel.dataset.plannerBound !== '1'){
        const handler = ()=>rememberSelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
        pluginSel.addEventListener('change', handler);
        pluginSel.addEventListener('input', handler);
        pluginSel.addEventListener('mousedown', evt=>toggleOptionSelection(evt, handler));
        pluginSel.dataset.plannerBound = '1';
        handler();
      }
      const docSel = document.getElementById('planner_docs');
      if(docSel && docSel.dataset.plannerBound !== '1'){
        const handler = ()=>rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
        docSel.addEventListener('change', handler);
        docSel.addEventListener('input', handler);
        docSel.addEventListener('mousedown', evt=>toggleOptionSelection(evt, handler));
        docSel.dataset.plannerBound = '1';
        handler();
      }
    }
    function toggleOptionSelection(evt, handler){
      const opt = evt.target;
      if(!opt || opt.tagName !== 'OPTION'){ return; }
      evt.preventDefault();
      const sel = opt.parentElement;
      const willSelect = !opt.selected;
      opt.selected = willSelect;
      if(!opt.selected){
        // keep focus on select to allow repeated toggles
        sel.focus();
      }
      handler();
    }
    async function planEnqueue(){
      rememberSelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
      rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
      const plugins = selValues('plugins');
      const doc_ids = selValues('planner_docs');
      const payload = {
        plugins,
        only_missing: document.getElementById('only_missing').checked,
        changed_since_min: parseInt(document.getElementById('changed_min').value||'0'),
        limit: parseInt(document.getElementById('limit').value||'0'),
        map_reduce: document.getElementById('map_reduce').checked
      };
      const pjson = document.getElementById('payload_json').value||'';
      if(pjson){ payload.payload_json = pjson; }
      if(doc_ids.length){ payload.doc_ids = doc_ids; }
      await fetch('/api/plan', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(payload)});
      await refresh();
    }
    async function enqueueSelected(){
      rememberSelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
      rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
      const plugins = selValues('plugins');
      let doc_ids = selValues('planner_docs');
      const manual = (document.getElementById('doc_ids').value||'').split(',').map(s=>s.trim()).filter(Boolean);
      doc_ids = Array.from(new Set([...doc_ids, ...manual]));
      if(doc_ids.length === 0){
        alert('Select docs from the Docs list or provide doc_ids (comma-separated).');
        return;
      }
      await fetch('/api/enqueue', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({plugins, doc_ids})});
      await refresh();
    }
    async function startIngest(){
      const repo = document.getElementById('ingest_path').value||'';
      const full = document.getElementById('ingest_full').checked;
      if(!repo){ alert('Enter a repo path'); return; }
      await fetch('/api/ingest', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({repo, full})});
    }
    async function startWorker(){
      const plugins = (document.getElementById('worker_plugins').value||'').split(',').map(s=>s.trim()).filter(Boolean);
      const batch = parseInt(document.getElementById('worker_batch').value||'8');
      const max_inflight = parseInt(document.getElementById('worker_max_inflight').value||'2');
      await fetch('/api/worker/start', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({plugins, batch, max_inflight})});
      await refresh();
    }
    async function stopWorkerAll(){
      await fetch('/api/worker/stop', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({})});
      await refresh();
    }
    async function startGateway(){
      const service = (document.getElementById('gateway_service').value||'').trim();
      const logLevel = (document.getElementById('gateway_loglevel').value||'INFO').trim() || 'INFO';
      const body = { log_level: logLevel };
      if(service){ body.service = service; }
      const r = await fetch('/api/gateway/start', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(body)});
      const data = await r.json();
      document.getElementById('gateway_status').textContent = JSON.stringify(data, null, 2);
      await refresh();
    }
    async function stopGateway(){
      const r = await fetch('/api/gateway/stop', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({})});
      const data = await r.json();
      document.getElementById('gateway_status').textContent = JSON.stringify(data, null, 2);
      await refresh();
    }
    async function testWorker(){
      const out = document.getElementById('worker_test_result');
      try{
        const r = await fetch('/api/status?force_models=1');
        const txt = await r.text();
        out.textContent = `HTTP ${r.status}\n${txt}`;
        console.log('[worker test]', r.status, txt);
      }catch(err){
        out.textContent = `Request failed: ${err}`;
        console.error('[worker test] error', err);
      }
    }
    function render(data){
      const llm = document.getElementById('llm');
      llm.innerHTML = `<div>LLM: <span class="${data.llm.reachable?'ok':'err'}">${data.llm.reachable?'reachable':'unreachable'}</span> <span class="mono">${data.llm.endpoint}</span></div>`
        + (data.llm.models && data.llm.models.length? `<div><small>models: ${data.llm.models.join(', ')}</small></div>`: '');
      if(data.llm.error){ llm.innerHTML += `<div class="err"><small>${data.llm.error}</small></div>`; }

      const gatewayEl = document.getElementById('gateway_status');
      if(gatewayEl && data.gateway){
        gatewayEl.textContent = JSON.stringify(data.gateway, null, 2);
      }

      const q = data.queue||{};
      const qdiv = document.getElementById('queue');
      const rows = [];
      const thr = q.throughput||{};
      const eta = q.eta||{};
      const overallEta = eta.overall_sec != null ? `, ETA: ${fmtDur(eta.overall_sec)}` : '';
      rows.push(`<div class="panel"><div><b>Total</b>: ${q.total||0}${overallEta}</div><div>by_status: ${JSON.stringify(q.by_status||{})}</div><div>oldest_pending_min: ${q.oldest_pending_min??'n/a'}</div><div><small>throughput: overall ${thr.overall_dps||0}/s over ${thr.window_min||0}m</small></div></div>`);
      const bp = q.by_plugin_status||{};
      const dur = q.durations||{};
      Object.keys(bp).sort().forEach(k=>{
        const d = dur[k];
        const est = d? `, avg ${d.avg_s}s over ${d.n}` : '';
        let etaPlug = '';
        if(eta.plugin_sec && eta.plugin_sec[k] != null){ etaPlug = `, ETA ${fmtDur(eta.plugin_sec[k])}`; }
        let dps = '';
        if(thr.plugin_dps && thr.plugin_dps[k] != null){ dps = `, ${thr.plugin_dps[k]}/s`; }
        rows.push(`<div class="panel"><div><b>${k}</b>${est}${dps}${etaPlug}</div><div><small>${JSON.stringify(bp[k])}</small></div></div>`);
      });
      qdiv.innerHTML = rows.join('');
      // refresh queue list silently if already populated
      if(document.getElementById('queue_list').getAttribute('data-has')==='1'){
        loadQueueList(true);
      }
      // auto refresh plugin defaults snapshot
      // (non-intrusive; we only auto-load once)
      if(!window.__PLUGINS_LOADED__){ loadPluginDefaults(); window.__PLUGINS_LOADED__=true; }

      renderCoverage(data);

      const domDiv = document.getElementById('domains');
      const d = data.domains||[];
      domDiv.innerHTML = '<ul>' + d.map(x=>`<li>${x.domain} <span class="badge">${x.docs}</span></li>`).join('') + '</ul>';

      const w = data.worker||{};
      const log = (w.log_tail||[]).join('\n');
      const statusPrefix = w.running ? '[running]\n' : '[stopped]\n';
      document.getElementById('worker_log').textContent = statusPrefix + log;
      renderWorkers(data.workers||[]);

      window.__STATUS__ = data;
      if(!window.__DOCS__){ loadDocs(); }
    }
    refresh();
    setInterval(()=>{
      const on = document.getElementById('auto_refresh').checked;
      const sec = parseInt(document.getElementById('refresh_sec').value||'5');
      if(on){ refresh(); }
    }, 5000);
    function fmtDur(sec){
      sec = Math.max(0, Math.round(sec));
      const h = Math.floor(sec/3600); const m = Math.floor((sec%3600)/60); const s = sec%60;
      return (h>0? h+'h ': '') + (m>0? m+'m ': '') + s + 's';
    }
    function renderWorkers(list){
      const root = document.getElementById('workers_list');
      if(!list || !list.length){ root.innerHTML = '<small>No workers</small>'; return; }
      root.innerHTML = '<div style="overflow-x:auto;">' +
        '<table style="min-width:820px;"><thead><tr><th>Current</th><th>ID</th><th>PID</th><th>Plugins</th><th>Batch</th><th>Started</th><th>Status</th><th></th></tr></thead><tbody>'+
        list.map(w=>`<tr><td><small class="mono" style="white-space:pre-wrap;">${w.current?fmtCurrent(w.current):''}</small></td><td>${w.id}</td><td>${w.pid||''}</td><td><small>${(w.plugins||[]).join(', ')}</small></td><td>${w.batch||''}</td><td><small>${w.started_at||''}</small></td><td>${w.running?'running':'stopped'}</td><td><button class=btn onclick="stopWorker(${w.id})">Stop</button></td></tr>`).join('')+
        '</tbody></table></div>';
    }
    function fmtCurrent(cur){
      if(!cur) return '';
      const parts = [];
      if(cur.plugin) parts.push(`plugin=${cur.plugin}`);
      if(cur.docs){
        const docs = cur.docs.length > 80 ? cur.docs.slice(0,77) + '…' : cur.docs;
        parts.push(`docs=${docs}`);
      }
      if(cur.since) parts.push(`since ${cur.since}`);
      return parts.map(p=>`• ${p}`).join('\n');
    }
    async function stopWorker(id){
      await fetch('/api/worker/stop', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({id})});
      await refresh();
    }
    async function loadQueueList(silent){
      const status = document.getElementById('queue_status').value;
      const plugin = document.getElementById('queue_plugin').value;
      const limit = parseInt(document.getElementById('queue_limit').value||'200');
      const params = new URLSearchParams();
      if(status) params.set('status', status);
      if(plugin) params.set('plugin', plugin);
      if(limit) params.set('limit', String(limit));
      const r = await fetch('/api/queue/list?' + params.toString());
      console.debug('[queue-list] status', r.status);
      const data = await r.json();
      const items = data.items||[];
      const root = document.getElementById('queue_list');
      root.setAttribute('data-has','1');
      root.innerHTML = '<table><thead><tr><th>ID</th><th>Status</th><th>Plugin</th><th>Doc</th><th>Created</th></tr></thead><tbody>'
        + items.map(i=>`<tr><td>${i.id}</td><td>${i.status}</td><td>${i.plugin}</td><td>${i.doc_id}</td><td>${i.created_at||''}</td></tr>`).join('')
        + '</tbody></table>';
    }
    async function clearQueue(){
      const mode = document.getElementById('queue_clear_mode').value||'non-done';
      console.debug('[queue-clear] mode', mode);
      await fetch('/api/queue/clear', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({mode})});
      await loadQueueList(true);
      await refresh();
    }
    async function loadPluginDefaults(){
      document.getElementById('plugin_defaults_json').value = 'Not supported on this build.';
    }
    async function savePluginDefaults(){
      alert('Saving plugin defaults is disabled in this build.');
    }
    async function loadCacheStats(){
      const r = await fetch('/api/cache/llm/stats');
      const data = await r.json();
      document.getElementById('cache_stats').innerText = `entries: ${data.count||0}, size: ${data.total_bytes||0} bytes`;
    }
    async function loadCacheList(){
      const limit = parseInt(document.getElementById('cache_limit').value||'200');
      const r = await fetch('/api/cache/llm/list?limit=' + limit);
      const data = await r.json();
      const items = data.items||[];
      document.getElementById('cache_list').innerHTML = '<table><thead><tr><th>Name</th><th>Size</th><th>MTime</th></tr></thead><tbody>' +
        items.map(i=>`<tr><td>${i.name}</td><td>${i.size}</td><td>${i.mtime}</td></tr>`).join('') + '</tbody></table>';
    }
    async function clearCache(){
      await fetch('/api/cache/llm/clear', {method:'POST'});
      await loadCacheStats();
      document.getElementById('cache_list').innerHTML='';
    }
    function renderCoverage(data){
      const cov = data.attributes_coverage||{};
      const covDiv = document.getElementById('coverage');
      const bp2 = (data.queue||{}).by_plugin_status||{};
      const dur2 = (data.queue||{}).durations||{};
      const thr2 = (data.queue||{}).throughput||{};
      const eta2 = (data.queue||{}).eta||{};
      const overallEtaTxt = (eta2 && eta2.overall_sec!=null) ? `Overall ETA: ${fmtDur(eta2.overall_sec)}` : '';
      document.getElementById('coverage_overall').innerHTML = overallEtaTxt? `<small>${overallEtaTxt}</small>` : '';
      const rows = ['<table><thead><tr><th></th><th>Attribute</th><th>Have/Total</th><th>pct</th><th>Pending</th><th>Running</th><th>Failed</th><th>Avg s</th><th>DPS</th><th>ETA</th><th>Missing (examples)</th></tr></thead><tbody>'];
      const plugins = Object.keys(cov).sort();
      for(const k of plugins){
        const v = cov[k];
        const st = bp2[k]||{}; const d = dur2[k];
        const dps = (thr2.plugin_dps||{})[k];
        const etaS = (eta2.plugin_sec||{})[k];
        const isOpen = window.__COV_OPEN__.has(k);
        rows.push(`<tr onclick="toggleCov('${k}')" style="cursor:pointer;">`
          + `<td style="width:18px;"><span id="cov_arrow_${k}">${isOpen?'▾':'›'}</span></td>`
          + `<td>${k}</td>`
          + `<td>${v.have}/${v.total_docs}</td>`
          + `<td>${v.pct}%</td>`
          + `<td>${st.pending||0}</td>`
          + `<td>${st.running||0}</td>`
          + `<td>${st.failed||0}</td>`
          + `<td>${st.failed||0}</td>`
          + `<td>${d? d.avg_s: ''}</td>`
          + `<td>${dps!=null? dps: ''}</td>`
          + `<td>${etaS!=null? fmtDur(etaS): ''}</td>`
          + `<td>${(v.missing_examples||[]).join(', ')}</td>`
          + `</tr>`);
        // doc rows
        if(isOpen){
          const pdata = window.__COV_DATA__[k];
          const present = (pdata && pdata.present) ? pdata.present : [];
          for(const item of present){
            const doc_id = item.doc_id; const prev = item.preview||'';
            const docOpen = (window.__COV_DOC_OPEN__[k]||new Set()).has(doc_id);
            rows.push(`<tr onclick="toggleCovDoc(event,'${k}','${doc_id}')" style="cursor:pointer;">`
              + `<td></td>`
              + `<td colspan=2><span id="cov_doc_arrow_${k}_${doc_id}">${docOpen?'▾':'›'}</span> <b>${doc_id}</b></td>`
              + `<td colspan=8 style="color:#555;">${escapeHtml(prev)}</td>`
              + `</tr>`);
            const key = `${k}|${doc_id}`;
            const hasFull = window.__ATTR_ITEM_CACHE__[key] != null;
            rows.push(`<tr id="cov_full_${k}_${doc_id}" style="display:${docOpen?'':'none'};"><td></td><td colspan=10><pre style="white-space:pre-wrap;background:var(--panel-alt);padding:8px;border-radius:6px;border:1px solid var(--border);" id="cov_pre_${k}_${doc_id}">${hasFull? escapeHtml(JSON.stringify(window.__ATTR_ITEM_CACHE__[key], null, 2)) : ''}</pre></td></tr>`);
          }
        }
      }
      rows.push('</tbody></table>');
      covDiv.innerHTML = rows.join('');
    }
    async function toggleCov(plugin){
      if(window.__COV_OPEN__.has(plugin)){
        window.__COV_OPEN__.delete(plugin);
        renderCoverage(window.__STATUS__);
        return;
      }
      // open
      window.__COV_OPEN__.add(plugin);
      if(!window.__COV_DATA__[plugin]){
        const r = await fetch(`/api/attr/${plugin}/docs`);
        const data = await r.json();
        window.__COV_DATA__[plugin] = data;
      }
      renderCoverage(window.__STATUS__);
    }
    async function toggleCovDoc(evt, plugin, doc_id){
      evt.stopPropagation();
      const set = (window.__COV_DOC_OPEN__[plugin] = window.__COV_DOC_OPEN__[plugin] || new Set());
      const key = `${plugin}|${doc_id}`;
      if(set.has(doc_id)){
        set.delete(doc_id);
        const row = document.getElementById(`cov_full_${plugin}_${doc_id}`);
        const arrow = document.getElementById(`cov_doc_arrow_${plugin}_${doc_id}`);
        if(row){ row.style.display = 'none'; }
        if(arrow){ arrow.textContent = '›'; }
        return;
      }
      set.add(doc_id);
      // ensure content
      if(window.__ATTR_ITEM_CACHE__[key] == null){
        const r = await fetch(`/api/doc/${doc_id}/attr/${plugin}`);
        const data = await r.json();
        window.__ATTR_ITEM_CACHE__[key] = (data.item||data);
      }
      const pre = document.getElementById(`cov_pre_${plugin}_${doc_id}`);
      const row = document.getElementById(`cov_full_${plugin}_${doc_id}`);
      const arrow = document.getElementById(`cov_doc_arrow_${plugin}_${doc_id}`);
      if(pre){ pre.textContent = JSON.stringify(window.__ATTR_ITEM_CACHE__[key], null, 2); }
      if(row){ row.style.display = ''; }
      if(arrow){ arrow.textContent = '▾'; }
    }
    function escapeHtml(s){
      return String(s).replace(/[&<>"']/g, c=>({"&":"&amp;","<":"&lt;",">":"&gt;","\"":"&quot;","'":"&#39;"}[c]));
    }
  </script>
  <script>
    // persist open state for docs and per-doc attributes
    window.__DOC_OPEN__ = window.__DOC_OPEN__ || new Set();
    window.__DOC_ATTR_OPEN__ = window.__DOC_ATTR_OPEN__ || {}; // doc_id -> Set(plugin)
    window.__DOC_META__ = window.__DOC_META__ || {}; // doc_id -> {meta, attributes}
    window.__DOC_CHUNK_ATTR_OPEN__ = window.__DOC_CHUNK_ATTR_OPEN__ || {}; // doc_id -> Set(chunk-plugin)
    window.__CHUNK_ATTR_CACHE__ = window.__CHUNK_ATTR_CACHE__ || {}; // key: chunks|plugin|doc_id -> items

    async function loadDocs(){
      const r = await fetch('/api/docs');
      const data = await r.json();
      window.__DOCS__ = data.docs||[];
      window.__DOCS_COVERAGE_TOTAL__ = data.coverage_total || 0;
      window.__RUNNABLE_PLUGINS__ = data.runnable_plugins || window.__RUNNABLE_PLUGINS__;
      renderDocs();
      populatePlannerDocs();
      restorePlannerSelections();
    }
    function renderDocs(){
      const docs = (window.__DOCS__||[]).slice();
      const q = (document.getElementById('doc_filter').value||'').toLowerCase();
      const root = document.getElementById('docs');
      const total = window.__DOCS_COVERAGE_TOTAL__ || (window.__RUNNABLE_PLUGINS__||[]).length;
      const rows = ['<table><thead><tr><th></th><th>Doc ID</th><th style="width:120px;">Coverage</th><th>Domain</th><th>Path</th></tr></thead><tbody>'];
      docs.filter(d=>!q || d.doc_id.toLowerCase().includes(q) || (d.domain||'').toLowerCase().includes(q))
          .forEach(d=>{
        const open = window.__DOC_OPEN__.has(d.doc_id);
        const have = d.coverage ? (d.coverage.have||0) : 0;
        const tot = d.coverage ? (d.coverage.total||total) : total;
        const pct = tot ? Math.round((have/tot)*100) : 0;
        rows.push(`<tr onclick="toggleDoc('${d.doc_id}')" style="cursor:pointer;">`
          + `<td style="width:18px;"><span id="doc_arrow_${d.doc_id}">${open?'▾':'›'}</span></td>`
          + `<td><b>${d.doc_id}</b></td>`
          + `<td><small>${pct}% (${have}/${tot})</small></td>`
          + `<td><small>${d.domain||''}</small></td>`
          + `<td><span class=mono>${d.path||''}</span></td>`
          + `</tr>`);
        if(open){
          const meta = window.__DOC_META__[d.doc_id];
          const attrs = meta && meta.attributes ? meta.attributes : {};
          const plugs = Object.keys(attrs).sort();
          rows.push(`<tr><td></td><td colspan=4><div class="row" style="gap:6px;">
            <button class="btn small" onclick="event.stopPropagation(); enqueueDocAll('${d.doc_id}', false);">All</button>
            <button class="btn small" onclick="event.stopPropagation(); enqueueDocAll('${d.doc_id}', true);">All (Force)</button>
          </div></td></tr>`);
          for(const plug of plugs){
            const attrOpen = (window.__DOC_ATTR_OPEN__[d.doc_id]||new Set()).has(plug);
            const metaInfo = attrs[plug] || {};
            const runnable = metaInfo.runnable !== false;
            const when = metaInfo.mtime ? new Date(metaInfo.mtime * 1000).toLocaleString() : '';
            rows.push(`<tr onclick="toggleDocAttr(event,'${d.doc_id}','${plug}')" style="cursor:pointer;">`
              + `<td></td>`
              + `<td colspan=2><span id="doc_attr_arrow_${d.doc_id}_${plug}">${attrOpen?'▾':'›'}</span> ${plug}${when?` <small style='opacity:.6;'>(${when})</small>`:''}</td>`
              + `<td colspan=2><div class="row" style="justify-content:flex-end; gap:4px;">
                    <button class="btn small" ${runnable?'':'disabled'} onclick="event.stopPropagation(); enqueueDocAttr('${d.doc_id}','${plug}', false);">Run</button>
                    <button class="btn small" ${runnable?'':'disabled'} onclick="event.stopPropagation(); enqueueDocAttr('${d.doc_id}','${plug}', true);">Run (Force)</button>
                  </div></td>`
              + `</tr>`);
            rows.push(`<tr id="doc_attr_full_${d.doc_id}_${plug}" style="display:${attrOpen?'':'none'};"><td></td><td colspan=4><pre style="white-space:pre-wrap;background:var(--panel-alt);padding:8px;border-radius:6px;border:1px solid var(--border);" id="doc_attr_pre_${d.doc_id}_${plug}"></pre></td></tr>`);
          }
          // Chunk-level attributes (currently exposing chunk-summary)
          const cplug = 'chunk-summary';
          const cOpen = (window.__DOC_CHUNK_ATTR_OPEN__[d.doc_id]||new Set()).has(cplug);
          rows.push(`<tr onclick="toggleDocChunkAttr(event,'${d.doc_id}','${cplug}')" style="cursor:pointer;">`
            + `<td></td>`
            + `<td colspan=2><span id="doc_chunk_attr_arrow_${d.doc_id}_${cplug}">${cOpen?'▾':'›'}</span> ${cplug} <span style="opacity:.65;">(per-chunk)</span></td>`
            + `<td colspan=2></td>`
            + `</tr>`);
          rows.push(`<tr id="doc_chunk_attr_full_${d.doc_id}_${cplug}" style="display:${cOpen?'':'none'};"><td></td><td colspan=4><div style="background:var(--panel-alt);padding:8px;border-radius:6px;max-height:260px;overflow:auto;border:1px solid var(--border);" id="doc_chunk_attr_div_${d.doc_id}_${cplug}"></div></td></tr>`);
        }
      });
      rows.push('</tbody></table>');
      root.innerHTML = rows.join('');
    }
    async function toggleDoc(doc_id){
      if(window.__DOC_OPEN__.has(doc_id)){
        window.__DOC_OPEN__.delete(doc_id);
        renderDocs();
        return;
      }
      window.__DOC_OPEN__.add(doc_id);
      if(!window.__DOC_META__[doc_id]){
        const r = await fetch(`/api/doc/${doc_id}`);
        const data = await r.json();
        window.__DOC_META__[doc_id] = data;
      }
      renderDocs();
    }
    async function toggleDocAttr(evt, doc_id, plug){
      evt.stopPropagation();
      const set = (window.__DOC_ATTR_OPEN__[doc_id] = window.__DOC_ATTR_OPEN__[doc_id] || new Set());
      const key = `${plug}|${doc_id}`;
      if(set.has(plug)){
        set.delete(plug);
        const row = document.getElementById(`doc_attr_full_${doc_id}_${plug}`);
        const arrow = document.getElementById(`doc_attr_arrow_${doc_id}_${plug}`);
        if(row){ row.style.display = 'none'; }
        if(arrow){ arrow.textContent = '›'; }
        return;
      }
      set.add(plug);
      // fetch content if not cached
      if(window.__ATTR_ITEM_CACHE__[key] == null){
        const r = await fetch(`/api/doc/${doc_id}/attr/${plug}`);
        const data = await r.json();
        window.__ATTR_ITEM_CACHE__[key] = (data.item||data);
      }
      const pre = document.getElementById(`doc_attr_pre_${doc_id}_${plug}`);
      const row = document.getElementById(`doc_attr_full_${doc_id}_${plug}`);
      const arrow = document.getElementById(`doc_attr_arrow_${doc_id}_${plug}`);
      if(pre){ pre.textContent = JSON.stringify(window.__ATTR_ITEM_CACHE__[key], null, 2); }
      if(row){ row.style.display = ''; }
      if(arrow){ arrow.textContent = '▾'; }
    }

    async function toggleDocChunkAttr(evt, doc_id, plug){
      evt.stopPropagation();
      const set = (window.__DOC_CHUNK_ATTR_OPEN__[doc_id] = window.__DOC_CHUNK_ATTR_OPEN__[doc_id] || new Set());
      const key = `chunks|${plug}|${doc_id}`;
      if(set.has(plug)){
        set.delete(plug);
        const row = document.getElementById(`doc_chunk_attr_full_${doc_id}_${plug}`);
        const arrow = document.getElementById(`doc_chunk_attr_arrow_${doc_id}_${plug}`);
        if(row){ row.style.display = 'none'; }
        if(arrow){ arrow.textContent = '›'; }
        return;
      }
      set.add(plug);
      if(window.__CHUNK_ATTR_CACHE__[key] == null){
        const r = await fetch(`/api/doc/${doc_id}/chunks/attr/${plug}`);
        const data = await r.json();
        window.__CHUNK_ATTR_CACHE__[key] = (data.items||[]);
      }
      const items = window.__CHUNK_ATTR_CACHE__[key] || [];
      const div = document.getElementById(`doc_chunk_attr_div_${doc_id}_${plug}`);
      const row = document.getElementById(`doc_chunk_attr_full_${doc_id}_${plug}`);
      const arrow = document.getElementById(`doc_chunk_attr_arrow_${doc_id}_${plug}`);
      if(div){
        if(items.length === 0){
          div.innerHTML = '<em style="opacity:.75;">No items</em>';
        } else {
          const lines = ['<table style="width:100%;"><thead><tr><th style="width:64px;">Seq</th><th style="width:320px;">Chunk</th><th>Preview</th></tr></thead><tbody>'];
          for(const it of items){
            const seq = (it.seq == null ? '' : it.seq);
            const chunk = it.chunk_id || '';
            const prev = escapeHtml(it.preview||'');
            lines.push(`<tr><td class=mono>${seq}</td><td class=mono style="word-break:break-all;">${chunk}</td><td>${prev}</td></tr>`);
          }
          lines.push('</tbody></table>');
          div.innerHTML = lines.join('');
        }
      }
      if(row){ row.style.display = ''; }
      if(arrow){ arrow.textContent = '▾'; }
    }
    async function enqueueDocAttr(doc_id, plugin, force){
      const meta = window.__DOC_META__[doc_id];
      const attrs = meta && meta.attributes ? meta.attributes : {};
      const info = attrs[plugin];
      if(!info || info.runnable === false){
        alert(`Plugin ${plugin} is not runnable from dashboard.`);
        return;
      }
      const payload = force ? { overwrite: true, force: true } : { force: true };
      await fetch('/api/enqueue', {
        method:'POST',
        headers:{'Content-Type':'application/json'},
        body: JSON.stringify({doc_ids:[doc_id], plugins:[plugin], payload})
      });
      await refreshDocMeta(doc_id, plugin);
      renderDocs();
    }
    async function enqueueDocAll(doc_id, force){
      const meta = window.__DOC_META__[doc_id];
      const attrs = meta && meta.attributes ? meta.attributes : {};
      const runnable = Object.keys(attrs).filter(p=>attrs[p] && attrs[p].runnable !== false);
      if(!runnable.length){
        alert('No runnable attributes for this document.');
        return;
      }
      const payload = force ? { overwrite: true, force: true } : { force: true };
      await fetch('/api/enqueue', {
        method:'POST',
        headers:{'Content-Type':'application/json'},
        body: JSON.stringify({doc_ids:[doc_id], plugins: runnable, payload})
      });
      await refreshDocMeta(doc_id);
      renderDocs();
    }
    async function refreshDocMeta(doc_id, plugin){
      const r = await fetch(`/api/doc/${doc_id}`);
      const data = await r.json();
      window.__DOC_META__[doc_id] = data;
      if(plugin){
        const key = `${plugin}|${doc_id}`;
        delete window.__ATTR_ITEM_CACHE__[key];
        if(window.__DOC_ATTR_OPEN__[doc_id] && window.__DOC_ATTR_OPEN__[doc_id].has(plugin)){
          const r2 = await fetch(`/api/doc/${doc_id}/attr/${plugin}`);
          const js = await r2.json();
          window.__ATTR_ITEM_CACHE__[key] = (js.item||js);
        }
      }
    }
    async function loadFailedJobs(){
      const r = await fetch('/api/queue/list?status=failed&limit=50');
      const data = await r.json();
      const items = data.items || [];
      const root = document.getElementById('failed_jobs');
      if(!items.length){ root.innerHTML = '<small>No failed jobs</small>'; return; }
      root.innerHTML = '<ul>' + items.map(i=>`<li><a href="#" onclick="viewJob(${i.id});return false;">${i.plugin} :: ${i.doc_id} (#${i.id})</a>${i.last_error?` <small class=err>${escapeHtml((i.last_error||'').slice(0,120))}</small>`:''}</li>`).join('') + '</ul>';
    }
    async function viewJob(id){
      const r = await fetch('/api/queue/job/' + id);
      if(!r.ok){ alert('Job '+id+' not found'); return; }
      const job = await r.json();
      const pre = document.createElement('pre');
      pre.style.whiteSpace = 'pre-wrap';
      pre.style.background = 'var(--panel-alt)';
      pre.style.padding = '8px';
      pre.style.borderRadius = '6px';
      pre.style.border = '1px solid var(--border)';
      pre.textContent = JSON.stringify(job, null, 2);
      const root = document.getElementById('failed_jobs');
      root.innerHTML = '';
      root.appendChild(pre);
    }
    
  </script>
</body>
</html>



./kn\__init__.py


./kn\config.py
import os, yaml, pathlib, re
from dotenv import load_dotenv

def _merge(a, b):
    if not isinstance(b, dict): return a
    out = a.copy()
    for k, v in b.items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _merge(out[k], v)
        else:
            out[k] = v
    return out

def load_configs():
    load_dotenv(override=True)
    root = pathlib.Path(os.getenv("KN_ROOT", ".knowledge"))
    cfg_dir = root / "config"
    models_yml = cfg_dir / "models.yml"
    pipeline_yml = cfg_dir / "pipeline.yml"
    cfg_dir.mkdir(parents=True, exist_ok=True)
    cfg = {
        "llm": {
            "base_url": os.getenv("OPENAI_BASE_URL", "http://localhost:1234/v1"),
            "api_key": os.getenv("OPENAI_API_KEY", "lm-studio"),
            "model": os.getenv("OPENAI_MODEL", "meta-llama-3.1-8b-instruct"),
            "max_tokens": 4096,
            "temperature": 0.2,
            "service": os.getenv("OPENAI_SERVICE", "lmstudio"),
        },
        "embeddings": {"name": os.getenv("EMBED_MODEL", "BAAI/bge-small-en-v1.5"), "normalize": True, "device": "auto"},
        "stores": {
            "vector": {"kind": "hnsw", "path": str(root/"indexes"/"embeddings"/"hnsw.index")},
            "graph": {"kind": "networkx"},
            "jobs": {"kind": "sqlite", "path": str(root/"queues"/"jobs.sqlite")},
        },
        "llm_gateway": {
            "queue_path": str(root/"queues"/"llm_gateway.sqlite"),
            "poll_interval_ms": 200,
            "wait_timeout_sec": 600,
            "health_timeout_sec": 10,
        },
        "llm_services": {
            "lmstudio": {
                "provider": "lmstudio",
                "base_url": os.getenv("OPENAI_BASE_URL", "http://localhost:1234/v1"),
                "api_key": os.getenv("OPENAI_API_KEY", "lm-studio"),
                "default_model": os.getenv("OPENAI_MODEL", "meta-llama-3.1-8b-instruct"),
                "max_context_tokens": 8192,
                "max_response_tokens": 4096,
                "timeout": 120,
                "max_concurrency": 1,
                "min_request_spacing_ms": 0,
                "unload_on_switch": True,
                "force_model_reload": False,
                "idle_unload_sec": 0,
            }
        },
        "watch": {"paths": [os.getenv("REPO_PATH", "../your-repo")], "ignore": ["**/.git/**","**/.knowledge/**"]},
        "ocr": {"enabled": os.getenv("OCR_ENABLED","false").lower() == "true", "tesseract_cmd": ""},
        "export": {"default_budget_tokens": 600000, "strategy":"hierarchy-first", "exclude_pii": False, "format":"md"},
        "retrieval": {"dense_k":12, "bm25_k":8, "graph_hops":2, "rerank": False},
        "graph": {"community_detection":"louvain", "edge_conf_threshold":0.55, "cross_domain_bridge_threshold":0.75,
                  "entity_extraction":"selective", "relation_extraction":"selective"},
        "chunking": {"policies":{"default":{"max_chars":4000,"overlap":400},
                                 "code":{"max_chars":2400,"overlap":200},
                                  "pdf":{"max_chars":3500,"overlap":200}}},
        "attributes": {"plugins": ["summary-20w", "topic-tags", "pii-scan"], "auto_enqueue": True},
        "plugins": {
            "summary-20w": {
                "llm": {
                    "model": "meta-llama-3.1-8b-instruct",
                    "timeout": 120,
                    "max_tokens": 2048,
                    "temperature": 0.3
                }
            },
            "summary-short": {
                "llm": {
                    "model": "meta-llama-3.1-8b-instruct",
                    "timeout": 120,
                    "max_tokens": 2048,
                    "temperature": 0.3
                }
            },
            "topic-tags": {
                "llm": {
                    "model": "meta-llama-3.1-8b-instruct",
                    "timeout": 120,
                    "max_tokens": 1536,
                    "temperature": 0.2
                }
            },
            "multi-basic": {
                "llm": {
                    "model": "qwen2.5-32b-instruct",
                    "timeout": 420,
                    "max_tokens": 8192,
                    "temperature": 0.2
                }
            },
            "doc-skeleton": {
                "llm": {
                    "model": "qwen2.5-32b-instruct",
                    "timeout": 420,
                    "max_tokens": 8192,
                    "temperature": 0.2
                }
            }
        }
    }
    if models_yml.exists():
        cfg = _merge(cfg, yaml.safe_load(models_yml.read_text(encoding="utf-8")))
    if pipeline_yml.exists():
        cfg = _merge(cfg, yaml.safe_load(pipeline_yml.read_text(encoding="utf-8")))
    cfg = _expand_env_vars(cfg)
    cfg["_root"] = str(root)
    return cfg

_ENV_VAR_PATTERN = re.compile(r"\$\{([A-Z0-9_]+)\}")

def _expand_env_vars(obj):
    if isinstance(obj, dict):
        return {k: _expand_env_vars(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_expand_env_vars(v) for v in obj]
    if isinstance(obj, str):
        def repl(m):
            key = m.group(1)
            return os.getenv(key, m.group(0))
        return _ENV_VAR_PATTERN.sub(repl, obj)
    return obj

./kn\hashing.py
import hashlib

def content_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()

def short_hash(h: str, n=8) -> str:
    return h[:n]

def chunk_hash(doc_id: str, chunk_index: int) -> str:
    return f"{doc_id}-{chunk_index:04d}"

./kn\file_index.py
import pathlib, time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from .utils.io import read_text_safely, ensure_dirs
from .hashing import content_hash, short_hash
from .jobs_sqlite import ensure_db as _ensure_jobs_db, enqueue as _enqueue_job

_manifest = {}  # path -> {doc_id, hash, mtime}
IGNORE_DIRS = {".git", ".knowledge"}
IGNORE_EXT = {".lock", ".tmp", ".log"}

def initial_scan(repo_path: pathlib.Path, cfg):
    docs = []
    for p in repo_path.rglob("*"):
        if p.is_dir():
            if p.name in IGNORE_DIRS: continue
            continue
        if any(part in IGNORE_DIRS for part in p.parts): continue
        if p.suffix.lower() in IGNORE_EXT: continue
        text = read_text_safely(p)
        if not text: continue
        ch = content_hash(text)
        doc_id = short_hash(ch)
        _manifest[str(p)] = {"doc_id": doc_id, "hash": ch, "mtime": p.stat().st_mtime}
        docs.append({"path": str(p), "doc_id": doc_id, "hash": ch, "text": text})
    return docs

class _Evt(FileSystemEventHandler):
    def __init__(self, cfg):
        self.cfg = cfg
    def on_any_event(self, event):
        p = pathlib.Path(getattr(event, 'dest_path', event.src_path))
        if p.is_dir() or any(part in IGNORE_DIRS for part in p.parts): return
        if not p.exists(): return
        text = read_text_safely(p)
        if text is None: return
        ch = content_hash(text)
        m = _manifest.get(str(p))
        if m and m["hash"] == ch:
            m["mtime"] = p.stat().st_mtime
            return
        record = {"doc_id": short_hash(ch), "hash": ch, "mtime": p.stat().st_mtime}
        _manifest[str(p)] = record
        print(f"[watch] change: {p}")
        # auto-enqueue attribute jobs (doc-level) if configured
        attrs = self.cfg.get("attributes", {})
        if attrs.get("auto_enqueue", False):
            plugins = attrs.get("plugins", [])
            for plugin in plugins:
                _enqueue_job(self.cfg, plugin, record["doc_id"], payload={"path": str(p)})
        # TODO: optionally enqueue ingest/graph rebuild jobs (separate scheduler handles embed/graph nightly)

def watch_changes(repo_path: pathlib.Path, cfg):
    ensure_dirs(cfg)
    # ensure jobs db exists if auto-enqueue is enabled
    attrs = cfg.get("attributes", {})
    if attrs.get("auto_enqueue", False):
        _ensure_jobs_db(cfg)
    obs = Observer()
    obs.schedule(_Evt(cfg), str(repo_path), recursive=True)
    obs.start()
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        obs.stop()
    obs.join()

./kn\chunking.py
import pathlib, json, re
from .utils.io import ensure_dirs
from .hashing import chunk_hash

CHUNKS_DIR = pathlib.Path('.knowledge/indexes/chunks')

def _split_paragraphs(text: str):
    # split on blank lines; keep headings by not stripping too aggressively
    paras = re.split(r"\n\s*\n+", text)
    # normalize paragraphs
    return [p.strip() for p in paras if p and p.strip()]

def chunk_text(text: str, max_chars=3000, overlap_ratio=0.1):
    # paragraph-first packing with small overlap (last paragraph of previous chunk)
    paras = _split_paragraphs(text)
    chunks = []
    current = []
    current_len = 0
    for para in paras:
        p_len = len(para) + 2  # account for join newlines
        if current_len + p_len <= max_chars or not current:
            current.append(para)
            current_len += p_len
        else:
            chunks.append("\n\n".join(current))
            # overlap: keep last paragraph from previous chunk
            overlap_n = 1 if current else 0
            tail = current[-overlap_n:] if overlap_n else []
            current = tail + [para]
            current_len = sum(len(t) + 2 for t in current)
    if current:
        chunks.append("\n\n".join(current))
    return chunks

def detect_domain(rel_path: pathlib.Path):
    parts = rel_path.parts
    return parts[0] if parts else "root"

def chunk_repo(docs, cfg, repo_root: pathlib.Path):
    ensure_dirs(cfg)
    policy = cfg.get("chunking", {}).get("policies", {}).get("default", {"max_chars":3000,"overlap":300})
    chunks = []
    CHUNKS_DIR.mkdir(parents=True, exist_ok=True)
    for d in docs:
        path = pathlib.Path(d["path"]).resolve()
        try: rel = path.relative_to(repo_root)
        except Exception: rel = pathlib.Path(path.name)
        domain = detect_domain(rel)
        parts = chunk_text(d.get("text",""), max_chars=policy.get("max_chars",3000), overlap_ratio=(policy.get("overlap",300)/max(policy.get("max_chars",3000),1)))
        total = len(parts)
        for idx, t in enumerate(parts):
            chunk_id = chunk_hash(d["doc_id"], idx)
            rec = {
                "doc_id": d["doc_id"],
                "chunk_id": chunk_id,
                "text": t,
                "meta": {
                    "path": str(path),
                    "rel": str(rel),
                    "domain": domain,
                    "mimetype": "text/plain",
                    "seq": idx,
                    "total": total,
                    "prev": chunk_hash(d["doc_id"], idx-1) if idx>0 else None,
                    "next": chunk_hash(d["doc_id"], idx+1) if idx<total-1 else None
                }
            }
            chunks.append(rec)
            (CHUNKS_DIR / f"{chunk_id}.json").write_text(json.dumps(rec, ensure_ascii=False), encoding='utf-8')
    return chunks

./kn\embeddings.py
from sentence_transformers import SentenceTransformer
import numpy as np
from .vector_hnsw import HNSWIndex

_model = None
def _get_model(name: str, device: str="auto"):
    global _model
    if _model is None:
        _model = SentenceTransformer(name, device=(None if device=="auto" else device))
    return _model

def embed_chunks(chunks, cfg):
    emc = cfg.get("embeddings", {})
    m = _get_model(emc.get("name", "BAAI/bge-small-en-v1.5"), emc.get("device", "auto"))
    if not chunks:
        return
    texts = [c["text"] for c in chunks]
    vecs = m.encode(texts, normalize_embeddings=emc.get("normalize", True), show_progress_bar=True)
    idx = HNSWIndex.open(cfg, dim=len(vecs[0]))
    idx.add([c["chunk_id"] for c in chunks], np.asarray(vecs, dtype=np.float32))
    idx.save()

./kn\vector_hnsw.py
import pathlib, json
import numpy as np
try:
    import hnswlib  # optional on Windows
    _HNSW_AVAILABLE = True
except Exception:
    hnswlib = None
    _HNSW_AVAILABLE = False

class _BruteForceIndex:
    def __init__(self, space: str, dim: int, store_path: str | None = None):
        self.space = space
        self.dim = dim
        self.vectors = None  # np.ndarray shape (N, D)
        self.store_path = store_path
    def load_index(self, path: str):
        # For brute-force, read from an .npy file alongside meta
        npy = self.store_path or (path + ".npy")
        try:
            if pathlib.Path(npy).exists():
                self.vectors = np.load(npy).astype(np.float32)
        except Exception:
            self.vectors = None
    def set_ef(self, ef: int):
        pass
    def init_index(self, max_elements: int, ef_construction: int, M: int):
        pass
    def add_items(self, vecs: np.ndarray, labels: np.ndarray):
        if self.vectors is None:
            self.vectors = vecs.astype(np.float32)
        else:
            self.vectors = np.vstack([self.vectors, vecs.astype(np.float32)])
    def save_index(self, path: str):
        # Persist vectors so future processes can search
        npy = self.store_path or (path + ".npy")
        try:
            if self.vectors is not None:
                pathlib.Path(npy).parent.mkdir(parents=True, exist_ok=True)
                np.save(npy, self.vectors)
        except Exception:
            pass
    def knn_query(self, q: np.ndarray, k: int = 10):
        if self.vectors is None or len(self.vectors) == 0:
            return np.zeros((len(q), 0), dtype=int), np.zeros((len(q), 0), dtype=np.float32)
        # cosine or l2
        Q = q.astype(np.float32)
        X = self.vectors
        if self.space == 'cosine':
            def normalize(a):
                n = np.linalg.norm(a, axis=1, keepdims=True) + 1e-12
                return a / n
            Qn = normalize(Q)
            Xn = normalize(X)
            sims = Qn @ Xn.T
            # higher is better; convert to distances for compatibility
            dists = 1.0 - sims
        else:
            # l2 distance
            # ||Q - X||^2 = ||Q||^2 + ||X||^2 - 2 Q X^T
            Q2 = np.sum(Q * Q, axis=1, keepdims=True)
            X2 = np.sum(X * X, axis=1)[None, :]
            dists = Q2 + X2 - 2.0 * (Q @ X.T)
        idxs = np.argsort(dists, axis=1)[:, :k]
        rows = np.take_along_axis(dists, idxs, axis=1)
        return idxs, rows
class HNSWIndex:
    def __init__(self, path: pathlib.Path, dim=768, space='cosine'):
        self.path = path
        self.meta_path = path.with_suffix('.meta.json')
        self.dim = dim
        self.space = space
        if _HNSW_AVAILABLE:
            self.index = hnswlib.Index(space=space, dim=dim)
        else:
            self.index = _BruteForceIndex(space=space, dim=dim, store_path=str(self.path) + ".npy")
        self.inited = False
        self.ids = []
    @classmethod
    def open(cls, cfg, dim=768):
        path = pathlib.Path(cfg["stores"]["vector"]["path"])
        meta = path.with_suffix('.meta.json')
        if path.exists() and meta.exists():
            m = json.loads(meta.read_text())
            dim = m.get("dim", dim)
            obj = cls(path, dim=dim)
            # if brute-force, nothing to load
            try:
                obj.index.load_index(str(path))
                obj.index.set_ef(128)
            except Exception:
                pass
            obj.inited = True
            obj.ids = m.get("ids", [])
            return obj
        return cls(path, dim=dim)
    def _ensure_init(self, total=10000):
        if not self.inited:
            try:
                self.index.init_index(max_elements=total, ef_construction=200, M=16)
                self.index.set_ef(128)
            except Exception:
                pass
            self.inited = True
    def add(self, keys, vecs: np.ndarray):
        self._ensure_init(max(10000, len(self.ids) + len(keys) + 1000))
        labels = np.arange(len(self.ids), len(self.ids)+len(keys))
        self.index.add_items(vecs, labels)
        self.ids.extend(list(keys))
    def save(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        try:
            self.index.save_index(str(self.path))
        except Exception:
            pass
        self.meta_path.write_text(json.dumps({"dim": self.dim, "ids": self.ids}))
    def search(self, vecs: np.ndarray, k=10):
        labels, dists = self.index.knn_query(vecs, k=k)
        inv = self.ids
        mapped = [[inv[i] for i in row] for row in labels]
        return mapped, dists

./kn\graph_nx.py
import pathlib, json, networkx as nx
from collections import defaultdict
GRAPH_PATH = pathlib.Path('.knowledge/indexes/graph/graph.jsonl')
COMM_PATH = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')
G = nx.Graph()
def naive_entities(text: str):
    import re
    ents = set(re.findall(r"[A-Z][a-zA-Z0-9_]+|\b[A-Za-z0-9_]+\.\w+|#\w+", text))
    return list(ents)[:50]
def build_or_update_graph(chunks, cfg):
    for c in chunks:
        ents = naive_entities(c["text"])
        for e in ents:
            if not G.has_node(e):
                G.add_node(e, kind='entity')
        for i in range(len(ents)):
            for j in range(i+1, len(ents)):
                u, v = ents[i], ents[j]
                w = G[u][v]["weight"]+1 if G.has_edge(u,v) else 1
                G.add_edge(u, v, weight=w)
    GRAPH_PATH.parent.mkdir(parents=True, exist_ok=True)
    with GRAPH_PATH.open('w', encoding='utf-8') as f:
        for u, v, d in G.edges(data=True):
            f.write(json.dumps({"src": u, "dst": v, "weight": d.get("weight",1)})+"\n")
def summarize_communities(cfg):
    try:
        import community as community_louvain
        part = community_louvain.best_partition(G) if len(G) else {}
        comm_to_nodes = defaultdict(list)
        for n, c in part.items():
            comm_to_nodes[c].append(n)
        comms = [{"id": f"C{k}", "size": len(v), "summary": ", ".join(v[:10])} for k, v in comm_to_nodes.items()]
    except Exception:
        comms = []
        for idx, comp in enumerate(nx.connected_components(G)):
            nodes = list(comp)
            summary = ", ".join(nodes[:10])
            comms.append({"id": f"C{idx}", "size": len(nodes), "summary": summary})
    COMM_PATH.parent.mkdir(parents=True, exist_ok=True)
    with COMM_PATH.open('w', encoding='utf-8') as f:
        for c in comms:
            f.write(json.dumps(c)+"\n")

./kn\retrieval.py
import json, pathlib
import numpy as np
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from .vector_hnsw import HNSWIndex
COMM_PATH = pathlib.Path('.knowledge/indexes/summaries/communities.jsonl')
CHUNK_DIR = pathlib.Path('.knowledge/indexes/chunks')
_embed_model = None
def _get_model(name):
    global _embed_model
    if _embed_model is None:
        _embed_model = SentenceTransformer(name)
    return _embed_model
def _iter_chunks():
    for p in CHUNK_DIR.glob('*.json'):
        yield json.loads(p.read_text(encoding='utf-8'))
def _matches_scope(rec, scope: str) -> bool:
    if not scope:
        return True
    meta = rec.get('meta', {})
    rel = str(meta.get('rel', ''))
    path = str(meta.get('path', ''))
    domain = str(meta.get('domain', ''))
    s = scope.lower()
    return (s in rel.lower()) or (s in path.lower()) or (s == domain.lower())
def _bm25_corpus():
    docs = list(_iter_chunks())
    corpus = [ (d["chunk_id"], (d["text"] or "").split()) for d in docs ]
    return docs, corpus
def answer_query(q: str, cfg, scope=None, topk=12):
    emc = cfg.get('embeddings', {})
    m = _get_model(emc.get('name','BAAI/bge-small-en-v1.5'))
    qv = m.encode([q], normalize_embeddings=emc.get('normalize', True))
    idx = HNSWIndex.open(cfg, dim=len(qv[0]))
    ids, dists = idx.search(np.asarray(qv, dtype=np.float32), k=topk)
    idset = set(ids[0]) if ids else set()
    chunks_by_id = { }
    for rec in _iter_chunks():
        chunks_by_id[rec['chunk_id']] = rec
    dense_hits = [chunks_by_id[i] for i in idset if i in chunks_by_id]
    bm25_hits = []
    try:
        docs, corpus = _bm25_corpus()
        bm25 = BM25Okapi([tokens for _, tokens in corpus])
        scores = bm25.get_scores(q.split())
        k = cfg.get("retrieval", {}).get("bm25_k", 8)
        ranked = np.argsort(scores)[::-1][:k]
        for idx_i in ranked:
            bm25_hits.append(docs[idx_i])
    except Exception:
        pass
    used = set()
    merged = []
    for h in dense_hits + bm25_hits:
        if scope and not _matches_scope(h, scope):
            continue
        cid = h['chunk_id']
        if cid in used: continue
        used.add(cid)
        merged.append(h)
        if len(merged) >= topk: break
    pref = []
    if COMM_PATH.exists():
        for line in COMM_PATH.read_text(encoding='utf-8').splitlines()[:3]:
            c = json.loads(line)
            pref.append(f"[Community {c['id']} size={c['size']}] {c['summary']}")
    text = "\n\n".join(pref) + "\n\n" + "\n\n".join([h['text'] for h in merged])
    return text

def retrieve_context(q: str, cfg, scope=None, topk=64):
    """Return preface lines and selected chunk records for downstream assembly/export."""
    emc = cfg.get('embeddings', {})
    m = _get_model(emc.get('name','BAAI/bge-small-en-v1.5'))
    qv = m.encode([q], normalize_embeddings=emc.get('normalize', True))
    idx = HNSWIndex.open(cfg, dim=len(qv[0]))
    ids, dists = idx.search(np.asarray(qv, dtype=np.float32), k=topk)
    idset = set(ids[0]) if ids else set()
    chunks_by_id = {}
    for rec in _iter_chunks():
        chunks_by_id[rec['chunk_id']] = rec
    dense_hits = [chunks_by_id[i] for i in idset if i in chunks_by_id]
    bm25_hits = []
    try:
        docs, corpus = _bm25_corpus()
        bm25 = BM25Okapi([tokens for _, tokens in corpus])
        scores = bm25.get_scores(q.split())
        k = cfg.get("retrieval", {}).get("bm25_k", 8)
        ranked = np.argsort(scores)[::-1][:k]
        for idx_i in ranked:
            bm25_hits.append(docs[idx_i])
    except Exception:
        pass
    used = set()
    merged = []
    for h in dense_hits + bm25_hits:
        if scope and not _matches_scope(h, scope):
            continue
        cid = h['chunk_id']
        if cid in used: continue
        used.add(cid)
        merged.append(h)
        if len(merged) >= topk: break
    pref = []
    if COMM_PATH.exists():
        for line in COMM_PATH.read_text(encoding='utf-8').splitlines()[:3]:
            c = json.loads(line)
            pref.append(f"[Community {c['id']} size={c['size']}] {c['summary']}")
    return pref, merged

./kn\exporter.py
import pathlib
from .retrieval import retrieve_context
import json, pathlib as _p

def _approx_trim_to_tokens(text: str, max_tokens: int) -> str:
    if not max_tokens:
        return text
    # ~4 chars/token heuristic
    max_chars = max(0, int(max_tokens * 4))
    return text[:max_chars]

def _pii_flagged(doc_id: str) -> bool:
    p = _p.Path('.knowledge/indexes/attributes/pii-scan') / f"{doc_id}.json"
    if not p.exists():
        return False
    try:
        data = json.loads(p.read_text(encoding='utf-8'))
        return (data.get('risk') or 0) >= 0.8
    except Exception:
        return False

def export_monofile(q: str, cfg, out: pathlib.Path, budget: int|None=None, scope=None, include_meta: bool=True):
    pref, chunks = retrieve_context(q, cfg, scope=scope, topk=64)
    is_md = cfg.get("export", {}).get("format","md") == "md"
    if cfg.get('export', {}).get('exclude_pii', False):
        chunks = [c for c in chunks if not _pii_flagged(c.get('doc_id'))]
    parts = []
    if is_md:
        parts.append(f"# Export: {q}\n")
    if pref:
        parts.append("\n".join(pref) + ("\n\n" if is_md else "\n\n"))
    for c in chunks:
        meta = c.get('meta', {})
        if include_meta and is_md:
            parts.append(f"## {meta.get('rel', meta.get('path',''))} — {c['chunk_id']}\n")
        parts.append(c.get('text',''))
        parts.append("\n\n")
    body = "".join(parts)
    body = _approx_trim_to_tokens(body, budget or cfg.get('export',{}).get('default_budget_tokens'))
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(body, encoding='utf-8')

./kn\jobs_sqlite.py
import sqlite3, json, pathlib
JOB_ATTR_PREFIX = "attribute:"
def _db_path(cfg):
    return pathlib.Path(cfg["stores"]["jobs"]["path"])
def ensure_db(cfg):
    p = _db_path(cfg); p.parent.mkdir(parents=True, exist_ok=True)
    con = sqlite3.connect(p)
    con.execute("""
    CREATE TABLE IF NOT EXISTS jobs(
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      plugin TEXT NOT NULL,
      doc_id TEXT NOT NULL,
      payload TEXT,
      status TEXT DEFAULT 'pending',
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    # add optional columns if missing (migrations)
    try:
        cols = [r[1] for r in con.execute("PRAGMA table_info(jobs)").fetchall()]
        if 'completed_at' not in cols:
            con.execute("ALTER TABLE jobs ADD COLUMN completed_at TIMESTAMP")
        if 'retries' not in cols:
            con.execute("ALTER TABLE jobs ADD COLUMN retries INTEGER DEFAULT 0")
        if 'last_error' not in cols:
            con.execute("ALTER TABLE jobs ADD COLUMN last_error TEXT")
    except Exception:
        pass
    # limits and counters tables for concurrency control
    try:
        con.execute("CREATE TABLE IF NOT EXISTS limits(name TEXT PRIMARY KEY, value INTEGER)")
        con.execute("CREATE TABLE IF NOT EXISTS counters(name TEXT PRIMARY KEY, value INTEGER)")
    except Exception:
        pass
    con.commit(); con.close()
def enqueue(cfg, plugin: str, doc_id: str, payload: dict|None=None):
    con = sqlite3.connect(_db_path(cfg))
    con.execute("INSERT INTO jobs(plugin, doc_id, payload) VALUES (?,?,?)",
                (plugin, doc_id, json.dumps(payload or {})))
    con.commit(); con.close()
def dequeue_batch(cfg, wanted_plugins, limit=16):
    if not wanted_plugins:
        return []
    con = sqlite3.connect(_db_path(cfg)); cur = con.cursor()
    qmarks = ",".join(["?"]*len(wanted_plugins))
    cur.execute(f"""
      SELECT id, plugin, doc_id, payload FROM jobs
      WHERE status='pending' AND plugin IN ({qmarks})
      ORDER BY COALESCE(retries,0) ASC, id ASC LIMIT ?
    """, (*wanted_plugins, limit))
    rows = cur.fetchall(); ids = [r[0] for r in rows]
    if ids:
      cur.execute(f"UPDATE jobs SET status='running' WHERE id IN ({','.join(['?']*len(ids))})", ids)
    con.commit(); con.close()
    return [{"id": r[0], "plugin": r[1], "doc_id": r[2], "payload": json.loads(r[3] or '{}')} for r in rows]
def ack_job(cfg, job_id: int):
    con = sqlite3.connect(_db_path(cfg)); con.execute("UPDATE jobs SET status='done', completed_at=CURRENT_TIMESTAMP WHERE id=?", (job_id,))
    con.commit(); con.close()

def fail_and_requeue_job(cfg, job_id: int, error_message: str|None=None, back_to_pending: bool=True):
    con = sqlite3.connect(_db_path(cfg))
    if back_to_pending:
        con.execute("UPDATE jobs SET status='pending', retries=coalesce(retries,0)+1, last_error=? WHERE id=?", (error_message, job_id))
    else:
        con.execute("UPDATE jobs SET status='failed', retries=coalesce(retries,0)+1, last_error=? WHERE id=?", (error_message, job_id))
    con.commit(); con.close()

def reset_status(cfg, job_ids, status='pending'):
    if not job_ids:
        return
    con = sqlite3.connect(_db_path(cfg))
    placeholders = ",".join(["?"]*len(job_ids))
    con.execute(f"UPDATE jobs SET status=? WHERE id IN ({placeholders})", (status, *job_ids))
    con.commit(); con.close()

def reset_running_jobs(cfg, status='pending'):
    con = sqlite3.connect(_db_path(cfg))
    try:
        cur = con.cursor()
        ids = [row[0] for row in cur.execute("select id from jobs where status='running'").fetchall()]
    finally:
        con.close()
    if not ids:
        return []
    reset_status(cfg, ids, status=status)
    return ids

def list_pending_plugins(cfg):
    con = sqlite3.connect(_db_path(cfg))
    rows = [r[0] for r in con.execute("select distinct plugin from jobs where status='pending'").fetchall()]
    con.close()
    return rows

# Concurrency primitives (best-effort with SQLite)
def set_limit(cfg, name: str, value: int):
    con = sqlite3.connect(_db_path(cfg)); cur = con.cursor()
    cur.execute("INSERT INTO limits(name,value) VALUES(?,?) ON CONFLICT(name) DO UPDATE SET value=excluded.value", (name, value))
    # ensure counter row exists
    cur.execute("INSERT INTO counters(name,value) VALUES(?,0) ON CONFLICT(name) DO NOTHING", (name,))
    con.commit(); con.close()

def get_limit(cfg, name: str) -> int|None:
    con = sqlite3.connect(_db_path(cfg)); cur = con.cursor()
    row = cur.execute("select value from limits where name=?", (name,)).fetchone()
    con.close()
    return row[0] if row else None

def get_counter(cfg, name: str) -> int:
    con = sqlite3.connect(_db_path(cfg)); cur = con.cursor()
    row = cur.execute("select value from counters where name=?", (name,)).fetchone()
    con.close()
    return row[0] if row else 0

def reset_counter(cfg, name: str):
    con = sqlite3.connect(_db_path(cfg))
    try:
        con.execute("UPDATE counters SET value=0 WHERE name=?", (name,))
        con.commit()
    finally:
        con.close()

def try_acquire(cfg, name: str) -> bool:
    con = sqlite3.connect(_db_path(cfg))
    try:
        con.isolation_level = None
        con.execute("BEGIN IMMEDIATE")
        cur = con.cursor()
        row = cur.execute("select value from limits where name=?", (name,)).fetchone()
        limit = row[0] if row else None
        if limit is None:
            # default no limit
            limit = 1
            cur.execute("INSERT INTO limits(name,value) VALUES(?,?) ON CONFLICT(name) DO NOTHING", (name, limit))
        row = cur.execute("select value from counters where name=?", (name,)).fetchone()
        current = row[0] if row else 0
        if current < limit:
            if row:
                cur.execute("update counters set value=value+1 where name=?", (name,))
            else:
                cur.execute("insert into counters(name,value) values(?,1)", (name,))
            con.execute("COMMIT")
            return True
        con.execute("ROLLBACK")
        return False
    except Exception:
        try:
            con.execute("ROLLBACK")
        except Exception:
            pass
        return False
    finally:
        con.close()

def release(cfg, name: str):
    con = sqlite3.connect(_db_path(cfg))
    try:
        con.execute("update counters set value=max(value-1,0) where name=?", (name,))
        con.commit()
    finally:
        con.close()
def iter_docs_for_jobs(jobs):
    from pathlib import Path
    cdir = Path('.knowledge/indexes/chunks'); out = {}
    for j in jobs:
        for p in sorted(cdir.glob(f"{j['doc_id']}-*.json")):
            out[j['doc_id']] = json.loads(p.read_text(encoding='utf-8')); break
    return out

./kn\utils\io.py
import pathlib, chardet, os
try:
    import pytesseract
    from PIL import Image
except Exception:
    pytesseract = None
    Image = None
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None
ROOT = pathlib.Path('.knowledge')
def ensure_dirs(cfg=None):
    (ROOT/"indexes"/"chunks").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"embeddings").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"graph").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"summaries").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"attributes").mkdir(parents=True, exist_ok=True)
    (ROOT/"indexes"/"manifests").mkdir(parents=True, exist_ok=True)
    (ROOT/"queues").mkdir(parents=True, exist_ok=True)
    (ROOT/"exports"/"dumps").mkdir(parents=True, exist_ok=True)
def read_text_safely(path: pathlib.Path):
    try:
        suffix = path.suffix.lower()
        if suffix == '.pdf' and fitz is not None:
            text_parts = []
            with fitz.open(str(path)) as doc:
                for page in doc:
                    text_parts.append(page.get_text("text"))
            txt = "\n".join(text_parts)
            if txt.strip():
                return txt
            # fall through to OCR if enabled
        # OCR for images (or empty PDFs) when enabled
        if (suffix in {'.png','.jpg','.jpeg','.tif','.tiff'} or (suffix=='.pdf' and fitz is not None)) and (os.getenv('OCR_ENABLED','false').lower()=='true') and pytesseract and Image:
            tcmd = os.getenv('TESSERACT_CMD') or ""
            if tcmd:
                try: pytesseract.pytesseract.tesseract_cmd = tcmd
                except Exception: pass
            try:
                if suffix=='.pdf' and fitz is not None:
                    # render pages to images and OCR
                    out = []
                    with fitz.open(str(path)) as doc:
                        for page in doc:
                            pix = page.get_pixmap(dpi=200)
                            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                            out.append(pytesseract.image_to_string(img))
                    ocr_txt = "\n".join(out)
                else:
                    ocr_txt = pytesseract.image_to_string(Image.open(str(path)))
                if ocr_txt.strip():
                    return ocr_txt
            except Exception:
                pass
        # default: try bytes decode with chardet
        data = path.read_bytes()
        enc = chardet.detect(data).get('encoding') or 'utf-8'
        return data.decode(enc, errors='ignore')
    except Exception:
        return None

./kn\utils\skeleton.py
import json, pathlib

SKELETON_DIR = pathlib.Path('.knowledge/indexes/attributes/doc-skeleton')

def load_skeleton_text(doc_id: str) -> str|None:
    p = SKELETON_DIR / f"{doc_id}.json"
    if not p.exists():
        return None
    try:
        data = json.loads(p.read_text(encoding='utf-8'))
        val = data.get('value') or {}
        parts = []
        if isinstance(val, dict):
            outline = val.get('outline')
            if outline:
                parts.append("Outline:\n" + outline)
            s100 = val.get('summary100') or val.get('summary')
            if s100:
                parts.append("Summary:\n" + s100)
            kp = val.get('keyphrases')
            if kp:
                parts.append("Keyphrases: " + kp)
        return "\n\n".join(parts) if parts else None
    except Exception:
        return None



./kn\utils\cache.py
import hashlib, json, pathlib
from typing import Any, Optional

CACHE_DIR = pathlib.Path('.knowledge/cache/llm')

def _hash_key(s: str) -> str:
    return hashlib.sha1(s.encode('utf-8', errors='ignore')).hexdigest()

def get_cached_response(key: str) -> Optional[str]:
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    p = CACHE_DIR / (_hash_key(key) + '.json')
    if not p.exists():
        return None
    try:
        data = json.loads(p.read_text(encoding='utf-8'))
        return data.get('content')
    except Exception:
        return None

def set_cached_response(key: str, content: str) -> None:
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    p = CACHE_DIR / (_hash_key(key) + '.json')
    p.write_text(json.dumps({'content': content}, ensure_ascii=False), encoding='utf-8')



./kn\utils\__init__.py
"""Utility helpers for kn package."""

__all__ = []




./kn\utils\llm_client.py
"""Backward-compatible LLM client that delegates to the gateway."""

from __future__ import annotations

from typing import Any, Dict, Optional

from kn.llm_gateway.client import submit_chat_request


def chat(
    prompt: str,
    max_tokens: int = 512,
    temperature: float = 0.2,
    overrides: Optional[Dict[str, Any]] = None,
    cache_key: Optional[str] = None,
    plugin_name: Optional[str] = None,
    force: bool = False,
    overwrite: bool = False,
) -> str:
    """Proxy chat helper used by legacy code paths."""

    return submit_chat_request(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        overrides=overrides,
        cache_key=cache_key,
        plugin_name=plugin_name,
        force=force,
        overwrite=overwrite,
    )








./kn\llm_gateway\__init__.py
"""LLM gateway package for managing service workers and queue interactions."""

from .client import submit_chat_request, submit_generic_request
from .runner import run_gateway_service

__all__ = [
    "submit_chat_request",
    "submit_generic_request",
    "run_gateway_service",
]




./kn\llm_gateway\models.py
"""Data models for LLM gateway requests and responses."""

from __future__ import annotations

import dataclasses
import time
from typing import Any, Dict, Optional


@dataclasses.dataclass(slots=True)
class GatewayRequest:
    """A unit of work destined for an LLM service agent."""

    request_id: str
    service: str
    model: str
    payload: Dict[str, Any]
    metadata: Dict[str, Any]
    created_ts: float = dataclasses.field(default_factory=lambda: time.time())

    def to_dict(self) -> Dict[str, Any]:
        return {
            "request_id": self.request_id,
            "service": self.service,
            "model": self.model,
            "payload": json.loads(json.dumps(self.payload, ensure_ascii=False, default=str)),
            "metadata": json.loads(json.dumps(self.metadata, ensure_ascii=False, default=str)),
            "created_ts": self.created_ts,
        }

    @classmethod
    def from_row(cls, row: Dict[str, Any]) -> "GatewayRequest":
        return cls(
            request_id=row["request_id"],
            service=row["service"],
            model=row["model"],
            payload=row["payload"],
            metadata=row.get("metadata", {}) or {},
            created_ts=row.get("created_ts", time.time()),
        )


@dataclasses.dataclass(slots=True)
class GatewayResponse:
    """Response emitted by an LLM service agent."""

    request_id: str
    success: bool
    content: Optional[str]
    raw: Optional[Dict[str, Any]]
    error: Optional[str]
    latency_ms: Optional[float]

    def to_dict(self) -> Dict[str, Any]:
        return {
            "request_id": self.request_id,
            "success": self.success,
            "content": self.content,
            "raw": self.raw,
            "error": self.error,
            "latency_ms": self.latency_ms,
        }








./kn\llm_gateway\storage.py
"""Persistence helpers for the LLM gateway SQLite queue."""

from __future__ import annotations

import json
import sqlite3
import threading
import time
from pathlib import Path
from typing import Any, Dict, Optional

from .models import GatewayRequest, GatewayResponse

_SCHEMA = """
CREATE TABLE IF NOT EXISTS requests (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    request_id TEXT UNIQUE,
    service TEXT NOT NULL,
    model TEXT NOT NULL,
    payload TEXT NOT NULL,
    metadata TEXT,
    created_ts REAL NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending'
);

CREATE TABLE IF NOT EXISTS responses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    request_id TEXT UNIQUE,
    success INTEGER NOT NULL,
    content TEXT,
    raw TEXT,
    error TEXT,
    latency_ms REAL,
    created_ts REAL NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_requests_status ON requests(status);
CREATE INDEX IF NOT EXISTS idx_requests_service ON requests(service);
"""


class QueueStorage:
    """Thin SQLite wrapper for storing gateway requests and responses."""

    def __init__(self, db_path: Path, busy_timeout_ms: int = 5000):
        self.db_path = Path(db_path)
        self._lock = threading.Lock()
        self._busy_timeout_ms = busy_timeout_ms
        self._ensure_schema()

    def _connect(self) -> sqlite3.Connection:
        con = sqlite3.connect(str(self.db_path))
        con.execute(f"PRAGMA busy_timeout = {self._busy_timeout_ms}")
        return con

    def _ensure_schema(self):
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        with self._connect() as con:
            con.executescript(_SCHEMA)
            con.commit()

    def enqueue(self, req: GatewayRequest):
        payload_json = _safe_json(req.payload)
        metadata_json = _safe_json(req.metadata or {})
        with self._connect() as con:
            con.execute(
                "INSERT OR REPLACE INTO requests(request_id, service, model, payload, metadata, created_ts, status)"
                " VALUES(?,?,?,?,?,?, 'pending')",
                (
                    req.request_id,
                    req.service,
                    req.model,
                    payload_json,
                    metadata_json,
                    req.created_ts,
                ),
            )
            con.commit()

    def dequeue_pending(self, service: str) -> Optional[GatewayRequest]:
        with self._lock:
            with self._connect() as con:
                cur = con.execute(
                    "SELECT request_id, service, model, payload, metadata, created_ts FROM requests"
                    " WHERE status='pending' AND service=? ORDER BY id LIMIT 1",
                    (service,),
                )
                row = cur.fetchone()
                if not row:
                    return None
                con.execute("UPDATE requests SET status='running' WHERE request_id=?", (row[0],))
                con.commit()
        payload = json.loads(row[3]) if row[3] else {}
        metadata = json.loads(row[4]) if row[4] else {}
        return GatewayRequest(
            request_id=row[0],
            service=row[1],
            model=row[2],
            payload=payload,
            metadata=metadata,
            created_ts=row[5],
        )

    def store_response(self, response: GatewayResponse):
        content = _safe_text(response.content)
        raw_json = _safe_json(response.raw) if response.raw is not None else None
        error_text = _safe_text(response.error)
        with self._connect() as con:
            con.execute(
                "INSERT OR REPLACE INTO responses(request_id, success, content, raw, error, latency_ms, created_ts)"
                " VALUES(?,?,?,?,?,?,?)",
                (
                    response.request_id,
                    1 if response.success else 0,
                    content,
                    raw_json,
                    error_text,
                    response.latency_ms,
                    time.time(),
                ),
            )
            con.execute("UPDATE requests SET status=? WHERE request_id=?", ("done" if response.success else "failed", response.request_id))
            con.commit()

    def fetch_response(self, request_id: str) -> Optional[GatewayResponse]:
        with self._connect() as con:
            cur = con.execute(
                "SELECT success, content, raw, error, latency_ms FROM responses WHERE request_id=?",
                (request_id,),
            )
            row = cur.fetchone()
            if not row:
                return None
        raw = json.loads(row[2]) if row[2] else None
        return GatewayResponse(
            request_id=request_id,
            success=bool(row[0]),
            content=row[1],
            raw=raw,
            error=row[3],
            latency_ms=row[4],
        )

    def mark_failed(self, request_id: str, error_message: str):
        resp = GatewayResponse(
            request_id=request_id,
            success=False,
            content=None,
            raw=None,
            error=error_message,
            latency_ms=None,
        )
        self.store_response(resp)

    def purge_stale(self, max_age_sec: float):
        cutoff = time.time() - max_age_sec
        with self._connect() as con:
            con.execute("DELETE FROM requests WHERE status IN ('done','failed') AND created_ts<?", (cutoff,))
            con.execute("DELETE FROM responses WHERE created_ts<?", (cutoff,))
            con.commit()

    def stats(self, service: Optional[str] = None) -> Dict[str, Any]:
        where = ""
        params = ()
        if service:
            where = " WHERE service=?"
            params = (service,)
        with self._connect() as con:
            pending = con.execute(f"SELECT COUNT(*) FROM requests WHERE status='pending'" + (" AND service=?" if service else ""), params).fetchone()[0]
            running = con.execute(f"SELECT COUNT(*) FROM requests WHERE status='running'" + (" AND service=?" if service else ""), params).fetchone()[0]
        return {"pending": pending, "running": running}


def _safe_json(value: Any) -> str:
    try:
        text = json.dumps(value, ensure_ascii=False)
    except Exception:
        text = json.dumps(str(value), ensure_ascii=False)
    return _safe_text(text)


def _safe_text(value: Optional[str]) -> Optional[str]:
    if value is None:
        return None
    if not isinstance(value, str):
        value = str(value)
    return value.encode('utf-8', errors='replace').decode('utf-8')








./kn\llm_gateway\service_base.py
"""Base classes and utilities for LLM service agents."""

from __future__ import annotations

import abc
import logging
import threading
import time
from typing import Any, Dict, Optional


class ServiceAgent(abc.ABC):
    """Abstract base for a throttled, single-backend LLM agent."""

    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.logger = logging.getLogger(f"llm_service.{name}")
        self._lock = threading.Lock()
        self._last_request_ts: Optional[float] = None
        self._current_model: Optional[str] = None
        self._last_model_switch: Optional[float] = None

    # ------------------------------------------------------------------
    # Abstract API
    # ------------------------------------------------------------------
    @abc.abstractmethod
    def ensure_model_loaded(self, model: str):
        """Make sure the requested model is loaded and ready."""

    @abc.abstractmethod
    def invoke(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Perform the actual request against the backend."""

    @abc.abstractmethod
    def unload_current_model(self):
        """Attempt to unload the currently loaded model if supported."""

    @abc.abstractmethod
    def health(self) -> Dict[str, Any]:
        """Return backend-specific health information."""

    # ------------------------------------------------------------------
    # Shared helpers
    # ------------------------------------------------------------------
    def guard_rate_limits(self):
        spacing_ms = int(self.config.get("min_request_spacing_ms", 0))
        if spacing_ms <= 0:
            return
        with self._lock:
            if self._last_request_ts is None:
                self._last_request_ts = time.time()
                return
            elapsed_ms = (time.time() - self._last_request_ts) * 1000.0
            if elapsed_ms < spacing_ms:
                delay = (spacing_ms - elapsed_ms) / 1000.0
                if delay > 0:
                    time.sleep(delay)
            self._last_request_ts = time.time()

    def should_unload_idle(self) -> bool:
        idle_sec = int(self.config.get("idle_unload_sec", 0) or 0)
        if idle_sec <= 0:
            return False
        if self._last_request_ts is None:
            return False
        return (time.time() - self._last_request_ts) >= idle_sec

    def update_model_tracking(self, model: str):
        if model != self._current_model:
            self._current_model = model
            self._last_model_switch = time.time()

    def current_model(self) -> Optional[str]:
        return self._current_model

    def model_switch_info(self) -> Optional[float]:
        return self._last_model_switch








./kn\llm_gateway\services.py
"""Service agent registry and configuration loader."""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from kn.config import load_configs

from .service_base import ServiceAgent
from .services_lmstudio import LmStudioAgent
from .errors import ServiceNotConfiguredError

_REGISTRY = {
    "lmstudio": LmStudioAgent,
}


def load_service_agent(name: str, cfg: Optional[Dict[str, Any]] = None) -> ServiceAgent:
    config = cfg or load_configs().get("llm_services", {}).get(name)
    if not config:
        raise ServiceNotConfiguredError(f"LLM service '{name}' not configured")
    provider = config.get("provider") or name
    cls = _REGISTRY.get(provider)
    if not cls:
        raise ServiceNotConfiguredError(f"No service agent registered for provider '{provider}'")
    logging.getLogger("llm_service").debug("Loading service agent %s (provider=%s)", name, provider)
    return cls(name=name, config=config)








./kn\llm_gateway\client.py
"""Client-facing helpers to submit requests to the LLM gateway."""

from __future__ import annotations

import os
import time
import uuid
from pathlib import Path
from typing import Any, Dict, Optional

from kn.config import load_configs
from kn.utils.cache import get_cached_response, set_cached_response

from .errors import QueueTimeoutError, RequestRejectedError
from .models import GatewayRequest, GatewayResponse
from .storage import QueueStorage


def _storage(cfg: Optional[Dict[str, Any]] = None) -> QueueStorage:
    cfg = cfg or load_configs()
    path = cfg.get("llm_gateway", {}).get("queue_path")
    if path:
        return QueueStorage(Path(path))
    return QueueStorage(Path(cfg.get("_root", ".knowledge")) / "queues" / "llm_gateway.sqlite")


def submit_chat_request(
    prompt: str,
    max_tokens: int = 512,
    temperature: float = 0.2,
    overrides: Optional[Dict[str, Any]] = None,
    cache_key: Optional[str] = None,
    plugin_name: Optional[str] = None,
    cfg: Optional[Dict[str, Any]] = None,
    wait: bool = True,
    force: bool = False,
    overwrite: bool = False,
) -> str:
    cfg = cfg or load_configs()
    merged_overrides = _merge_overrides(cfg, overrides, plugin_name)
    service_name = merged_overrides.get("service") or cfg.get("llm", {}).get("service", "lmstudio")
    model_name = merged_overrides.get("model") or cfg.get("llm", {}).get("model")
    max_tokens = int(merged_overrides.get("max_tokens", max_tokens))
    temperature = float(merged_overrides.get("temperature", temperature))

    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature,
    }
    if "top_p" in merged_overrides:
        payload["top_p"] = merged_overrides["top_p"]

    local_models = os.getenv("LOCAL_MODELS_DIR")
    if local_models:
        payload.setdefault("extra_body", {})["local_models_dir"] = local_models

    ck = cache_key or f"{service_name}|{model_name}|{max_tokens}|{temperature}|{prompt}"
    cached = get_cached_response(ck)
    if cached is not None:
        if not force and not overwrite:
            return cached

    req_id = str(uuid.uuid4())
    metadata = {
        "cache_key": ck,
        "plugin": plugin_name,
    }
    if force or overwrite:
        metadata = dict(metadata)
        metadata['force'] = force or overwrite
        if overwrite:
            metadata['overwrite'] = True
    request = GatewayRequest(request_id=req_id, service=service_name, model=model_name, payload=payload, metadata=metadata)
    storage = _storage(cfg)
    storage.enqueue(request)

    if not wait:
        return ""

    timeout_sec = float(cfg.get("llm_gateway", {}).get("wait_timeout_sec", 600))
    deadline = time.time() + timeout_sec
    while time.time() < deadline:
        response = storage.fetch_response(request.request_id)
        if response:
            return _handle_response(response, ck)
        time.sleep(0.2)
    raise QueueTimeoutError(f"LLM gateway timed out waiting for request {req_id}")


def submit_generic_request(service: str, payload: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None, cfg: Optional[Dict[str, Any]] = None, wait: bool = True, timeout_override: Optional[float] = None) -> GatewayResponse:
    cfg = cfg or load_configs()
    req_id = str(uuid.uuid4())
    storage = _storage(cfg)
    request = GatewayRequest(request_id=req_id, service=service, model=payload.get("model", ""), payload=payload, metadata=metadata or {})
    storage.enqueue(request)
    if not wait:
        return GatewayResponse(request_id=req_id, success=True, content=None, raw=None, error=None, latency_ms=None)
    timeout_sec = float(timeout_override if timeout_override is not None else cfg.get("llm_gateway", {}).get("wait_timeout_sec", 600))
    deadline = time.time() + timeout_sec
    while time.time() < deadline:
        response = storage.fetch_response(req_id)
        if response:
            return response
        time.sleep(0.2)
    raise QueueTimeoutError(f"Timeout waiting for generic gateway request {req_id}")


def _handle_response(response: GatewayResponse, cache_key: Optional[str]) -> str:
    if not response.success:
        raise RequestRejectedError(response.error or "Gateway request failed")
    content = ""
    if response.content:
        content = response.content
    elif response.raw:
        choices = (response.raw or {}).get("choices", [])
        if choices:
            content = choices[0].get("message", {}).get("content", "")
    if cache_key and content:
        set_cached_response(cache_key, content)
    return content


def _merge_overrides(cfg: Dict[str, Any], overrides: Optional[Dict[str, Any]], plugin_name: Optional[str]) -> Dict[str, Any]:
    merged: Dict[str, Any] = {}
    plugin_block = (cfg.get("plugins") or {}).get(plugin_name or "") if plugin_name else None
    if plugin_block and plugin_block.get("llm"):
        merged.update(plugin_block["llm"])
    if overrides:
        merged.update(overrides)
    return merged



./kn\llm_gateway\runner.py
"""Gateway daemon that drains the queue and forwards to service agents."""

from __future__ import annotations

import logging
import time
from pathlib import Path
from typing import Dict, Optional

from kn.config import load_configs

from .errors import GatewayError
from .models import GatewayRequest, GatewayResponse
from .services import load_service_agent
from .storage import QueueStorage

logger = logging.getLogger("llm_gateway.runner")


def run_gateway_service(service_name: str, loop_forever: bool = True, cfg: Optional[Dict] = None):
    cfg = cfg or load_configs()
    gateway_cfg = cfg.get("llm_gateway", {})
    poll_interval_ms = int(gateway_cfg.get("poll_interval_ms", 200))
    storage = _load_storage(gateway_cfg, cfg)
    agent = load_service_agent(service_name, cfg.get("llm_services", {}).get(service_name))

    logger.info("Gateway service '%s' starting (poll=%sms)", service_name, poll_interval_ms)
    while True:
        req = storage.dequeue_pending(service_name)
        if not req:
            if not loop_forever:
                break
            time.sleep(poll_interval_ms / 1000.0)
            if agent.should_unload_idle():
                logger.info("Idle timeout reached; unloading model")
                agent.unload_current_model()
            continue

        logger.info("Processing request %s model=%s", req.request_id, req.model)
        try:
            response = _handle_request(agent, req)
        except Exception as exc:  # pylint: disable=broad-except
            logger.exception("Gateway request failed: %s", exc)
            storage.mark_failed(req.request_id, str(exc))
        else:
            storage.store_response(response)

        if not loop_forever:
            break


def _handle_request(agent, req: GatewayRequest) -> GatewayResponse:
    payload = req.payload or {}
    action = payload.get("action")
    if action == "health":
        result = agent.health()
        ok = bool(result.get("ok", True)) if isinstance(result, dict) else True
        return GatewayResponse(
            request_id=req.request_id,
            success=ok,
            content=None,
            raw=result if isinstance(result, dict) else {"result": result},
            error=None if ok else (result.get("error") if isinstance(result, dict) else "health check failed"),
            latency_ms=None,
        )

    agent.guard_rate_limits()
    agent.ensure_model_loaded(req.model)
    payload = req.payload

    max_context = int(agent.config.get("max_context_tokens", 8192))
    if _estimate_prompt_tokens(payload) > max_context:
        raise GatewayError(f"Context too large for model {req.model}")

    result = agent.invoke(payload)
    latency_ms = result.pop("_latency_ms", None)
    content = None
    if result.get("choices"):
        content = result["choices"][0].get("message", {}).get("content")
    return GatewayResponse(
        request_id=req.request_id,
        success=True,
        content=content,
        raw=result,
        error=None,
        latency_ms=latency_ms,
    )


def _estimate_prompt_tokens(payload: Dict) -> int:
    messages = payload.get("messages", [])
    total_chars = sum(len(m.get("content", "")) for m in messages)
    return int(total_chars / 3.5) + 1


def _load_storage(gateway_cfg: Dict, cfg: Dict) -> QueueStorage:
    queue_path = gateway_cfg.get("queue_path")
    if queue_path:
        return QueueStorage(Path(queue_path))
    root = Path(cfg.get("_root", ".knowledge"))
    return QueueStorage(root / "queues" / "llm_gateway.sqlite")








./kn\llm_gateway\errors.py
class GatewayError(Exception):
    """Base exception for gateway-level errors."""


class QueueTimeoutError(GatewayError):
    """Raised when a request response is not received within the wait window."""


class RequestRejectedError(GatewayError):
    """Raised when the downstream service reports a failure for the request."""


class ServiceNotConfiguredError(GatewayError):
    """Raised when a requested service does not have a configuration block."""


./kn\llm_gateway\services_lmstudio.py
"""LM Studio-specific service agent implementation."""

from __future__ import annotations

import logging
import time
from typing import Any, Dict

import requests

from .service_base import ServiceAgent


class LmStudioAgent(ServiceAgent):
    """Agent that serializes requests to a single LM Studio backend."""

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.session = requests.Session()
        self.base_url = config.get("base_url", "http://127.0.0.1:12345/v1").rstrip("/")
        self.api_key = config.get("api_key", "lm-studio")
        self.timeout = int(config.get("timeout", 120))
        self.manage_models = bool(config.get("manage_models"))

    # ------------------------------------------------------------------
    def ensure_model_loaded(self, model: str):
        if not self.manage_models:
            if self.current_model() != model:
                self.logger.debug("Skipping explicit load for %s (manage_models disabled)", model)
                self.update_model_tracking(model)
            return
        force_reload = bool(self.config.get("force_model_reload"))
        if self.current_model() == model and not force_reload:
            return
        if self.config.get("unload_on_switch", True) and self.current_model() and self.current_model() != model:
            self.unload_current_model()
        payload = {"model": model}
        response = self.session.post(
            f"{self.base_url}/models/load",
            headers=self._headers(),
            json=payload,
            timeout=self.timeout,
        )
        if response.status_code != 200:
            response.raise_for_status()
        self.logger.info("Loaded model %s", model)
        self.update_model_tracking(model)

    def invoke(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        request_started = time.time()
        response = self.session.post(
            f"{self.base_url}/chat/completions",
            headers=self._headers(),
            json=payload,
            timeout=self.timeout,
        )
        latency_ms = (time.time() - request_started) * 1000.0
        response.raise_for_status()
        data = response.json()
        data.setdefault("_latency_ms", latency_ms)
        return data

    def unload_current_model(self):
        if not self.manage_models:
            self.logger.debug("Skipping explicit unload (manage_models disabled)")
            self.update_model_tracking(model="")
            return
        model = self.current_model()
        if not model:
            return
        try:
            response = self.session.post(
                f"{self.base_url}/models/unload",
                headers=self._headers(),
                json={"model": model},
                timeout=self.timeout,
            )
            if response.status_code == 200:
                self.logger.info("Unloaded model %s", model)
            else:
                self.logger.warning("Failed to unload model %s: %s", model, response.text)
        finally:
            self.update_model_tracking(model="")

    def health(self) -> Dict[str, Any]:
        try:
            response = self.session.get(
                f"{self.base_url}/models",
                headers=self._headers(),
                timeout=self.timeout,
            )
            response.raise_for_status()
            data = response.json()
            return {
                "ok": True,
                "available_models": [m.get("id") for m in data.get("data", [])],
            }
        except Exception as exc:  # pylint: disable=broad-except
            return {"ok": False, "error": str(exc)}

    # ------------------------------------------------------------------
    def _headers(self) -> Dict[str, str]:
        headers = {
            "Content-Type": "application/json",
        }
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers








./plugins\attributes\summary_20w.py
import json
import pathlib
import sys

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summary-20w')
OUTDIR.mkdir(parents=True, exist_ok=True)

_ATTEMPT_LIMITS = [6000, 3500, 2000]


def _safe_text(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode('utf-8', errors='replace').decode('utf-8')


def _trim_text(text: str, limit: int) -> str:
    if len(text) <= limit:
        return text
    snippet = text[:limit]
    cut = snippet.rfind('\n\n')
    if cut >= int(limit * 0.6):
        snippet = snippet[:cut]
    return snippet + '\n\n[Truncated for length]'


def _write_attr(doc_id: str, value: str, confidence: float = 0.8, notes: dict | None = None):
    payload = {
        "doc_id": doc_id,
        "attribute": "summary-20w",
        "value": value.strip(),
        "confidence": confidence,
        "pass": "cheap",
    }
    if notes:
        payload.update(notes)
    OUTDIR.joinpath(f"{doc_id}.json").write_text(json.dumps(payload, ensure_ascii=False), encoding='utf-8')


def _fallback_value(reason: str) -> str:
    return f"[summary unavailable: {reason}]"


for line in sys.stdin:
    job = json.loads(line)
    doc_id = job.get('doc_id')
    text = job.get('text', '')
    if not text:
        print(json.dumps({"status": "skip", "doc_id": doc_id}, ensure_ascii=False))
        continue

    text = _safe_text(text)
    success = False
    last_error = None

    for limit in _ATTEMPT_LIMITS:
        trimmed = _trim_text(text, limit)
        prompt = _safe_text(f"Summarize the following in ~20 words, terse and factual. If truncated, focus on the excerpt provided.\n\n{trimmed}")
        try:
            out = chat(prompt, max_tokens=100, temperature=0.1, plugin_name='summary-20w', overwrite=True)
            out = _safe_text(out)
            _write_attr(doc_id, out)
            print(json.dumps({"status": "ok", "doc_id": doc_id, "length": len(out)}, ensure_ascii=False))
            success = True
            break
        except RequestRejectedError as exc:
            last_error = str(exc)
            if 'Context too large' in last_error:
                continue
            _write_attr(doc_id, _fallback_value('LLM request error'), notes={"error": last_error})
            print(json.dumps({"status": "error", "doc_id": doc_id, "error": last_error}, ensure_ascii=False))
            success = True
            break
        except Exception as exc:
            last_error = str(exc)
            _write_attr(doc_id, _fallback_value('unexpected error'), notes={"error": last_error})
            print(json.dumps({"status": "error", "doc_id": doc_id, "error": last_error}, ensure_ascii=False))
            success = True
            break

    if not success:
        msg = last_error or 'context too large even after truncation'
        _write_attr(doc_id, _fallback_value('context too large'), notes={"error": msg})
        print(json.dumps({"status": "error", "doc_id": doc_id, "error": msg}, ensure_ascii=False))

./plugins\attributes\topic_tags.py
import json
import pathlib
import sys

from kn.llm_gateway.client import submit_chat_request
from kn.utils.skeleton import load_skeleton_text

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/topic-tags')
OUTDIR.mkdir(parents=True, exist_ok=True)


def _safe_text(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode('utf-8', errors='replace').decode('utf-8')


for line in sys.stdin:
    job = json.loads(line)
    text = load_skeleton_text(job.get('doc_id')) or job.get('text', '')
    if not text:
        continue
    text = _safe_text(text)
    prompt = _safe_text(
        "Extract 3-7 topical tags (comma-separated, lowercase, no spaces, use-hyphens).\n"
        "Prefer domain-relevant terms.\n\n"
        f"TEXT:\n{text}\n"
    )
    out = submit_chat_request(prompt, max_tokens=64, temperature=0.2, plugin_name='topic-tags', overwrite=True)
    out = _safe_text(out)
    OUTDIR.joinpath(f"{job['doc_id']}.json").write_text(json.dumps({
        "doc_id": job['doc_id'],
        "attribute": "topic-tags",
        "value": out.strip(),
        "confidence": 0.7,
        "pass": "cheap"
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({"status": "ok", "doc_id": job['doc_id']}, ensure_ascii=False))

./plugins\attributes\glossary.py
import json
import pathlib
import sys

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/glossary')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Extract 5-12 project-specific terms from the text with brief definitions. "
    "Return a bullet list 'term: definition' focusing on unique or overloaded terms."
)

_ATTEMPT_LIMITS = [6000, 3500, 2000]


def _safe_text(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode('utf-8', errors='replace').decode('utf-8')


def _trim_text(text: str, limit: int) -> str:
    if len(text) <= limit:
        return text
    snippet = text[:limit]
    cut = snippet.rfind('\n\n')
    if cut >= int(limit * 0.6):
        snippet = snippet[:cut]
    return snippet + '\n\n[Truncated for length]'


def _write_attr(doc_id: str, value: str, confidence: float = 0.6, error: str | None = None):
    payload = {
        'doc_id': doc_id,
        'attribute': 'glossary',
        'value': value.strip(),
        'confidence': confidence,
    }
    if error:
        payload['error'] = error
    OUTDIR.joinpath(f"{doc_id}.json").write_text(json.dumps(payload, ensure_ascii=False), encoding='utf-8')


for line in sys.stdin:
    job = json.loads(line)
    doc_id = job.get('doc_id')
    text = job.get('text', '') or ''
    if not text.strip():
        print(json.dumps({'status': 'skip', 'doc_id': doc_id}))
        continue

    text = _safe_text(text)
    success = False
    last_error = None

    for limit in _ATTEMPT_LIMITS:
        trimmed = _trim_text(text, limit)
        try:
            out = chat(PROMPT + "\n\n" + trimmed, max_tokens=300, temperature=0.2, plugin_name='glossary', overwrite=True)
            out = _safe_text(out)
            _write_attr(doc_id, out)
            print(json.dumps({'status': 'ok', 'doc_id': doc_id}))
            success = True
            break
        except RequestRejectedError as exc:
            last_error = str(exc)
            if 'Context too large' in last_error or exc.args and exc.args[0].startswith('404'):
                continue
            _write_attr(doc_id, '[glossary unavailable: gateway error]', error=last_error)
            print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': last_error}))
            success = True
            break
        except Exception as exc:
            last_error = str(exc)
            _write_attr(doc_id, '[glossary unavailable: unexpected error]', error=last_error)
            print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': last_error}))
            success = True
            break

    if not success:
        msg = last_error or 'context too large or endpoint missing'
        _write_attr(doc_id, '[glossary unavailable: context too large]', error=msg)
        print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': msg}))



./plugins\attributes\requirements.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/requirements')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Extract requirement-like statements (shall/should/must or implied constraints). "
    "Output 5-15 bullets, each concise and testable. If none, return an empty list."
)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    out = chat(PROMPT + "\n\n" + text, max_tokens=350, temperature=0.2, plugin_name='requirements', overwrite=True)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'requirements', 'value': out.strip(), 'confidence': 0.6
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\todo_items.py
import sys, json, pathlib, re

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/todo-items')
OUTDIR.mkdir(parents=True, exist_ok=True)

PAT = re.compile(r"(?im)^(?:#|//|\*|[-\d.]*)?\s*(TODO|FIXME|NOTE)[:\-\s]+(.{4,120})$")

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    items = []
    for m in PAT.finditer(text):
        items.append({'kind': m.group(1).upper(), 'text': m.group(2).strip()})
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'todo-items', 'value': items, 'confidence': 0.6
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\faq_pairs.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/faq-pairs')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Generate 5-10 FAQ Q/A pairs strictly grounded in the provided text. "
    "Format as '- Q: ...\n  A: ...'. Avoid hallucinations; answer 'unknown' if not supported."
)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    out = chat(PROMPT + "\n\n" + text, max_tokens=600, temperature=0.2, plugin_name='faq-pairs', overwrite=True)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'faq-pairs', 'value': out.strip(), 'confidence': 0.6
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\summaries.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summaries')
OUTDIR.mkdir(parents=True, exist_ok=True)

TEMPLATES = {
    'short':   ("Summarize the text in ~50 words, terse, factual.", 120),
    'medium':  ("Summarize the text in ~150 words, clear sections if relevant.", 360),
    'long':    ("Summarize the text in ~400 words, comprehensive but concise.", 900),
    'outline': ("Produce a hierarchical outline (bullets) of the main ideas.", 360),
}

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    payload = job.get('payload') or {}
    mode = (payload.get('mode') or 'short').lower()
    words = payload.get('words')
    tpl, max_tokens = TEMPLATES.get(mode, TEMPLATES['short'])
    if words:
        tpl = f"Summarize the text in ~{words} words, terse and factual."
        max_tokens = int(words) * 3
    prompt = tpl + "\n\n" + text
    out = chat(prompt, max_tokens=max_tokens, temperature=0.2, plugin_name='summaries', overwrite=True)
    (OUTDIR / f"{job['doc_id']}_{mode}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': f'summary-{mode}', 'value': out.strip(), 'mode': mode, 'confidence': 0.7
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id'],'mode':mode}))



./plugins\attributes\keyphrases.py
import json
import pathlib
import sys

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/keyphrases')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Extract 8-20 keyphrases capturing entities, terms, and concepts.\n"
    "- Return a comma-separated list\n"
    "- Lowercase, hyphenate multi-word terms, no spaces around commas\n"
    "- Prefer domain-specific terminology\n\nTEXT:\n{body}\n"
)

_ATTEMPT_LIMITS = [6000, 3500, 2000]


def _safe_text(text: str) -> str:
    if not isinstance(text, str):
        text = str(text)
    return text.encode('utf-8', errors='replace').decode('utf-8')


def _trim_text(text: str, limit: int) -> str:
    if len(text) <= limit:
        return text
    snippet = text[:limit]
    cut = snippet.rfind('\n\n')
    if cut >= int(limit * 0.6):
        snippet = snippet[:cut]
    return snippet + '\n\n[Truncated for length]'


def _write_attr(doc_id: str, value, confidence=0.7, error: str | None = None):
    payload = {
        'doc_id': doc_id,
        'attribute': 'keyphrases',
        'value': value,
        'confidence': confidence,
    }
    if error:
        payload['error'] = error
    OUTDIR.joinpath(f"{doc_id}.json").write_text(json.dumps(payload, ensure_ascii=False), encoding='utf-8')


for line in sys.stdin:
    job = json.loads(line)
    doc_id = job.get('doc_id')
    text = job.get('text', '') or ''
    if not text.strip():
        print(json.dumps({'status': 'skip', 'doc_id': doc_id}))
        continue

    text = _safe_text(text)
    success = False
    last_error = None

    for limit in _ATTEMPT_LIMITS:
        trimmed = _trim_text(text, limit)
        prompt = PROMPT.format(body=trimmed)
        try:
            out = chat(prompt, max_tokens=120, temperature=0.2, plugin_name='keyphrases', overwrite=True)
            out = _safe_text(out)
            tags = [t.strip() for t in out.split(',') if t.strip()]
            _write_attr(doc_id, tags)
            print(json.dumps({'status': 'ok', 'doc_id': doc_id, 'count': len(tags)}))
            success = True
            break
        except RequestRejectedError as exc:
            last_error = str(exc)
            if 'Context too large' in last_error:
                continue
            _write_attr(doc_id, [], error=last_error)
            print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': last_error}))
            success = True
            break
        except Exception as exc:
            last_error = str(exc)
            _write_attr(doc_id, [], error=last_error)
            print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': last_error}))
            success = True
            break

    if not success:
        msg = last_error or 'context too large even after truncation'
        _write_attr(doc_id, 'summary_unavailable:context-too-large', error=msg)
        print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': msg}))



./plugins\attributes\bridge_candidates.py
import sys, json, pathlib, re
from collections import defaultdict

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/bridge-candidates')
OUTDIR.mkdir(parents=True, exist_ok=True)

CHUNKS_DIR = pathlib.Path('.knowledge/indexes/chunks')

def naive_entities(text: str):
    return list(set(re.findall(r"[A-Z][a-zA-Z0-9_]+|\b[A-Za-z0-9_]+\.\w+|#\w+", text)))[:200]

def build_entity_domains_map(limit_files: int|None=None):
    ent_to_domains: dict[str,set[str]] = defaultdict(set)
    count = 0
    for p in CHUNKS_DIR.glob('*.json'):
        rec = json.loads(p.read_text(encoding='utf-8'))
        dom = rec.get('meta',{}).get('domain','root')
        for e in naive_entities(rec.get('text','')):
            ent_to_domains[e].add(dom)
        count += 1
        if limit_files and count >= limit_files:
            break
    return ent_to_domains

ENT_DOMAINS = build_entity_domains_map()

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    dom = (job.get('meta') or {}).get('domain','root')
    ents = naive_entities(text)
    bridges = []
    for e in ents:
        doms = ENT_DOMAINS.get(e, set())
        other = [d for d in doms if d != dom]
        if other:
            bridges.append({'entity': e, 'other_domains': sorted(other)})
    # rank by number of other domains desc
    bridges.sort(key=lambda x: (-len(x['other_domains']), x['entity'].lower()))
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'bridge-candidates', 'value': bridges[:25]
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\risk_scan.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/risk-scan')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Assess the following text for risks. Return JSON with fields:"
    " {{contradictions:boolean, speculation:boolean, outdated:boolean, pii_hint:boolean, notes:string}}."
    " Keep notes <= 40 words.\n\nTEXT:\n{body}\n"
)

for line in sys.stdin:
    job = json.loads(line)
    text = load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
    if not text.strip():
        continue
    raw = chat(PROMPT.format(body=text), max_tokens=160, temperature=0.1, plugin_name='risk-scan', overwrite=True)
    # best-effort JSON parse; if fails, wrap as notes
    try:
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError()
    except Exception:
        data = {'contradictions': False, 'speculation': False, 'outdated': False, 'pii_hint': False, 'notes': raw.strip()[:240]}
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'risk-scan', 'value': data
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\recent_summary.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/recent-summary')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Summarize the most salient updates and current state. Be concise (<=120 words)."
    " If there are TODOs or open questions, list them as short bullets.\n\nTEXT:\n{body}\n"
)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    out = chat(PROMPT.format(body=text), max_tokens=220, temperature=0.2, plugin_name='recent-summary', overwrite=True)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'recent-summary', 'value': out.strip(), 'confidence': 0.7
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\summary_short.py
import sys
import json
import pathlib
from kn.llm_gateway.client import submit_chat_request

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summary-short')
OUTDIR.mkdir(parents=True, exist_ok=True)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text', '')
    if not text:
        print(json.dumps({'status': 'skip', 'doc_id': job.get('doc_id')}, ensure_ascii=False))
        continue
    doc_id = job.get('doc_id')
    payload = job.get('payload') or {}
    prompt = payload.get('prompt') or "Summarize the text in ~120 words, clear and structured.\n\n" + text
    try:
        out = submit_chat_request(
            prompt,
            max_tokens=payload.get('max_tokens', job.get('max_tokens', 512)),
            temperature=payload.get('temperature', 0.2),
            overrides=payload.get('llm'),
            cache_key=f"summary-short|{doc_id}",
            plugin_name='summary-short',
            overwrite=True
        )
    except Exception as exc:
        print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': str(exc)}, ensure_ascii=False))
        continue
    rec = {
        'doc_id': doc_id,
        'attribute': 'summary-short',
        'value': out.strip(),
        'confidence': 0.75,
        'pass': payload.get('pass', 'default')
    }
    OUTDIR.joinpath(f"{doc_id}.json").write_text(json.dumps(rec, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status': 'ok', 'doc_id': doc_id, 'length': len(out)}, ensure_ascii=False))



./plugins\attributes\summary_medium.py
import sys, json, pathlib

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text


OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summary-medium')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = "Summarize the text in ~150 words, clear, objective.\n\n{body}\n"
_ATTEMPT_LIMITS = [10000, 6000, 3500]


def _safe_text(text: str) -> str:
    if not isinstance(text, str):
        text = str(text or '')
    return text.encode('utf-8', errors='replace').decode('utf-8')


def _trim_text(text: str, limit: int) -> str:
    if len(text) <= limit:
        return text
    snippet = text[:limit]
    cut = snippet.rfind('\n\n')
    if cut >= int(limit * 0.6):
        snippet = snippet[:cut]
    return snippet + '\n\n[Truncated for length]'


def _write_attr(doc_id: str, value: str, confidence: float = 0.7, error: str | None = None):
    payload = {
        'doc_id': doc_id,
        'attribute': 'summary-medium',
        'value': value.strip(),
        'confidence': confidence,
    }
    if error:
        payload['error'] = error
    OUTDIR.joinpath(f"{doc_id}.json").write_text(json.dumps(payload, ensure_ascii=False), encoding='utf-8')


def _fallback(reason: str) -> str:
    return f"[summary unavailable: {reason}]"


for line in sys.stdin:
    job = json.loads(line)
    doc_id = job.get('doc_id')
    text = load_skeleton_text(doc_id) or job.get('text', '') or ''
    if not text.strip():
        print(json.dumps({'status': 'skip', 'doc_id': doc_id}, ensure_ascii=False))
        continue

    text = _safe_text(text)
    success = False
    last_err = None

    for limit in _ATTEMPT_LIMITS:
        trimmed = _trim_text(text, limit)
        prompt = PROMPT.format(body=trimmed)
        try:
            out = chat(prompt, max_tokens=380, temperature=0.2, plugin_name='summary-medium', overwrite=True)
            out = _safe_text(out)
            _write_attr(doc_id, out)
            print(json.dumps({'status': 'ok', 'doc_id': doc_id, 'length': len(out)}, ensure_ascii=False))
            success = True
            break
        except RequestRejectedError as exc:
            last_err = str(exc)
            if 'Context too large' in last_err:
                continue
            _write_attr(doc_id, _fallback('LLM request error'), error=last_err)
            print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': last_err}, ensure_ascii=False))
            success = True
            break
        except Exception as exc:
            last_err = str(exc)
            _write_attr(doc_id, _fallback('unexpected error'), error=last_err)
            print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': last_err}, ensure_ascii=False))
            success = True
            break

    if not success:
        reason = last_err or 'context too large'
        _write_attr(doc_id, _fallback('context too large'), error=reason)
        print(json.dumps({'status': 'error', 'doc_id': doc_id, 'error': reason}, ensure_ascii=False))



./plugins\attributes\summary_long.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summary-long')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = "Summarize the text in ~400 words, comprehensive but concise.\n\n{body}\n"

for line in sys.stdin:
    job = json.loads(line)
    text = load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
    if not text.strip():
        continue
    out = chat(PROMPT.format(body=text), max_tokens=900, temperature=0.2, plugin_name='summary-long', overwrite=True)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'summary-long', 'value': out.strip(), 'confidence': 0.7
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\summary_outline.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/summary-outline')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = "Produce a hierarchical outline (bulleted) of the main ideas. Keep concise.\n\n{body}\n"

for line in sys.stdin:
    job = json.loads(line)
    text = load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
    if not text.strip():
        continue
    out = chat(PROMPT.format(body=text), max_tokens=360, temperature=0.2, plugin_name='summary-outline', overwrite=True)
    (OUTDIR / f"{job['doc_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'attribute': 'summary-outline', 'value': out.strip(), 'confidence': 0.7
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':job['doc_id']}))



./plugins\attributes\multi_basic.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUT = pathlib.Path('.knowledge/indexes/attributes')
OUT.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "You are given a document. Produce a compact JSON with keys:"
    " summary_short (~50w), summary_medium (~150w), outline (bulleted), tags (3-7 comma-separated), risk (JSON with contradictions/speculation/outdated/pii_hint boolean and notes<=40w).\n"
    "Return only JSON.\n\nTEXT:\n{body}\n"
)

def write_attr(attr, doc_id, value, extra=None):
    (OUT/attr).mkdir(parents=True, exist_ok=True)
    rec = {'doc_id': doc_id, 'attribute': attr, 'value': value}
    if isinstance(extra, dict): rec.update(extra)
    (OUT/attr/f"{doc_id}.json").write_text(json.dumps(rec, ensure_ascii=False), encoding='utf-8')

for line in sys.stdin:
    job = json.loads(line)
    text = load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
    if not text.strip():
        continue
    doc_id = job.get('doc_id')
    # Allow per-plugin model override via payload
    overrides = (job.get('payload') or {}).get('llm') or {}
    # keep under tighter limits to reduce timeouts
    raw = chat(PROMPT.format(body=text), max_tokens=400, temperature=0.2, overrides=overrides, plugin_name='multi-basic', overwrite=True)
    data = {}
    try:
        data = json.loads(raw)
    except Exception:
        # best-effort parse: write whole text as medium summary
        data = {'summary_medium': raw.strip()}
    # write individual attribute files
    if 'summary_short' in data:
        write_attr('summary-short', doc_id, data['summary_short'], {'confidence':0.7})
    if 'summary_medium' in data:
        write_attr('summary-medium', doc_id, data['summary_medium'], {'confidence':0.7})
    if 'outline' in data:
        write_attr('summary-outline', doc_id, data['outline'], {'confidence':0.7})
    tags = data.get('tags')
    if isinstance(tags, str):
        write_attr('keyphrases', doc_id, tags, {'confidence':0.7})
    risk = data.get('risk')
    if isinstance(risk, dict):
        write_attr('risk-scan', doc_id, risk)
    print(json.dumps({'status':'ok','doc_id':doc_id}))



./plugins\attributes\doc_skeleton.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/doc-skeleton')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "Produce a compact JSON skeleton for the document with keys: outline (bulleted), summary100 (~100 words), keyphrases (comma-separated)."
    " Return only JSON.\n\nTEXT:\n{body}\n"
)

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    doc_id = job.get('doc_id')
    overrides = (job.get('payload') or {}).get('llm') or {}
    # Qwen 32B has ~12k context; keep outputs compact
    raw = chat(PROMPT.format(body=text), max_tokens=240, temperature=0.2, overrides=overrides, plugin_name='doc-skeleton', overwrite=True)
    try:
        data = json.loads(raw)
    except Exception:
        data = {'summary100': raw.strip()}
    (OUTDIR/f"{doc_id}.json").write_text(json.dumps({'doc_id':doc_id,'attribute':'doc-skeleton','value':data}, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':doc_id}))



./plugins\attributes\chunk_summary.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/chunk-summary')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = "Summarize this chunk in 2-3 sentences, terse and factual.\n\n{body}\n"

for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text','') or ''
    if not text.strip():
        continue
    out = chat(PROMPT.format(body=text), max_tokens=120, temperature=0.2, plugin_name='chunk-summary', overwrite=True)
    (OUTDIR / f"{job['chunk_id']}.json").write_text(json.dumps({
        'doc_id': job['doc_id'], 'chunk_id': job.get('chunk_id'), 'attribute': 'chunk-summary', 'value': out.strip(), 'confidence': 0.7
    }, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','chunk_id': job.get('chunk_id')}))



./plugins\attributes\doc_reduce.py
import sys, json, pathlib
from kn.utils.llm_client import chat

CHUNK_SUM_DIR = pathlib.Path('.knowledge/indexes/attributes/chunk-summary')
OUTDIR = pathlib.Path('.knowledge/indexes/attributes/doc-reduce')
OUTDIR.mkdir(parents=True, exist_ok=True)

PROMPT = (
    "You are given ordered chunk summaries for a single document.\n"
    "Produce a compact JSON with keys: outline (bulleted), summary (~150 words), tags (3-7 comma-separated).\n"
    "Return only JSON.\n\nSUMMARIES:\n{body}\n"
)

def load_chunk_summaries(doc_id: str):
    items = []
    for p in sorted(CHUNK_SUM_DIR.glob('*.json')):
        try:
            rec = json.loads(p.read_text(encoding='utf-8'))
            if rec.get('doc_id') == doc_id:
                items.append(rec)
        except Exception:
            continue
    # try to sort by chunk_id numeric tail if present in metadata; fallback original order
    def _key(r):
        cid = r.get('chunk_id','')
        try:
            return int(cid.split('-')[-1], 16)
        except Exception:
            return 0
    items.sort(key=_key)
    return items

for line in sys.stdin:
    job = json.loads(line)
    doc_id = job.get('doc_id')
    if not doc_id:
        continue
    chunks = load_chunk_summaries(doc_id)
    if not chunks:
        continue
    ordered = '\n- '.join([''] + [c.get('value','') for c in chunks])
    raw = chat(PROMPT.format(body=ordered), max_tokens=360, temperature=0.2, plugin_name='doc-reduce', overwrite=True)
    try:
        data = json.loads(raw)
    except Exception:
        data = {'summary': raw.strip()}
    (OUTDIR/f"{doc_id}.json").write_text(json.dumps({'doc_id':doc_id,'attribute':'doc-reduce','value':data}, ensure_ascii=False), encoding='utf-8')
    print(json.dumps({'status':'ok','doc_id':doc_id}))



./plugins\attributes\pii_scan.py
import sys, json, re, pathlib

OUTDIR = pathlib.Path('.knowledge/indexes/attributes/pii-scan')
OUTDIR.mkdir(parents=True, exist_ok=True)

EMAIL = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE = re.compile(r"\+?\d[\d\s().-]{6,}\d")
CREDIT = re.compile(r"\b(?:\d[ -]*?){13,16}\b")


def score_flags(text):
    findings = []
    for m in EMAIL.findall(text or ""):
        findings.append({"type": "email", "value": m, "severity": 0.5})
    for m in PHONE.findall(text or ""):
        findings.append({"type": "phone", "value": m, "severity": 0.4})
    for m in CREDIT.findall(text or ""):
        findings.append({"type": "credit", "value": m, "severity": 0.9})
    risk = max((f["severity"] for f in findings), default=0.0)
    return findings, risk


for line in sys.stdin:
    job = json.loads(line)
    text = job.get('text', '') or ""
    findings, risk = score_flags(text)
    OUTDIR.joinpath(f"{job['doc_id']}.json").write_text(
        json.dumps(
            {
                "doc_id": job['doc_id'],
                "attribute": "pii-scan",
                "value": findings,
                "risk": risk,
                "confidence": 0.6,
            },
            ensure_ascii=False,
        ),
        encoding='utf-8',
    )
    print(json.dumps({"status": "ok", "doc_id": job['doc_id']}))



