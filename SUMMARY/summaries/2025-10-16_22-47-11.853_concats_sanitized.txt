Here is the file tree and contents of all files in the project:
target_folder: .
-- rag (361914 bytes)
-- .env (193 bytes)
-- .env.example (196 bytes)
X-- .git (skipped)
X-- .gitignore (612 bytes)
X-- .knowledge (skipped)
X-- .venv (skipped)
-- DESIGN.md (42141 bytes)
-- DESIGN_additions.md (59188 bytes)
X-- FAE (skipped)
-- README.md (4168 bytes)
X-- SUMMARY (skipped)
-- bin (138924 bytes)
-- dashboard_server.py (29039 bytes)
-- dashboard_static (47609 bytes)
-- index.html (47609 bytes)
-- enrich_worker.py (8064 bytes)
-- export_graph.py (771 bytes)
-- export_monofile.py (724 bytes)
-- ingest_build_graph.py (956 bytes)
-- jobs_reset.py (1103 bytes)
-- jobs_status.py (9377 bytes)
-- llm_gateway_service.py (984 bytes)
-- plan_enqueue.py (4393 bytes)
-- planner_llm.py (3719 bytes)
-- query_rag.py (413 bytes)
-- render_report.py (18985 bytes)
-- report_attrs.py (1348 bytes)
-- report_communities.py (697 bytes)
-- test_llm_gateway.py (1751 bytes)
-- test_lmstudio.py (8575 bytes)
-- watch_daemon.py (416 bytes)
-- kn (61880 bytes)
-- __init__.py (0 bytes)
X-- __pycache__ (skipped)
-- chunking.py (2810 bytes)
-- config.py (5288 bytes)
-- embeddings.py (791 bytes)
-- exporter.py (1601 bytes)
-- file_index.py (2615 bytes)
-- graph_nx.py (1878 bytes)
-- hashing.py (278 bytes)
-- jobs_sqlite.py (6727 bytes)
-- llm_gateway (25951 bytes)
-- __init__.py (297 bytes)
X-- __pycache__ (skipped)
-- client.py (5072 bytes)
-- errors.py (470 bytes)
-- models.py (1891 bytes)
-- runner.py (3651 bytes)
-- service_base.py (2862 bytes)
-- services.py (996 bytes)
-- services_lmstudio.py (3983 bytes)
-- storage.py (6729 bytes)
-- retrieval.py (4150 bytes)
-- utils (5149 bytes)
-- __init__.py (59 bytes)
X-- __pycache__ (skipped)
-- cache.py (795 bytes)
-- io.py (2633 bytes)
-- llm_client.py (794 bytes)
-- skeleton.py (868 bytes)
-- vector_hnsw.py (4642 bytes)
X-- kn-graphRAG-starter.zip (16255 bytes)
X-- on.tool (4833 bytes)
X-- planner.log (84 bytes)
-- plugins (31725 bytes)
-- attributes (31725 bytes)
-- bridge_candidates.py (1752 bytes)
-- chunk_summary.py (824 bytes)
-- doc_reduce.py (1778 bytes)
-- doc_skeleton.py (1097 bytes)
-- faq_pairs.py (868 bytes)
-- glossary.py (2949 bytes)
-- keyphrases.py (3004 bytes)
-- multi_basic.py (2190 bytes)
-- pii_scan.py (1369 bytes)
-- recent_summary.py (893 bytes)
-- requirements.py (879 bytes)
-- risk_scan.py (1303 bytes)
-- summaries.py (1406 bytes)
-- summary_20w.py (3093 bytes)
-- summary_long.py (870 bytes)
-- summary_medium.py (3055 bytes)
-- summary_outline.py (893 bytes)
-- summary_short.py (1500 bytes)
-- todo_items.py (725 bytes)
-- topic_tags.py (1277 bytes)
-- pyproject.toml (153 bytes)
-- requirements-win.txt (197 bytes)
-- requirements.txt (317 bytes)
-- setup.ps1 (1048 bytes)
X-- worker.log (0 bytes).README.md
kn-graphRAG-starter

Local-first Graph RAG toolkit: ingest a codecontent repo, chunk it, embed, build a graph, enrich with attribute plugins, and query with lightweight retrieval. Optimized for Windows local LLMs (LM Studio), but works crossplatform.

Features
- Ingest and incremental updates over a target repo or folder
- Chunking embeddings (SentenceTransformers)
- Graph build community summaries (NetworkX Louvain)
- Attribute plugins (summaries, keyphrases, glossary, PII scan, FAQs, etc.)
- Simple dashboardAPI to planenqueue jobs and inspect coverage

Quickstart (Windows PowerShell)
1) Python 3.10 installed
2) From the project root, run:
- .u200bsetup.ps1 -RepoPath ".FAE" -Model "llama-3.1-8b-instruct"

What it does:
- Creates .venv, installs dependencies from requirements-win.txt
- Writes a .env with defaults (LLM and embedding models, repo path, knowledge root)
- Runs initial ingest: scan chunk embed graph summarize

LM Studio optional: start a server at http:localhost:12345 (or adjust OPENAI_BASE_URL in .env).

Manual setup
If you prefer manual steps:
1) Create and activate a venv, then install deps
- Windows: python -m venv .venv; . ..venvScriptsActivate.ps1; pip install -r requirements-win.txt
- NonWindows: python -m venv .venv; source .venvbinactivate; pip install -r requirements.txt
2) Configure .env (autogenerated by setup.ps1, example keys):
- OPENAI_BASE_URL (e.g., http:127.0.0.1:12345v1)
- OPENAI_API_KEY (placeholder for local providers)
- OPENAI_MODEL (chatinstruct model id)
- EMBED_MODELBAAIbge-small-en-v1.5
- REPO_PATH.FAE
- KN_ROOT.knowledge
- OCR_ENABLEDfalse
3) Ingest a repofolder:
- python biningest_build_graph.py --repo path --full

Common workflows
Query
bash
python binquery_rag.py --q "What does the planner do?" --topk 12


Dashboard API server
bash
python bindashboard_server.py

Open http:localhost:5051 for the minimal dashboard.

Selected endpoints:
- GET apistatus system status, plugin coverage, queue, LLM health
- POST apiingest repo, full start ingest
- POST apiplan plugins, only_missing, limit, ... plan jobs
- POST apiworkerstart plugins, batch start a worker
- POST apiworkerstop id? stop a worker (or all)

Attribute enrichment worker
bash
python binenrich_worker.py --plugins summaries,keyphrases --batch 32 --watch

Runs continuously with --watch, pulling jobs from .knowledgequeuesjobs.sqlite.

Exports and reports
- binexport_graph.py export graph
- binexport_monofile.py export consolidated artifacts
- binrender_report.py binreport_ static coveragereport generation

Data layout (.knowledge)
- indexeschunks chunk JSONs
- indexesattributesplugin plugin outputs
- queuesjobs.sqlite job queue
- exportsreports generated report site

Plugins
See pluginsattributes for available attribute extractors:
- summaries, topic-tags, pii-scan, glossary, requirements, todo-items,
faq-pairs, keyphrases, bridge-candidates, risk-scan, recent-summary, etc.

Configuration
kn.config.load_configs() merges defaults and .knowledgeconfig.yml.
- Plugin defaults are persisted to .knowledgeconfigmodels.yml under plugins.
- The dashboard exposes GETPOST apipluginsconfig to readupdate these.

Notes (Windows)
- requirements-win.txt includes python-magic-bin and references CUDA wheels if you want PyTorch GPU. CPU works fine for small repos.
- hnswlib is disabled on Windows by default (requirements.txt enables it for nonWindows). Vector search falls back to a purePython path when needed.

Troubleshooting
- LLM health shows unreachable: confirm OPENAI_BASE_URL (LM Studio or other local endpoint) and that the server is running.
- No chunksattributes found: ensure REPO_PATH points to a folder with files, and rerun ingest with --full for a fresh build.

License
See repository terms or contact the author.


.requirements.txt
pyyaml
watchdog
hnswlib; sys_platform ! 'win32'
networkx
scikit-learn
numpy
pandas
rapidfuzz
python-dotenv
rank-bm25
python-louvain

sentencepiece
transformers4.43
accelerate0.33
sentence-transformers3.0
regex
unidecode

python-magic-bin; sys_platform 'win32'
PyMuPDF
chardet
pytesseract optional
flask

.requirements-win.txt
-r requirements.txt
CUDA PyTorch (CUDA 12.8) if needed:
pip install --index-url https:download.pytorch.orgwhlcu128
torch2.6.0cu128 torchvision0.21.0cu128 torchaudio2.6.0cu128

..env.example
OPENAI_BASE_URLhttp:localhost:12345v1
OPENAI_API_KEYlm-studio
OPENAI_MODELllama-3.1-8b-instruct
EMBED_MODELBAAIbge-small-en-v1.5
REPO_PATH..your-repo
KN_ROOT.knowledge
OCR_ENABLEDfalse

.pyproject.toml

name "kn-graphrag-starter"
version "0.1.0"
description "Local modular GraphRAG pipeline (Windows LM Studio)"
requires-python "3.10"

..env
OPENAI_BASE_URLhttp:10.5.0.2:12345v1
OPENAI_API_KEYlm-studio
OPENAI_MODELllama-3.1-8b-instruct
EMBED_MODELBAAIbge-small-en-v1.5
REPO_PATH.FAE
KN_ROOT.knowledge
OCR_ENABLEDfalse


.DESIGN.md
kn-graphRAG-starter (Windows LM Studio)

A local, modular GraphRAG-style pipeline for Windows 10 Pro, optimized for:

Non-intrusive sidecar indexing (your repo stays untouched)
.knowledge for all derived artifacts (movableportable)
Incremental updates via hashing (movesrenames dont force full rebuilds)
Local models first (LM Studio by default; OllamavLLM optional)
Fast retrieval with HNSW (hnswlib) by default (simple, Windows-friendly)
Graph using NetworkX (DB-optional), with community summaries
Attribute plugins as standalone scripts (stdinstdout JSONL)
Mono-file export under token budget for send to big LLM workflows

Minimal thirdparty complexity; you can later swap in FAISSQdrantNeo4jvLLM without redesign.

---

Final requirements design choices (v1)

Platform runtime

OS: Windows 10 Pro.
GPUs: RTX 5090 primary; RTX 3090 optional (no device pinning by default; auto device selection).
LLM runtime: LM Studio local server (OpenAIcompatible) as default; can swap to vLLMOllama later.
Offlinefirst: no outbound calls unless explicitly enabled via .env.

Data identity

Sidecar outputs in .knowledge; original repo stays untouched.
Document identity: SHA256 of normalized full text doc_id (short hash displayed).
Chunk identity: doc_id zeropadded index (docid-0001).
Movesrenames: update path metadata only (no reembed) if content hash unchanged.
Future (prostyle): add chunklevel content hashes (already implied by docid-XXXX) and optional nearduplicate detection (MinHashSimHash) for dedupe across repos.

Multirepo domains

Watcher can track multiple repo paths; domains default to toplevel folder under each repo.
Optional crossdomain bridge edges with threshold graph.cross_domain_bridge_threshold (default 0.75). Toggleadjust per your repos.
Git submodules are supported implicitly; each path contributes to the global graph.

Ingestion OCR

Chunking: default 4k chars with 10 overlap; type policies for codePDF.
OCR: optional (Tesseract pytesseract) controlled by OCR_ENABLED (default false). Only used for imagesPDFs without extractable text, not all docs.

Embeddings vector index

Default embedder: BAAIbge-small-en-v1.5 (Englishoptimized, fast). Easy to switch to bgem3 for multilingual.
Vector store: hnswlib (Windowsfriendly). Nightlyperiodic reingest recommended to rebuild and clear stale vectors (deletes can be added later).

Graph communities

Build an entity cooccurrence graph (LLMassisted extraction can be added later).
Community detection: Louvain (python-louvain) when available; fallback to connected components.
Community summaries: generated and stored in .knowledgeindexessummaries.

Retrieval export

Retrieval: hybrid (dense via HNSW BM25 via rank-bm25) with simple unionmerge; reranker optional later.
Graphaware context: preface with top community summaries; roadmap: include 12 hop neighbors in prompt assembly.
Export: .md by default, token budget heuristic (4 charstoken). Optional exclude PII in exports via export.exclude_pii.

Attributes enrichment

Plugins standalone Python scripts (stdin JSONL stdout) with outputs written to .knowledgeindexesattributesplugin.
Included: summary-20w, topic-tags, pii-scan (regexbased, fast).
Multiple passes allowed (e.g., pass: "cheap" vs pass: "full"), keep all outputs with versionmetadata; downstream tools choose by pass or confidence.
Job queue: SQLite (upgrade path: Redis). Background worker processes batches.

Operations

Watcher runs continuously (file addsedits trigger enrichment). Embeddings refresh via scheduled ingest (e.g., nightly).
Optional Task Scheduler job to run ingest_build_graph.py nightly.
Logs caches live under .knowledge and are safe to deleterebuild.

Security PII

Offline by default; .env toggles for any cloud usage.
pii-scan flags emailsphonescreditlike sequences; set export.exclude_pii: true to omit flagged chunks from exports. Future: LLMassisted PII patterns.

Deliverables available

A readytorun starter repo zip accompanies this document with Windows bootstrap (requirements-win.txt, setup.ps1), code scaffold, and default configs.

---

Folder Tree


kn-graphRAG-starter
README.md
pyproject.toml
requirements.txt
.env.example
.gitignore
bin
ingest_build_graph.py
watch_daemon.py
enrich_worker.py
query_rag.py
export_monofile.py
kn
__init__.py
config.py
hashing.py
file_index.py
chunking.py
embeddings.py
vector_hnsw.py
graph_nx.py
retrieval.py
exporter.py
jobs.py
utils
llm_client.py
io.py
plugins
attributes
summary_20w.py
topic_tags.py
chunkers (placeholder)
.knowledge (generated at runtime)
config
models.yml
pipeline.yml
indexes
chunks
embeddings
graph
summaries
attributes
manifests
queues
cache
exports
dumps
bundles
logs


---

README.md

md
kn-graphRAG-starter (Windows LM Studio)

Local GraphRAG-style pipeline with incremental sidecar indexing. Default stack:
- LM Studio (OpenAI-compatible endpoint) for LLM calls
- hnswlib for vector search (fast on Windows)
- NetworkX for graph communities (Louvain if available)
- watchdog for file change detection

Quickstart (Windows 10 Pro)

1) Install prerequisites
- Python 3.10
- LM Studio (start local server: Settings Local Server Enable; default http:localhost:1234v1)
- (Optional) Git for repo operations; optional Tesseract for OCR

2) Create venv install
powershell
python -m venv .venv
. ..venvScriptsActivate.ps1
pip install -r requirements.txt


3) Configure

Copy .env.example to .env and set your LM Studio endpointmodel.
Adjust .knowledgeconfigmodels.yml and pipeline.yml as needed.

4) Index a repo

powershell
python .biningest_build_graph.py --repo ..your-repo --full


5) Run watcher enrichment in background terminals

powershell
Terminal A: file watcher
python .binwatch_daemon.py --repo ..your-repo

Terminal B: enrichment plugins (periodic)
python .binenrich_worker.py --plugins summary-20w,topic-tags,pii-scan --watch


6) Query

powershell
python .binquery_rag.py --q "Design overview of Project X" --scope projectX --topk 12


7) Export to mono-file (for big LLMs)

powershell
python .binexport_monofile.py --q "Design overview of Project X" --budget 600000 --out .knowledgeexportsdumpsprojectX_dump.md


Incremental Non-intrusive

content_hash SHA256(normalized_text) for doc identity; docid-0001 style chunk IDs.
Movesrenames only update metadata; edits rechunk only the changed doc; graph summaries update incrementally.

Swaps

Vector: hnswlib FAISS or Qdrant later
Graph: NetworkX Neo4jMemgraph later
LLM runtime: LM Studio Ollama or vLLM later
Embedder: bge-small-en bge-m3 for multilingual

md
kn-graphRAG-starter (Windows LM Studio)

Local GraphRAG-style pipeline with incremental sidecar indexing. Default stack:
- LM Studio (OpenAI-compatible endpoint) for LLM calls
- hnswlib for vector search (fast CPU ANN on Windows)
- NetworkX for graph communities
- watchdog for file change detection

Quickstart (Windows 10 Pro)

1) Install prerequisites
- Python 3.10
- LM Studio (start local server: Settings Local Server Enable; default http:localhost:1234v1)
- (Optional) Git for repo operations

2) Create venv install
powershell
python -m venv .venv
. ..venvScriptsActivate.ps1
pip install -r requirements.txt


3) Configure

Copy .env.example to .env and set your LM Studio endpointmodel.
Adjust .knowledgeconfigmodels.yml and pipeline.yml as needed.

4) Index a repo

powershell
python .biningest_build_graph.py --repo ..your-repo --full


5) Run watcher enrichment in background terminals

powershell
Terminal A: file watcher
python .binwatch_daemon.py --repo ..your-repo

Terminal B: enrichment plugins (periodic)
python .binenrich_worker.py --plugins summary-20w,topic-tags --watch


6) Query

powershell
python .binquery_rag.py --q "Design overview of Project X" --scope projectX --topk 12


7) Export to mono-file (for big LLMs)

powershell
python .binexport_monofile.py --q "Design overview of Project X" --budget 600000 --out .knowledgeexportsdumpsprojectX_dump.md


Incremental Non-intrusive

We compute content_hash SHA256(normalized_text). Movesrenames update metadata only.
Only changed docs are re-chunkedembedded; graph summaries update incrementally.

Swaps

Vector: hnswlib FAISS or Qdrant later (no redesign)
Graph: NetworkX Neo4jMemgraph later
LLM runtime: LM Studio Ollama or vLLM (OpenAIcompatible) later
Embedder: bge-small-en-v1.5 bge-m3 for multilingual



---

requirements.txt
txt
core
pyyaml
watchdog
hnswlib
networkx
scikit-learn
numpy
pandas
rapidfuzz
python-dotenv
rank-bm25
python-louvain

text
sentencepiece
transformers4.43
accelerate0.33
sentence-transformers3.0
regex
unidecode

io mime pdf
python-magic-bin; sys_platform 'win32'
PyMuPDF
chardet

optional (comment out if not needed)
pytesseract enable OCR when installed OCR_ENABLEDtrue
spacy
spacy
txt
core
pyyaml
watchdog
hnswlib
networkx
scikit-learn
numpy
pandas
rapidfuzz
python-dotenv

text
sentencepiece
transformers
accelerate
sentence-transformers
fuzzywuzzy
regex

io
tqdm
rich
unidecode
python-magic-bin; sys_platform 'win32'
PyMuPDF

optional (comment out if not needed)
spacy
spacy


---

pyproject.toml

toml

name "kn-graphrag-starter"
version "0.1.0"
description "Local modular GraphRAG pipeline (Windows LM Studio)"
requires-python "3.10"
dependencies


line-length 100


---

.env.example

env
OPENAI_BASE_URLhttp:localhost:1234v1
OPENAI_API_KEYlm-studio
OPENAI_MODELllama-3.1-8b-instruct
EMBED_MODELBAAIbge-small-en-v1.5
REPO_PATH..your-repo
KN_ROOT.knowledge
OCR_ENABLEDfalse
env
OPENAI_BASE_URLhttp:localhost:1234v1
OPENAI_API_KEYlm-studio
OPENAI_MODELllama-3.1-8b-instruct
EMBED_MODELbge-m3
REPO_PATH..your-repo
KN_ROOT.knowledge


---

.knowledgeconfigmodels.yml

yaml
llm:
provider: openai
base_url: OPENAI_BASE_URL
api_key: OPENAI_API_KEY
model: OPENAI_MODEL
max_tokens: 2048
temperature: 0.2

embeddings:
kind: sentence-transformers
name: EMBED_MODEL
device: auto
normalize: true

stores:
vector:
kind: hnsw
path: .knowledgeindexesembeddingshnsw.index
graph:
kind: networkx
jobs:
kind: sqlite
path: .knowledgequeuesjobs.sqlite
yaml
llm:
provider: openai
base_url: OPENAI_BASE_URL
api_key: OPENAI_API_KEY
model: OPENAI_MODEL
max_tokens: 2048
temperature: 0.2

embeddings:
kind: sentence-transformers
name: BAAIbge-m3
device: auto
normalize: true

stores:
vector:
kind: hnsw
path: .knowledgeindexesembeddingshnsw.index
graph:
kind: networkx
jobs:
kind: sqlite
path: .knowledgequeuesjobs.sqlite


---

.knowledgeconfigpipeline.yml

yaml
watch:
paths:
ignore:

chunking:
policies:
default: max_chars: 4000, overlap: 400
code: max_chars: 2400, overlap: 200
pdf: max_chars: 3500, overlap: 200

ocr:
enabled: OCR_ENABLED
tesseract_cmd: ""

graph:
entity_extraction: selective selective all rule_based
relation_extraction: selective
community_detection: louvain
edge_conf_threshold: 0.55
cross_domain_bridge_threshold: 0.75

retrieval:
dense_k: 12
bm25_k: 8
graph_hops: 2
rerank: false

attributes:
plugins:
max_parallel: 4

export:
default_budget_tokens: 600000
strategy: hierarchy-first
exclude_pii: false
format: md
yaml
watch:
paths:
ignore:

chunking:
policies:
default: max_chars: 4000, overlap: 400
code: max_chars: 2400, overlap: 200
pdf: max_chars: 3500, overlap: 200

graph:
entity_extraction: selective selective all rule_based
relation_extraction: selective
community_detection: louvain
edge_conf_threshold: 0.55

retrieval:
dense_k: 12
bm25_k: 8
graph_hops: 2
rerank: true

attributes:
plugins:
max_parallel: 4

export:
default_budget_tokens: 600000
strategy: hierarchy-first


---

biningest_build_graph.py

python
import argparse, pathlib
from kn.config import load_configs
from kn.file_index import initial_scan
from kn.chunking import chunk_repo
from kn.embeddings import embed_chunks
from kn.graph_nx import build_or_update_graph, summarize_communities

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--repo", requiredTrue)
ap.add_argument("--full", action"store_true")
args ap.parse_args()

cfg load_configs()
repo pathlib.Path(args.repo).resolve()

print(" scanning repo...")
docs initial_scan(repo, cfg)
print(f" len(docs) docs found")

print(" building chunks...")
chunks chunk_repo(docs, cfg)

print(" embedding chunks...")
embed_chunks(chunks, cfg)

print(" updating graph...")
build_or_update_graph(chunks, cfg)

print(" community summaries...")
summarize_communities(cfg)

print(" ")


---

binwatch_daemon.py

python
import argparse, time, pathlib
from kn.config import load_configs
from kn.file_index import watch_changes

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--repo", requiredTrue)
args ap.parse_args()

cfg load_configs()
repo pathlib.Path(args.repo).resolve()
print(f" monitoring repo ... (CtrlC to quit)")
watch_changes(repo, cfg)


---

binenrich_worker.py

python
import argparse
from kn.jobs import run_enrichment_loop
from kn.config import load_configs

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--plugins", requiredTrue, help"comma-separated plugin names")
ap.add_argument("--watch", action"store_true")
args ap.parse_args()

cfg load_configs()
plugins
run_enrichment_loop(plugins, cfg, watchargs.watch)


---

binquery_rag.py

python
import argparse
from kn.retrieval import answer_query
from kn.config import load_configs

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--q", requiredTrue)
ap.add_argument("--scope", defaultNone)
ap.add_argument("--topk", typeint, default12)
args ap.parse_args()

cfg load_configs()
print(answer_query(args.q, cfg, scopeargs.scope, topkargs.topk))


---

binexport_monofile.py

python
import argparse, pathlib
from kn.exporter import export_monofile
from kn.config import load_configs

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--q", requiredTrue)
ap.add_argument("--budget", typeint, defaultNone)
ap.add_argument("--scope", defaultNone)
ap.add_argument("--out", requiredTrue)
args ap.parse_args()

cfg load_configs()
out pathlib.Path(args.out)
out.parent.mkdir(parentsTrue, exist_okTrue)
export_monofile(args.q, cfg, out, budgetargs.budget, scopeargs.scope)
print(f" wrote out")


---

knconfig.py

python
import os, yaml, pathlib
from dotenv import load_dotenv

ROOT pathlib.Path(os.getenv("KN_ROOT", ".knowledge"))
CONFIG_DIR ROOT "config"
MODELS_YML CONFIG_DIR "models.yml"
PIPELINE_YML CONFIG_DIR "pipeline.yml"

_defaults
"llm":
"base_url": os.getenv("OPENAI_BASE_URL", "http:localhost:1234v1"),
"api_key": os.getenv("OPENAI_API_KEY", "lm-studio"),
"model": os.getenv("OPENAI_MODEL", "llama-3.1-8b-instruct"),
"max_tokens": 2048,
"temperature": 0.2,
,
"embeddings": "name": os.getenv("EMBED_MODEL", "BAAIbge-m3"), "normalize": True, "device": "auto",
"stores":
"vector": "kind": "hnsw", "path": str(ROOT"indexes""embeddings""hnsw.index"),
"graph": "kind": "networkx",
"jobs": "kind": "sqlite", "path": str(ROOT"queues""jobs.sqlite"),
,


_loaded None

def load_configs():
global _loaded
if _loaded: return _loaded
load_dotenv(overrideTrue)
CONFIG_DIR.mkdir(parentsTrue, exist_okTrue)
load with fallbacks
cfg _defaults.copy()
if MODELS_YML.exists():
cfg _merge(cfg, yaml.safe_load(MODELS_YML.read_text()))
if PIPELINE_YML.exists():
cfg _merge(cfg, yaml.safe_load(PIPELINE_YML.read_text()))
_loaded cfg
return cfg


def _merge(a, b):
if not isinstance(b, dict): return a
out a.copy()
for k, v in b.items():
if isinstance(v, dict) and isinstance(out.get(k), dict):
out _merge(out , v)
else:
out v
return out


---

knhashing.py

python
import hashlib

def content_hash(text: str) - str:
return hashlib.sha256(text.encode("utf-8", errors"ignore")).hexdigest()


def short_hash(h: str, n8) - str:
return h


---

knfile_index.py

python
import pathlib, time, json
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from .utils.io import read_text_safely, ensure_dirs
from .hashing import content_hash, short_hash

In-memory manifest (persist minimal JSON if you like)
_manifest

IGNORE_EXT ".lock", ".tmp", ".log"


def initial_scan(repo_path: pathlib.Path, cfg):
docs
for p in repo_path.rglob(""):
if p.is_dir():
continue
if any(part ".git" for part in p.parts):
continue
if p.suffix.lower() in IGNORE_EXT:
continue
text read_text_safely(p)
if not text:
continue
ch content_hash(text)
doc_id short_hash(ch)
_manifest "doc_id": doc_id, "hash": ch, "mtime": p.stat().st_mtime
docs.append("path": str(p), "doc_id": doc_id, "hash": ch, "text": text)
return docs


class _Evt(FileSystemEventHandler):
def __init__(self, repo, cfg):
self.repo repo
self.cfg cfg

def on_any_event(self, event):
p pathlib.Path(event.src_path)
if p.is_dir() or any(part ".git" for part in p.parts):
return
if not p.exists():
return
text read_text_safely(p)
if text is None:
return
ch content_hash(text)
m _manifest.get(str(p))
if m and m ch:
moverename or metadata change update path mapping only
m p.stat().st_mtime
return
changednew file enqueue lightweight downstream updates (left as TODO hook)
_manifest "doc_id": short_hash(ch), "hash": ch, "mtime": p.stat().st_mtime
print(f" changed: p")
TODO: enqueue chunkembedgraph tasks via jobs module


def watch_changes(repo_path: pathlib.Path, cfg):
ensure_dirs(cfg)
obs Observer()
h _Evt(repo_path, cfg)
obs.schedule(h, str(repo_path), recursiveTrue)
obs.start()

try:
while True:
time.sleep(1)
except KeyboardInterrupt:
obs.stop()
obs.join()


---

knchunking.py

python
import pathlib, json, re
from .utils.io import ensure_dirs
from .hashing import short_hash

CHUNKS_DIR ".knowledgeindexeschunks"


def chunk_text(text: str, max_chars4000, overlap400):
out
i 0
while i len(text):
out.append(text )
i max_chars - overlap
return out


def detect_domain(path: pathlib.Path):
Domain top-level folder under repo root (best-effort placeholder)
parts path.parts
return parts if parts else "root"


def chunk_repo(docs, cfg):
ensure_dirs(cfg)
policy cfg.get("chunking", ).get("policies", ).get("default", "max_chars": 4000, "overlap": 400)
chunks
outdir pathlib.Path(CHUNKS_DIR)
outdir.mkdir(parentsTrue, exist_okTrue)

for d in docs:
path pathlib.Path(d )
ddomain detect_domain(path.relative_to(path.anchor)) if path.is_absolute() else detect_domain(path)
parts chunk_text(d , policy)
for idx, t in enumerate(parts):
chunk_id f"d -idx:04d"
rec
"doc_id": d ,
"chunk_id": chunk_id,
"text": t,
"meta":
"path": str(path), "domain": ddomain, "mimetype": "textplain"


chunks.append(rec)
(outdir f"chunk_id.json").write_text(json.dumps(rec, ensure_asciiFalse))
return chunks


---

knembeddings.py

python
from sentence_transformers import SentenceTransformer
import numpy as np
from .vector_hnsw import HNSWIndex
from .utils.io import ensure_dirs

_model None
_index None


def _get_model(name: str, device: str"auto"):
global _model
if _model is None:
_model SentenceTransformer(name, device(None if device"auto" else device))
return _model


def embed_chunks(chunks, cfg):
ensure_dirs(cfg)
emc cfg.get("embeddings", )
m _get_model(emc.get("name", "BAAIbge-m3"), emc.get("device", "auto"))
texts for c in chunks
vecs m.encode(texts, normalize_embeddingsemc.get("normalize", True), show_progress_barTrue)
idx HNSWIndex.open(cfg)
idx.add( for c in chunks, np.asarray(vecs, dtypenp.float32))
idx.save()


---

knvector_hnsw.py

python
import hnswlib, numpy as np, pathlib, json

class HNSWIndex:
def __init__(self, path: pathlib.Path, dim1024, space'cosine'):
self.path path
self.meta_path path.with_suffix('.meta.json')
self.dim dim
self.space space
self.index hnswlib.Index(spacespace, dimdim)
self.inited False
self.ids

classmethod
def open(cls, cfg):
path pathlib.Path(cfg )
meta path.with_suffix('.meta.json')
if path.exists() and meta.exists():
m json.loads(meta.read_text())
dim m.get("dim", 1024)
obj cls(path, dimdim)
obj.index.load_index(str(path))
obj.index.set_ef(128)
obj.inited True
obj.ids m.get("ids", )
return obj
default new
return cls(path)

def _ensure_init(self, total10000):
if not self.inited:
self.index.init_index(max_elementstotal, ef_construction200, M16)
self.index.set_ef(128)
self.inited True

def add(self, keys, vecs: np.ndarray):
self._ensure_init(max(10000, len(self.ids) len(keys) 1000))
map external string ids to integer labels
labels np.arange(len(self.ids), len(self.ids)len(keys))
self.index.add_items(vecs, labels)
self.ids.extend(list(keys))

def save(self):
self.path.parent.mkdir(parentsTrue, exist_okTrue)
self.index.save_index(str(self.path))
self.meta_path.write_text(json.dumps("dim": self.dim, "ids": self.ids))

def search(self, vecs: np.ndarray, k10):
labels, dists self.index.knn_query(vecs, kk)
map back to external ids
inv self.ids
mapped for i in row for row in labels
return mapped, dists


---

kngraph_nx.py

python
import pathlib, json, networkx as nx
from collections import defaultdict
from .utils.io import ensure_dirs

GRAPH_PATH pathlib.Path('.knowledgeindexesgraphgraph.jsonl')
COMM_PATH pathlib.Path('.knowledgeindexessummariescommunities.jsonl')

G nx.Graph()

Very lightweight placeholder extraction (extend with LLM in utilsllm_client)
def naive_entities(text: str):
collect simple tokens (CamelCase, path-like, tags) placeholder
import re
ents set(re.findall(r" b .ww", text))
return list(ents)


def build_or_update_graph(chunks, cfg):
ensure_dirs(cfg)
add nodes co-occurrence edges (same chunk weak link)
for c in chunks:
ents naive_entities(c ) TODO swap to LLM-assisted if configured
for e in ents:
if not G.has_node(e):
G.add_node(e, kind'entity')
for i in range(len(ents)):
for j in range(i1, len(ents)):
u, v ents , ents
w G 1 if G.has_edge(u,v) else 1
G.add_edge(u, v, weightw)
persist
GRAPH_PATH.parent.mkdir(parentsTrue, exist_okTrue)
with GRAPH_PATH.open('w', encoding'utf-8') as f:
for u, v, d in G.edges(dataTrue):
f.write(json.dumps("src": u, "dst": v, "weight": d.get("weight",1))"n")


def summarize_communities(cfg):
Simple connected components as communities (swap to Louvain later)
comms
for idx, comp in enumerate(nx.connected_components(G)):
nodes list(comp)
naive summary string (placeholder); you can call LLM here
summary ", ".join(nodes )
comms.append("id": f"Cidx", "size": len(nodes), "summary": summary)
COMM_PATH.parent.mkdir(parentsTrue, exist_okTrue)
with COMM_PATH.open('w', encoding'utf-8') as f:
for c in comms:
f.write(json.dumps(c)"n")


---

knretrieval.py

python
import json, pathlib
import numpy as np
from sentence_transformers import SentenceTransformer
from .vector_hnsw import HNSWIndex

Simple dense-only retrieval baseline community preface

COMM_PATH pathlib.Path('.knowledgeindexessummariescommunities.jsonl')

_embed_model None

def _get_model(name):
global _embed_model
if _embed_model is None:
_embed_model SentenceTransformer(name)
return _embed_model


def _iter_chunks():
cdir pathlib.Path('.knowledgeindexeschunks')
for p in cdir.glob('.json'):
yield json.loads(p.read_text(encoding'utf-8'))


def answer_query(q: str, cfg, scopeNone, topk12):
emc cfg.get('embeddings', )
m _get_model(emc.get('name','BAAIbge-m3'))
qv m.encode( , normalize_embeddingsemc.get('normalize', True))
idx HNSWIndex.open(cfg)
ids, dists idx.search(np.asarray(qv, dtypenp.float32), ktopk)
idset set(ids )

Gather matched chunks
hits
for rec in _iter_chunks():
if rec in idset:
hits.append(rec)
prepend community summaries (lightweight)
pref
if COMM_PATH.exists():
for line in COMM_PATH.read_text(encoding'utf-8').splitlines():
c json.loads(line)
pref.append(f" sizec c ")
text "nn".join(pref ) "nn" "nn".join( for h in hits)
return text


---

knexporter.py

python
import json, pathlib
from .retrieval import answer_query


def export_monofile(q: str, cfg, out: pathlib.Path, budget: intNoneNone, scopeNone):
body answer_query(q, cfg, scopescope, topk64)
nave token budget by char length ( 4 charstoken heuristic)
if budget:
body body
out.write_text(body, encoding'utf-8')


---

knjobs.py

python
import time, subprocess, json, pathlib

QUEUE pathlib.Path('.knowledgequeuespending.jsonl')
QUEUE.parent.mkdir(parentsTrue, exist_okTrue)


def enqueue(doc_id: str, plugin: str):
with QUEUE.open('a', encoding'utf-8') as f:
f.write(json.dumps("doc_id": doc_id, "plugin": plugin)"n")


def run_enrichment_loop(plugins, cfg, watchFalse):
very simple loop that invokes plugins with stdin jsonl of docs (placeholder)
while True:
if not QUEUE.exists() or QUEUE.stat().st_size 0:
if not watch: break
time.sleep(2); continue
lines QUEUE.read_text(encoding'utf-8').splitlines()
QUEUE.write_text('', encoding'utf-8')
jobs
group by plugin
byp
for j in jobs:
if j not in plugins: continue
byp.setdefault(j , ).append(j)
for plugin, items in byp.items():
_run_plugin(plugin, items)
if not watch:
break


def _run_plugin(plugin, items):
pypath pathlib.Path(f'pluginsattributesplugin.py')
if not pypath.exists():
print(f" plugin not found: plugin")
return
stream jsonl to plugin stdin
inp "n".join( )) for i in items)
proc subprocess.Popen( , stdinsubprocess.PIPE, stdoutsubprocess.PIPE, textTrue)
out, _ proc.communicate(inp)
print(f" plugin: len(items) docs processed")


def _load_doc(doc_id):
minimal: pick first chunk for that doc
cdir pathlib.Path('.knowledgeindexeschunks')
for p in sorted(cdir.glob(f'doc_id-.json')):
return json.loads(p.read_text(encoding'utf-8'))
return "doc_id": doc_id, "text": ""


---

knutilsio.py

python
import pathlib, chardet

ROOT pathlib.Path('.knowledge')


def ensure_dirs(cfg):
(ROOT"indexes""chunks").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""embeddings").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""graph").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""summaries").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""attributes").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""manifests").mkdir(parentsTrue, exist_okTrue)
(ROOT"queues").mkdir(parentsTrue, exist_okTrue)
(ROOT"exports""dumps").mkdir(parentsTrue, exist_okTrue)


def read_text_safely(path: pathlib.Path):
try:
data path.read_bytes()
enc chardet.detect(data).get('encoding') or 'utf-8'
return data.decode(enc, errors'ignore')
except Exception:
return None


---

knutilsllm_client.py

python
import os, requests

BASE os.getenv('OPENAI_BASE_URL', 'http:localhost:1234v1')
KEY os.getenv('OPENAI_API_KEY', 'lm-studio')
MODEL os.getenv('OPENAI_MODEL', 'llama-3.1-8b-instruct')


def chat(prompt: str, max_tokens512, temperature0.2):
url f"BASEchatcompletions"
headers 'Authorization': f"Bearer KEY", 'Content-Type':'applicationjson'
payload
'model': MODEL,
'messages': ,
'max_tokens': max_tokens,
'temperature': temperature,

r requests.post(url, headersheaders, jsonpayload, timeout120)
r.raise_for_status()
return r.json()


---

pluginsattributessummary_20w.py

python
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributessummary-20w')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

for line in sys.stdin:
job json.loads(line)
text job.get('text','')
if not text: continue
prompt f"Summarize in 20 words, terse, factual.nntext"
out chat(prompt, max_tokens100, temperature0.1)
(OUTDIR f"job .json").write_text(json.dumps(
"doc_id": job ,
"attribute": "summary-20w",
"value": out.strip(),
"confidence": 0.8
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps("status":"ok","doc_id":job ))


---

pluginsattributestopic_tags.py

python
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributestopic-tags')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

for line in sys.stdin:
job json.loads(line)
text job.get('text','')
if not text: continue
prompt (
"Extract 3-7 topical tags (comma-separated, lowercase, no spaces, use-hyphens).n"
"Prefer domain-relevant terms.nn"
f"TEXT:ntextn"
)
out chat(prompt, max_tokens64, temperature0.2)
(OUTDIR f"job .json").write_text(json.dumps(
"doc_id": job , "attribute": "topic-tags", "value": out.strip(), "confidence": 0.7
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps("status":"ok","doc_id":job ))


---

Notes on LM Studio

Enable Local Server in LM Studio (http:localhost:1234v1).
Download a compatible instruct model (e.g., Llama-3.1-8B Instruct quantized) and ensure its serving.
SentenceTransformers handles embeddings locally (GPU if PyTorch CUDA available). If you later want to use LM Studio for embeddings, swap embeddings.py to call embeddings.

Operations playbook

Continuous: watch_daemon.py monitors repos and enqueues enrichment.
Periodic: schedule ingest_build_graph.py --repo path --full nightly to refresh embeddings rebuild index (cleans stale vectors).
Ad hoc: run export_monofile.py to produce big .md dumps with a token budget for external chats.

Open questions (status)

OCR for all docs? No only for imagesPDFs lacking text and only when OCR_ENABLEDtrue.
Crosslinking across repos? Yes, optional via cross_domain_bridge_threshold (default 0.75).
Quality addons now? BM25 enabled; lightweight reranker deferred.
Queue backend? SQLite now (simple); Redis is an easy upgrade later.
Multipass attributes? Supported; keep both cheap and full results with pass metadata.
Hashing scope? Doclevel SHA256 perchunk IDs now; roadmap: perchunk hashes nearduplicate (MinHashSimHash) for large corpora.

Known limitations (MVP)

hnswlib deletions not wired; recommend nightly rebuild.
Graph entity extraction is naive; LLMassisted extraction and relation typing are planned upgrades.
Exporter does not yet include perchunk citationsline ranges; future enhancement.

Future upgrades

Vector DB: QdrantFAISSGPU; Graph DB: Neo4jMemgraph; Runtime: vLLM with tensorkv cache.
Graphaware retrieval: 12 hop neighbor packing, graphranked reassembly.
Reranker: tiny crossencoder for final ordering.
PII: enhanced detector (LLMassist rules per region).

---

Addendum: Implementation decisions and operations log (Windows LM Studio)

This addendum chronicles the practical design choices we made while hardening the Windowsfirst GraphRAG starter into a dashboarddriven pipeline.

Runtime and platform
- Default runtime: LM Studio (OpenAIcompatible) on Windows 10 Pro; network base defaults to http:127.0.0.1:12345v1 with perplugin overrides.
- vLLM considered for higher throughput; deferred due to Windows constraints (uvloop unsupported; HF weights required, not GGUF). Dashboard retains LM Studio defaults.
- UTF8 and Windows console: advise -X utf8, PYTHONIOENCODING, and chcp 65001 when needed.

Data model and stores
- Sidecar storage under .knowledge (portable): indexes, graph, summaries, attributes, exports, queues, cache.
- Vector index: hnswlib on nonWindows; NumPy bruteforce fallback on Windows, persisted as .npy .meta.json.
- Graph: NetworkX with Louvain when available; communities saved to JSONL.

Config and environment
- .env and YAML configs merged at runtime with VAR expansion. Early load_dotenv(overrideTrue) to ensure env is respected (fixes EMBED_MODEL issues).
- .knowledgeconfigmodels.yml gains plugins: block for perplugin LLM defaults:
- llm: base_url, api_key, model, timeout
- process_timeout: perplugin subprocess wallclock limit.

Ingestion, chunking, OCRPDF
- Multirepo ingest; domains derived from toplevel folders (e.g., G:LOKIpapers).
- Chunking policies tunable per type; input token pressure reduced by skeletonfirst flow.
- PDF parsing via PyMuPDF (fitz) for text extraction on Windows.

Attributes and enrichment
- Attribute plugins are standalone scripts (stdin JSONL stdout), outputs in .knowledgeindexesattributesplugin.
- Core set: summary-20w, topic-tags, pii-scan (regex), plus glossary, requirements, todo-items, faq-pairs.
- New performance plugins:
- doc-skeleton: outline 100w keyphrases snapshot per doc (tokencheap intermediate).
- multi-basic: single LLM call emits multiple attributes (summaryshortmediumoutline, keyphrases, riskscan).
- Existing plugins updated to prefer skeleton content when present, reducing input tokens.

Queue, worker, robustness
- Job queue migrated to SQLite with columns: status, retries, completed_at, last_error.
- Dequeue ordering by (retries asc, id asc) to avoid starvation; any pending plugin worker mode added.
- Error handling: timeoutserrors requeue (or mark failed for fatal issues like missing plugin).
- Global inflight concurrency cap across workers via SQLite counters; configurable in dashboard.

Dashboard (Flask static HTMLJS)
- apistatus with docs, coverage, queue, LLM health, workers.
- Planner: enqueue plugins (single or list), optional JSON payload for perrun overrides.
- Ingest: start ingest for arbitrary repo path (multidomain).
- Worker control: startstop multiple workers, list active workers, set batch and maxinflight.
- Queue panel: filterlist items, clear queue modes (nondonependingall, reset running pending).
- Coverage panel: summary rows with dropdowns per attribute perdoc previews and full JSON.
- Documents panel: table with perdoc dropdowns perattribute full JSON.
- Plugin defaults panel: LoadSave plugins: map to .knowledgeconfigmodels.yml (perplugin modeltimeout).
- LLM cache panel: showclear promptlevel response cache.

Retrieval and export
- Hybrid retrieval (dense BM25), community preface; monofile export with token budgeting and PII exclusion.
- Graph export (GEXFGraphML) for external tools; HTML report with D3 preview, communities section, and offline D3 fallback.

Performance choices
- Accept LM Studio statelessness; avoid repeated fulldoc prompts by:
- Oneshot macro plugin (multi-basic) per doc.
- Skeletonfirst flow and downstream plugins consuming the skeleton.
- Promptlevel response caching keyed by (modelmax_tokenstemperatureprompt).
- Concurrency throttling (maxinflight) to reduce timeouts and stabilize throughput.
- Reasonable perplugin HTTP timeouts and perprocess walltime limits (e.g., skeletonmultibasic larger than others).
- Keep individual attributes runnable; treat multi-basic as a macro for convenience, not a replacement.

Known limits and next steps
- LM Studio parallelism is limited by modelruntime; increase throughput by reducing input tokens (skeleton), batching tasks, or migrating to vLLM (Linux) with HF weights and paged attention.
- Optional: small reranker; richer entityrelationship extraction; better cachingincremental invalidation; dedicated model routing per attribute class via UI.


.DESIGN_additions.md
Could you compare CorrectiveRAG to our GraphRAG design and give a high-level analysis on how to integrate the innovations from that into ours?

---

Skip to content
Our new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.
logo


Search

GitHub
0.6.6
17.5k
3k
Copy page

Get started
Guides
Reference
Examples
Additional resources
Table of contents
Setup
Create Index
LLMs
Web Search Tool
Create Graph
Define Graph State
Compile Graph
Use the graph
Corrective RAG (CRAG)
Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection self-grading on retrieved documents.

In the paper here, a few steps are taken:

If at least one document exceeds the threshold for relevance, then it proceeds to generation
Before generation, it performs knowledge refinement
This partitions the document into "knowledge strips"
It grades each strip, and filters our irrelevant ones
If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource
It will use web search to supplement retrieval
We will implement some of these ideas from scratch using LangGraph:

Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired.
If any documents are irrelevant, let's opt to supplement retrieval with web search.
We'll use Tavily Search for web search.
Let's use query re-writing to optimize the query for web search.
Screenshot 2024-04-01 at 9.28.30 AM.png

Setup
First, let's download our required packages and set our API keys


! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python

import getpass
import os


def _set_env(key: str):
if key not in os.environ:
os.environ getpass.getpass(f"key:")


_set_env("OPENAI_API_KEY")
_set_env("TAVILY_API_KEY")
Set up LangSmith for LangGraph development

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph read more about how to get started here.

Create Index
Let's index 3 blog posts.

API Reference: RecursiveCharacterTextSplitter WebBaseLoader Chroma OpenAIEmbeddings


from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

urls
"https:lilianweng.github.ioposts2023-06-23-agent",
"https:lilianweng.github.ioposts2023-03-15-prompt-engineering",
"https:lilianweng.github.ioposts2023-10-25-adv-attack-llm",


docs
docs_list

text_splitter RecursiveCharacterTextSplitter.from_tiktoken_encoder(
chunk_size250, chunk_overlap0
)
doc_splits text_splitter.split_documents(docs_list)

Add to vectorDB
vectorstore Chroma.from_documents(
documentsdoc_splits,
collection_name"rag-chroma",
embeddingOpenAIEmbeddings(),
)
retriever vectorstore.as_retriever()
LLMs
Using Pydantic with LangChain

This notebook uses Pydantic v2 BaseModel, which requires langchain-core 0.3. Using langchain-core 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.

API Reference: ChatPromptTemplate ChatOpenAI


Retrieval Grader

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from pydantic import BaseModel, Field


Data model
class GradeDocuments(BaseModel):
"""Binary score for relevance check on retrieved documents."""

binary_score: str Field(
description"Documents are relevant to the question, 'yes' or 'no'"
)


LLM with function call
llm ChatOpenAI(model"gpt-4o-mini", temperature0)
structured_llm_grader llm.with_structured_output(GradeDocuments)

Prompt
system """You are a grader assessing relevance of a retrieved document to a user question. n
If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. n
Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""
grade_prompt ChatPromptTemplate.from_messages(

("system", system),
("human", "Retrieved document: nn document nn User question: question"),

)

retrieval_grader grade_prompt structured_llm_grader
question "agent memory"
docs retriever.invoke(question)
doc_txt docs .page_content
print(retrieval_grader.invoke("question": question, "document": doc_txt))

binary_score'yes'
API Reference: StrOutputParser


Generate

from langchain import hub
from langchain_core.output_parsers import StrOutputParser

Prompt
prompt hub.pull("rlmrag-prompt")

LLM
llm ChatOpenAI(model_name"gpt-3.5-turbo", temperature0)


Post-processing
def format_docs(docs):
return "nn".join(doc.page_content for doc in docs)


Chain
rag_chain prompt llm StrOutputParser()

Run
generation rag_chain.invoke("context": docs, "question": question)
print(generation)

The design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave conditioned on past experience. Memory stream is a long-term memory module that records a comprehensive list of agents' experience in natural language. Short-term memory is utilized for in-context learning, while long-term memory allows agents to retain and recall information over extended periods.

Question Re-writer

LLM
llm ChatOpenAI(model"gpt-3.5-turbo-0125", temperature0)

Prompt
system """You a question re-writer that converts an input question to a better version that is optimized n
for web search. Look at the input and try to reason about the underlying semantic intent meaning."""
re_write_prompt ChatPromptTemplate.from_messages(

("system", system),
(
"human",
"Here is the initial question: nn question n Formulate an improved question.",
),

)

question_rewriter re_write_prompt llm StrOutputParser()
question_rewriter.invoke("question": question)

'What is the role of memory in artificial intelligence agents?'
Web Search Tool
API Reference: TavilySearchResults


Search

from langchain_community.tools.tavily_search import TavilySearchResults

web_search_tool TavilySearchResults(k3)
Create Graph
Now let's create our graph that will use CRAG

Define Graph State

from typing import List

from typing_extensions import TypedDict


class GraphState(TypedDict):
"""
Represents the state of our graph.

Attributes:
question: question
generation: LLM generation
web_search: whether to add search
documents: list of documents
"""

question: str
generation: str
web_search: str
documents: List
API Reference: Document


from langchain.schema import Document


def retrieve(state):
"""
Retrieve documents

Args:
state (dict): The current graph state

Returns:
state (dict): New key added to state, documents, that contains retrieved documents
"""
print("---RETRIEVE---")
question state

Retrieval
documents retriever.invoke(question)
return "documents": documents, "question": question


def generate(state):
"""
Generate answer

Args:
state (dict): The current graph state

Returns:
state (dict): New key added to state, generation, that contains LLM generation
"""
print("---GENERATE---")
question state
documents state

RAG generation
generation rag_chain.invoke("context": documents, "question": question)
return "documents": documents, "question": question, "generation": generation


def grade_documents(state):
"""
Determines whether the retrieved documents are relevant to the question.

Args:
state (dict): The current graph state

Returns:
state (dict): Updates documents key with only filtered relevant documents
"""

print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
question state
documents state

Score each doc
filtered_docs
web_search "No"
for d in documents:
score retrieval_grader.invoke(
"question": question, "document": d.page_content
)
grade score.binary_score
if grade "yes":
print("---GRADE: DOCUMENT RELEVANT---")
filtered_docs.append(d)
else:
print("---GRADE: DOCUMENT NOT RELEVANT---")
web_search "Yes"
continue
return "documents": filtered_docs, "question": question, "web_search": web_search


def transform_query(state):
"""
Transform the query to produce a better question.

Args:
state (dict): The current graph state

Returns:
state (dict): Updates question key with a re-phrased question
"""

print("---TRANSFORM QUERY---")
question state
documents state

Re-write question
better_question question_rewriter.invoke("question": question)
return "documents": documents, "question": better_question


def web_search(state):
"""
Web search based on the re-phrased question.

Args:
state (dict): The current graph state

Returns:
state (dict): Updates documents key with appended web results
"""

print("---WEB SEARCH---")
question state
documents state

Web search
docs web_search_tool.invoke("query": question)
web_results "n".join( for d in docs)
web_results Document(page_contentweb_results)
documents.append(web_results)

return "documents": documents, "question": question


Edges


def decide_to_generate(state):
"""
Determines whether to generate an answer, or re-generate a question.

Args:
state (dict): The current graph state

Returns:
str: Binary decision for next node to call
"""

print("---ASSESS GRADED DOCUMENTS---")
state
web_search state
state

if web_search "Yes":
All documents have been filtered check_relevance
We will re-generate a new query
print(
"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---"
)
return "transform_query"
else:
We have relevant documents, so generate answer
print("---DECISION: GENERATE---")
return "generate"
Compile Graph
The just follows the flow we outlined in the figure above.

API Reference: END StateGraph START


from langgraph.graph import END, StateGraph, START

workflow StateGraph(GraphState)

Define the nodes
workflow.add_node("retrieve", retrieve) retrieve
workflow.add_node("grade_documents", grade_documents) grade documents
workflow.add_node("generate", generate) generate
workflow.add_node("transform_query", transform_query) transform_query
workflow.add_node("web_search_node", web_search) web search

Build graph
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
"grade_documents",
decide_to_generate,

"transform_query": "transform_query",
"generate": "generate",
,
)
workflow.add_edge("transform_query", "web_search_node")
workflow.add_edge("web_search_node", "generate")
workflow.add_edge("generate", END)

Compile
app workflow.compile()
Use the graph

from pprint import pprint

Run
inputs "question": "What are the types of agent memory?"
for output in app.stream(inputs):
for key, value in output.items():
Node
pprint(f"Node 'key':")
Optional: print full state at each node
pprint.pprint(value , indent2, width80, depthNone)
pprint("n---n")

Final generation
pprint(value )

---RETRIEVE---
"Node 'retrieve':"
'n---n'
---CHECK DOCUMENT RELEVANCE TO QUESTION---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
"Node 'grade_documents':"
'n---n'
---ASSESS GRADED DOCUMENTS---
---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---
---TRANSFORM QUERY---
"Node 'transform_query':"
'n---n'
---WEB SEARCH---
"Node 'web_search_node':"
'n---n'
---GENERATE---
"Node 'generate':"
'n---n'
"Node '__end__':"
'n---n'
('Agents possess short-term memory, which is utilized for in-context learning, '
'and long-term memory, allowing them to retain and recall vast amounts of '
'information over extended periods. Some experts also classify working memory '
'as a distinct type, although it can be considered a part of short-term '
'memory in many cases.')

from pprint import pprint

Run
inputs "question": "How does the AlphaCodium paper work?"
for output in app.stream(inputs):
for key, value in output.items():
Node
pprint(f"Node 'key':")
Optional: print full state at each node
pprint.pprint(value , indent2, width80, depthNone)
pprint("n---n")

Final generation
pprint(value )

---RETRIEVE---
"Node 'retrieve':"
'n---n'
---CHECK DOCUMENT RELEVANCE TO QUESTION---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
"Node 'grade_documents':"
'n---n'
---ASSESS GRADED DOCUMENTS---
---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---
---TRANSFORM QUERY---
"Node 'transform_query':"
'n---n'
---WEB SEARCH---
"Node 'web_search_node':"
'n---n'
---GENERATE---
"Node 'generate':"
'n---n'
"Node '__end__':"
'n---n'
('The AlphaCodium paper functions by proposing a code-oriented iterative flow '
'that involves repeatedly running and fixing generated code against '
'input-output tests. Its key mechanisms include generating additional data '
'like problem reflection and test reasoning to aid the iterative process, as '
'well as enriching the code generation process. AlphaCodium aims to improve '
'the performance of Large Language Models on code problems by following a '
'test-based, multi-stage approach.')
LangSmith Traces -
https:smith.langchain.compublicf6b1716c-e842-4282-9112-1026b93e246br

https:smith.langchain.compublic497c8ed9-d9e2-429e-8ada-e64de3ec26c9r

Back to top
Copyright 2025 LangChain, Inc Consent Preferences
Made with Material for MkDocs
ChatGPT said:
Thought for 1m 16s

Heres a crisp compare-and-integrate pass: what CorrectiveRAG (CRAG) adds on top of plain GraphRAG, and exactly how to fold those ideas into our WindowsLM-Studio starter.

What CRAG is (in one paragraph)

CRAG adds a lightweight retrieval evaluator that self-grades the initial context, then routes the query through one of three actions: Correct (keep and refine), Incorrect (discard and expand with external search), or Ambiguous (combine both). On Correct, it decomposes documents into fine-grained knowledge strips, grades strips, filters, and recomposes. On Incorrect, it rewrites the query and brings in web search results (then also stripsfilters). Ablations show each piece helps; a small T5-based evaluator outperformed ChatGPT for relevance grading, and the method boosts both vanilla RAG and Self-RAG across shortlong-form tasks with modest overhead.

2401.15884v3

GraphRAG vs. CRAG (high-level)
Dimension	Our GraphRAG starter	CRAG additions (what we should borrow)
Global structure	Entity co-occurrence graph Louvain communities; hybrid denseBM25; community summaries	Not graph-centric; per-query quality control gate on retrieved chunks
Quality control	None yet (just hybrid merge)	Self-grading evaluator route to Correct Incorrect Ambiguous
Granularity	Chunk-level	Strip-level refinement (sentencemini-span filtering) before generation
Fallbacks	Expand k; walk graph neighbors (planned)	Web search (with query rewrite), then same refinement
Overhead	Low	Small extra pass (grader strip scoring); paper shows modest cost for clear gains

2401.15884v3


Robustness	Sensitive to bad chunks	Designed to be robust to bad retrieval; ablations confirm each action helps

2401.15884v3

Integration plan for our starter (minimal moving parts)
1) Add a local Relevance Grader (no fine-tuning required to start)

Use a fast cross-encoder as a plug-in scorer (local, GPU): e.g., cross-encoderms-marco-MiniLM-L-6-v2 (very fast) or BAAIbge-reranker-base (higher quality). Both run via sentence-transformers we already ship.

Score each (question, chunk_text) scalar in (normalize if model outputs logits).

Config (new, in .knowledgeconfigmodels.yml):

crag:
enabled: true
grader_model: cross-encoderms-marco-MiniLM-L-6-v2
upper: 0.60 upper Correct
lower: 0.30 lower Incorrect
topk_grade: 24 how many retrieved chunks to grade


Why cross-encoder first? The CRAG paper trains a small T5 evaluator and finds it beats prompting LLMs for grading; a cross-encoder is the closest off-the-shelf analogue thats local and fast.

2401.15884v3

2) Implement action policy (Correct Incorrect Ambiguous)

After our hybrid retrieval returns N chunks, grade them:

Correct if any grade upper keep only graded-relevant chunks and go to refinement.

Incorrect if all lower skip internal chunks and go to fallback retrieval (next section).

Ambiguous otherwise keep best few internal chunks and run fallback; merge.

3) Add strip-level refinement (the knowledge strips)

For chunks that survive:

Split each chunk into sentences (or 150300 char spans).

Grade each strip with the same grader; keep top m per chunk (e.g., 23).

Reassemble in source order.

This mirrors CRAGs decomposefilterrecompose, which drove gains in ablations.

2401.15884v3

Config knobs (new in pipeline.yml):

crag_refine:
strip_chars: 300
max_strips_per_chunk: 3

4) Add fallback retrieval without breaking offline-first

Default (offline) fallback when Incorrect (or Ambiguous):

Expand scope: raise dense k and relax BM25; include graph-neighbor chunks (12 hops from any entity in the top few chunks, if any).

Query rewrite (local LLM via LM Studio) to enrich keywords; then re-retrieve internally.

Optional (opt-in) web search module (TavilyBingSerper) if you ever add API keys. Paper prefers authoritative pages (e.g., Wikipedia) then applies the same strip-refine; well mirror that when online is enabled.

2401.15884v3

Config:

crag_fallback:
use_web: false stays offline by default
expand_dense_k: 48
graph_hops: 2
rewrite_with_llm: true

5) Wire it into our retrieval path (surgical changes)

New module kncrag.py:

grade_chunks(question, chunks) - List

decide_action(scores, upper, lower) - "Correct""Incorrect""Ambiguous"

refine_strips(question, chunks) - refined_text

fallback_retrieve(question, cfg) - chunks (expand k, graph hops; optional web)

Modify knretrieval.py:answer_query(...):

Do current hybrid retrieval (denseBM25).

If crag.enabled, call grader action.

Build final context:

Correct refine_strips(internal_hits)

Incorrect refine_strips(fallback_hits)

Ambiguous refine_strips(internal_top) refine_strips(fallback_hits)

Prepend our community snippets (unchanged), then hand off to generator or exporter.

Cache: write per-(query_hash, chunk_id) grades to .knowledgeindexesgrades to avoid rescoring on repeated queries.

6) Export safety

Exporter can note the CRAG path taken:

e.g., CRAG: Correct (6 strips kept from 3 chunks); or CRAG: Ambiguous (internalfallback).

If optional web fallback is enabled, pass those strips through the existing PII plugin before inclusion (paper calls out reliability concerns for web content; well add our own guardrails).

2401.15884v3

Why this is worth it (evidence from CRAG)

Self-grading matters: Their small evaluator correctly judged retrieval quality far better than ChatGPT in their test (PopQA) and drove consistent gains.

2401.15884v3

All three actions help: Removing Correct Incorrect Ambiguous each hurt performance; the Ambiguous middle path mitigates evaluator uncertainty.

2401.15884v3

Strip-level refinement helps: Dropping refinement, rewrite, or selection each reduced accuracyso do the decompositionfilterrecompose step, not just bigger k.

2401.15884v3

Overhead is modest relative to gains; CRAG keeps compute close to RAG in their measurements.

2401.15884v3

Defaults Id set for you now

Grader model: cross-encoderms-marco-MiniLM-L-6-v2 (fastest good baseline; upgrade to BAAIbge-reranker-base if you want more headroom).

Thresholds: upper0.60, lower0.30.

topk to grade: 24 (grade fewer than you retrieve; still plenty to decide and filter).

Strip size 300 chars, keep 23 per chunk.

Fallback: expand dense k to 48, include 12 graph hops, rewrite query via local LLM (LM Studio), keep use_webfalse.

Where CRAG GraphRAG complement each other

CRAG gives per-query robustness, GraphRAG gives global structure. Combining them: when Incorrect, instead of jumping straight to the web, we first walk our graph (neighbors communities) and only go online if youve opted in. That keeps us fast, offline, and aligned with your multi-repo setupwhile inheriting CRAGs resilience when initial hits are noisy.

If you want, I can produce the small code patch list (new kncrag.py, few edits in knretrieval.py, and the two config blocks) next.

Sources
You said:
2508.11386v1.pdf
PDF
Could you also incorporate the lessons learned by this paper, if they're replicable to our environment (or explain why not, if not)
ChatGPT said:
Thought for 1m 9s

Absolutelyheres how wed fold the key, replicable ideas from Retrieval-Augmented Reasoning with Lean Language Models into our GraphRAG starter, plus what isnt a great fit (and why). Ill keep this focused on changes you can actually run on your Windows LM Studio setup.

What to adopt (and where it lands in our repo)
1) Summary-first indexing (big win, easy to add)

Lesson: They index summarized versions of documents (85 shorter) and see higher pk than indexing full pages; then recover full content only when needed. This shrinks context, speeds everything, and improves first-hit relevance.

2508.11386v1

How we add it

New attribute plugin: pluginsattributesdoc-summary.py that creates a compact per-doc summary (static, offline) into .knowledgeindexessummariesdoc.

New summary index: build a second HNSW index from summaries alongside the full-chunk index.

Retrieval path: query summary-index select top docs rehydrate by pulling either (a) the docs best chunks from the full index or (b) the entire doc if you prefer (see next item).

Config (new):

summarize_index:
enabled: true
target_ratio: 0.15 85 reduction
model: OPENAI_MODEL via LM Studio locally
use_for_retrieval: true
rehydrate: full_doc or: best_chunks


Why it fits your constraints: offline, uses your local LLM, keeps original files untouched.

2) Promote to full document (source rehydration)

Lesson: If any chunk from a doc hits, they feed the whole doc to the reasoner so local context isnt lost. In our pipeline thats a controlled rehydration step.

2508.11386v1

How we add it

After we pick top-k summaries, rehydrate either to (a) entire sources or (b) N nearest chunks around those sources.

Export path also respects this, so mono-file includes full supporting text (optionally trimmed).

3) Retrieval as a tool (dont always retrieve)

Lesson: Treat retrieval as callable: the model decides when to retrieve vs. answer directly; also rewrite the query when needed. This matters in chat flows and follow-ups.

2508.11386v1

How we add it (without fancy tool-calling)

Keep it offline and model-agnostic: add a small should_retrieve gate (cross-encoder or logistic classifier over the last user turn short history). If score , skip retrieval.

If documents look weak (see 5 CRAG below), rewrite the query via local LLM (brief keyword-boosted rewrite) then re-retrieve.

4) CRAG-style self-grading strip filtering (pairs perfectly)

Lesson: Their results underscore that quality control before generation and fine-grain filtering lift accuracy with small overhead. Combine with our CRAG plan:

Grade retrieved chunks with a local cross-encoder (e.g., ms-marco-MiniLM-L-6-v2) route to Correct Incorrect Ambiguous.

Strip-level refinement: split surviving chunks into 150300-char strips, grade again, keep only the best few per chunk, and recompose the context. (This mirrors the papers knowledge refinement benefit.)

2508.11386v1

Config well ship

crag:
enabled: true
grader_model: cross-encoderms-marco-MiniLM-L-6-v2
upper: 0.60
lower: 0.30
topk_grade: 24
crag_refine:
strip_chars: 300
max_strips_per_chunk: 3
crag_fallback:
use_web: false stays offline
expand_dense_k: 48
graph_hops: 2
rewrite_with_llm: true

5) Two-tier retrieval budget (pick k smartly)

Lesson: They explicitly choose k by measuring pk trade-offs; retrieval of summaries let them run k5 with manageable context (and 0.76 upper-bound hit-rate in their domain).

2508.11386v1

How we add it

Small CLI to log pk on your corpora (using source-of-truth tags or quick heuristics).

Config preset: k_small5 for fast passes, auto-bump to k_large30..50 inside fallback only when needed.

6) Test-time scaling for lean models (no fine-tuning required)

Lesson: They show big gains by combining retrieval with lean reasoning and test-time tricks (self-consistencymajority vote, budget forcing). You want offline cheap we use the tricks, skip heavy SFT.

2508.11386v1

Add self-consistency: do n35 short generations and vote (string-match or cross-encoder evidence score over answers). Gate behind --fast vs --best.

Add budget forcing lite: set high max_tokens but cap visible reasoning; instruct the model to keep internal notes concise (well only surface the answer citations).

7) Conversation assembly that wont bloat history

Lesson: Put retrieved context in the system message for the current turn vs. appending into history (keeps history small and stable).

2508.11386v1

How we add it

Our chat runner (optional) will build a new system message per turn with the refined strips short community preface. History retains only userassistant turns, not past contexts.

8) Evaluation knobs youll appreciate

Lesson: They instrument retrieval pk and task accuracy to pick k, model size, and summarization ratio; they also tried reranking and query rewriting. Well mirror the easy bits.

2508.11386v1

New CLI: bineval_retrieval.py (pk over a labeled subset).

Optional query rewriter (local LLM) gated by the CRAG decision.

Reranker: leave as optional (they saw mixed results). We already have a cross-encoder on hand; flip on if you want.

What not to copy (and our substitutes)

Frontier-model synthetic data reasoning-trace SFT
They generate synthetic queries with GPT-4o and distill DeepSeek-R1 reasoning into Qwen2.5-32B via long-context SFT (block size 32k), then deploy that tuned model. Great for research; heavy for a local, offline workflow. We wont do this by default. Instead we:

Use summary-first CRAG strip filtering self-consistency to emulate much of the gain at inference time.

If you ever want SFT later, we can prepare datasets locally and fine-tune a smaller model (e.g., 7B) as an add-on.

2508.11386v1

Always retrieving full docs
Works for their NHS domain because pages are bounded and curated. On arbitrary repos, full-doc payloads can explode. We support rehydratebest_chunks (default) and let you toggle full_doc when files are small.

2508.11386v1

Query-aware summarization at retrieval time
They note its higher quality but slower (an extra LLM step per query). We keep it optional; static summaries are the fast default.

2508.11386v1

Concrete patch list for our starter

New: pluginsattributesdoc-summary.py (offline summarizer; writes .knowledgeindexessummariesdocdoc_id.md).

New: knsummary_index.py (buildread HNSW over summaries).

New: kncrag.py (grader, action policy, strip refinement, fallback).

Update: knretrieval.py

Step 1: search summary index (if enabled).

Step 2: CRAG grade Correct Incorrect Ambiguous.

Step 3: refine strips; on IncorrectAmbiguous, expand k and walk graph neighbors before any (optional) web fallback.

Step 4: assemble system prompt with refined strips (not appended to history).

New: bineval_retrieval.py (reports pk across a labeled CSVJSONL).

Config additions: summarize_index, crag, crag_refine, crag_fallback blocks shown above.

Defaults Id ship (tuned for your box)

Summary ratio 0.15, k_small8, fallback k_large48, graph_hops2.

Grader ms-marco-MiniLM-L-6-v2, thresholds upper0.60, lower0.30.

Strip size 300, keep 3 per chunk.

Self-consistency: off by default; --best enables n3 votes.

Why these changes are justified

Summary-first rehydrate improves retrieval and slashes context without losing fidelity; they measured better pk with summaries than with full pages.

2508.11386v1

CRAG strip filtering boosts robustness to bad hits; their ablations and narrative emphasize the value of pre-generation filteringcorrection and query rewrite.

2508.11386v1

Agentic retrieval (only when useful) and system-slot context keep chats snappy and memory-efficient.

2508.11386v1

Test-time scaling (self-consistency, budget control) is the practical path for lean, local models when SFT isnt desired.

2508.11386v1

If you want, I can generate the four new files (summary plugin, summary index, crag module, eval CLI) tailored to the scaffold you already downloaded and wire the config blocks exactly where they go.

Sources
You said:
so, RAG is nice to have but I want to be able to get as close to being able to ingest a list of every saved post on reddit and feed them into a single AI session in the most information-rich form we can despite compression. Alternatively if that means segmenting them into multiple sessions and reducing them down with summariessorting that might be necessary too. RAG by itself seems to be criticized as losing the nuance and deeper understanding of the data when digested, so im not sure if that's sufficient.

What is the breadth of other methods available here? what do we have as tools already and what might we still want to be able to pushthis system as close to a general-purpose one we can simply toss piles of data into ajd trust that it will process it about as well as any known method out there - dynamically picking methods for different data if it needs to, even.
ChatGPT said:
Thought for 40s

Youre right: plain RAG is great for lookup, but it tends to sand off nuance when youre trying to ingest a whole personal corpus (like every Reddit save) and think across it. Heres a compact playbook of methods beyond vanilla RAG, what we already have in your starter, and what Id add so you can toss piles of data in and get rich, trustworthy synthesis.

What we already have (good base)

Sidecar ingestion (non-intrusive): incremental hashing, chunking, embeddings (HNSW), BM25, NetworkX graph Louvain, community summaries, export to big .md.

Enrichment plugins: terse summary, topic tags, PII scan.

CRAG-style robustness (design ready): relevance grader strip filtering query rewrite fallback scope expansion (stays offline).

Summary-first retrieval (design ready): build a summary index to retrieve against; then rehydrate to full content.

What else is on the table (breadth of methods)

Think of these as composable capabilities you can mix per data question type.

1) Hierarchical map-reduce summarization (retains nuance)

Why: Single-pass summaries lose rare but important details.

How: Summarize per post per cluster global synthesis, keeping quotes citations (evidence packs) at each level. Use chain-of-density style prompts to add detail and dont drop facts instructions.

Where: New binhsum_build.py and an attributesdoc-summary.py (long-form) that writes layered summaries; exporter can assemble top-down digests.

2) Knowledge strips selective recomposition (CRAG refinement)

Why: Passages are long; keep only the salient spans tied to the questiontheme.

How: Sentence150300-char strips score via a local cross-encoder keep the top N per doc recombine in source order.

Where: kncrag.py with refine_strips(), already planned.

3) Topic discovery clustering (multi-view navigation)

Why: Youll have many domains with sparse links; you need lenses.

How: Embed posts, run KMeansAgglomerative (Windows-friendly), detect keyphrases per cluster, and surface diversity (not just upvoted).

Where: New kntopics.py outputs .knowledgeindexestopics an index.md nav. (We can add HDBSCAN later if you want.)

4) Argumentclaim mining (preserve positions)

Why: Reddit nuance claims counterclaims. Pure summaries blur this.

How: Lightweight claimstance extraction with a small local model or prompt; store (claim, support, oppose, quotes, links) per post.

Where: Plugin attributesclaims.py .knowledgeindexesattributesclaims.

5) Timeline event extraction (when posts talk about evolving stuff)

Why: Helps the model reason causally and temporally.

How: Extract timestamps (post time in-text dates), build event graph; cluster by entitytopic; generate timeline cards.

Where: pluginsattributesevents.py kntimeline.py.

6) Contextual compression dedup (keep signal, drop repeats)

Why: Saved posts often repeat the same ideas.

How: Near-duplicate detection (MinHashSimHash), MMRnovelty sampling when assembling contexts, quota per cluster to keep variety.

Where: kndedup.py novelty sampler in exporter.

7) Long-context multi-session packing (monofile, but smarter)

Why: Sometimes you do want to just feed it all in.

How: Pack by topic buckets, sorted by importancenovelty, then spill into sequenced sessions with a state summary passed between sessions.

Where: Extend exporter with --pack-mode topic,chronological,claims and --sessions N.

8) Self-consistency self-verification (better final answers)

Why: For global synthesis, single-shot reasoning can miss edges.

How: Generate n3 independent answers (fast local model), votererank via cross-encoder vs. evidence strips; run a verification pass that asks: Which claims are weak? Whats missing?

Where: knverify.py flag in query_rag.py --best.

9) Schema induction to a lightweight KB (when structure helps)

Why: Some subdomains want structured queries later.

How: Opportunistically extract entitiesrelations (subreddit, author, topic, claim, evidence_url, time) into ParquetSQLiteno heavy graph DB needed yet.

Where: knstruct_extract.py datawarehouse.sqlite.

10) Dynamic policy engine (pick the right method per query)

Why: Different questions need different pipelines.

How: A tiny orchestrator decides: lookup (RAG) vs synthesize (HSumstrips) vs explore (topicstimeline) vs argument (claims). Uses cheap signals: query intent, hit quality, corpus size.

Where: knorchestrator.py rules in pipeline.yml:

orchestrator:
rules:
- if: intent in or scope"global"
then:
- if: intent in and good_hits
then:
- if: intent"trend" or "timeline"
then:

Reddit-specific ingest to preserve nuance

Loader: biningest_reddit_saved.py that reads your exportAPI, normalizes:

post_id, subreddit, author, title, url, flair, score, created_utc, body_md, comments

Thread shaping: Build post highlighted comments docs (comments scored by upvotes, diversity, novelty, and author OP replies).

Quote harvesting: Keep top quotes with permalinks; tag with stance if available.

Attribution: Every summaryclaim keeps a back-link to source.

How these pieces interplay (a typical end-to-end)

Ingest normalize Reddit saves dedup chunk embed.

Run doc summaries (long-form) topic clustering claims events.

For global synthesis: Orchestrator picks HSum CRAG strips self-consistency verify; exporter builds an evidence-rich monofile (or multi-session pack).

For specific questions: Orchestrator tries hybrid RAG CRAG grading; if weak, expands to graph neighbors; if still weak (and you allow), query rewrite optional web (off by default).

For exploration: Topic index timeline quote booklets.

What this buys you (vs RAG-only)

Nuance preserved via quotesclaimstimelines and strip-level selection.

Coverage via hierarchical map-reduce topic diversity controls.

Robustness via CRAG grading, fallback scope, and self-verification.

Scalability via dedupnovelty sampling and multi-session packing.

Flexibility via the orchestrator (dynamic method selection).

Minimal add-ons to your starter to get there

New modules: kntopics.py, kndedup.py, knorchestrator.py, knverify.py, knsummary_index.py, kncrag.py (we already scoped CRAG), kntimeline.py, knstruct_extract.py.

New plugins: attributesdoc-summary.py (long form), attributesclaims.py, attributesevents.py.

New CLI: biningest_reddit_saved.py, binhsum_build.py, bineval_retrieval.py.

Config blocks: orchestrator, summarize_index, crag, pack_mode, sessions, dedup.

If you want, I can generate stubs for those modules and wire the orchestrator rules you prefer (e.g., prioritize HSum for whycomparenuance, RAG for factwhere, timeline for whenhow over time), plus a Reddit ingest script that works with your exported JSON.

You said:
Similarly we want this same system to be able to just easily digest: github repos (readmes at least), twitter dumps, entire subreddits, scientific papers, various .txt writings, signal chats, discord dumps, lists of comfyui node repos, kiwix data dumps, academictorrents dumps, arxiv paper dumps, huggingface model pages, news articles, hackernews, bluesky, previous chatgpt and gemini conversations, a messy local disk full of files, etc etc - a whole bunch of different sources with varying degrees of truth and trustworthiness. Ideally we want to be able to have it just churn on these and find the most useful things it can, for various priorities, and index them generally in preparation to start. How does that menu change things
ChatGPT said:
Thought for 19s

Love that menu. The good news: we dont need a different system we need a source-adapter layer, trustutility scoring, and a slightly smarter orchestrator. Your GraphRAGCRAG starter becomes a general throw piles of data at it engine by adding adapters for each dump format, normalizing to a common schema, running enrichment, then letting the orchestrator pick the right workflow per source and query.

Heres the high-level delta.

1) Architecture changes (small but powerful)

Adapters Canonical docs. Add sources modules that turn each dump (Reddit, GitHub, XTwitter, PDFs, Kiwix, etc.) into a Doc with the same fields:

doc_id, source_type, source_id, title, text, html_md, url, author, created_at,
metricsscore,stars,retweets,points..., tags , provenance , media ,
trust_signalsdomain_rank?, venue?, arxiv_ver?, ocr_conf?, repo_age...


Trust utility scoring. New kntrust.py computes two numbers per doc:

TrustScore (01): source-specific reliability prior.

UtilityScore (01): novelty recency engagement length quality.

Priority queue. knprioritizer.py orders work: cheap enrichments embeddings clustering heavy long-form summaries, weighted by UtilityScore and your priority policy (e.g., papers news social).

Method router (orchestrator). Extend rules so source_type trust band user intent select the pipeline:

Fact lookup hybrid RAG CRAG grade maybe expand.

Deep synthesisnuance hierarchical summarization strip-refine self-consistency.

Trendtimeline event extraction timeline pack.

Code repos code-aware chunking readmeissue synthesis.

2) Adapters youll want (all offline-friendly)

(Each is a small parser that yields Canonical Doc objects and writes to .knowledgeindexeschunks via the existing chunker.)

Reddit saved JSON (we already outlined): posts top comments, quote harvesting, subredditauthortime, score.

GitHub repos (local clones or zips): README, docs, LICENSE, .md, top .pyjstsgo with code-aware chunking; heuristics from .git (commit cadence, stars if present in dump).

TwitterX exports (zipJSON): tweets, retweets, likes; author verification flag if present; threads reconstructed.

DiscordSignal exports: threads conversation turns; per-speaker attribution; timestamps.

Subreddit dumps (PushshiftJSONL): similar to Reddit saved.

arXivacademic PDFs: PyMuPDF text; metadata from the dump (title, authors, categories, vN); optional OCR if no text layer.

Hugging Face model pages: model card (README.md), tags, downloads, last update; treat like docs.

Kiwix (ZIM): via libzim python binding; title, URL, text; provenance domain in trust signals.

AcademicTorrents: datasetpaper metadata from the dump; link back for provenance.

Hacker News dumps: item text, points, comments.

News articles (if you have local HTMLMarkdown): boilerplate removal text, domain from URL.

Bluesky (ATProto export): poststhreads like Twitter.

ChatGPTGemini transcripts: role-tagged turns; mark as internal notes (low trust externally).

ComfyUI node repos: from list fetch local READMEs (you supply zipclone), tag authorsnames.

We wont fetch anything from the web; we only parse your dumps. That keeps it offline and ToS-friendly.

3) Trust bands utility (how we believe and prioritize)

TrustScore by source (defaults; you can tune):

Peer-reviewed venue well-known docs: 0.850.95

arXivself-published research: 0.650.8 (bonus if many versions citations present in dump)

Major news: 0.550.75 (domain allowlist optional)

GitHub READMEscode: 0.50.7 (up with starsageconsistency)

HNRedditTwitterDiscord: 0.250.55

Personal chatsnotes: 0.20.4

UtilityScore (per doc):
Utility w1Novelty w2Recency w3Engagement w4LengthQuality w5TopicMatch

Novelty via near-dup detection (MinHashSimHash) and MMR at assembly time.

Engagement from upvotesretweetsstars (normalized).

TopicMatch cosine to your interest profile (optional).

We store bands: High (0.7), Medium (0.40.7), Low (0.4), which the orchestrator can use (e.g., avoid low-trust in final unless corroborated).

4) Enrichments that preserve nuance (beyond RAG)

Hierarchical summaries (per-doc per-cluster global) with quotes citations kept.

CRAG knowledge strips: sentenceshort-span scoring via a local cross-encoder; keep top strips.

Topic clustering (KMeans or Agglomerative) diversity sampler to avoid echo chambers.

Claimstance mining (argument structure) for socialnewspapers: (claim, support_evidence , oppose_evidence ).

Event extraction timelines for evolving topics.

Dedupnear-dup novelty sampling.

PII license guardrails (already in place for PII; add a lightweight licenseusage flag for coderepos).

5) How the orchestrator picks methods (examples)

Rules in pipeline.yml:

orchestrator:
rules:
- if: intent in or scope"global"
then:
- if: intent in and good_hits
then:
- if: source_mix includes and question mentions "timeline" or "trend"
then:
- if: source_type in
then:


Where:

hsum hierarchical summarization

crag_refine strip-level recomposition

verify self-consistency evidence check

eventstimeline temporal lens

code_chunking smaller chunks symbol-aware tagging

6) Packing single session mega-contexts (when you want it)

Pack by topic buckets, sorted by Utility Trust Novelty, include evidence quotes and source links.

If it wont fit: multi-session chain we export N sequential files; each session ends with a state summary the next session consumes. (Export flags: --pack-mode topic,chronological,claims --sessions N.)

7) What we add to your repo (small, modular)

New modules:

sources adapters per dump: reddit.py, github.py, twitter.py, discord.py, arxiv.py, zim.py, hn.py, news.py, chats.py, hf.py.

kntrust.py TrustScore domainvenue heuristics; pluggable bands.

knprioritizer.py schedules work by UtilityScore.

kntopics.py, kndedup.py, kntimeline.py, knsummary_index.py, kncrag.py (as planned), knorchestrator.py, knverify.py.
New plugins:

attributesdoc-summary.py (long-form), attributesclaims.py, attributesevents.py.
New CLIs:

biningest_source.py for each adapter,

binhsum_build.py, bineval_retrieval.py.

Config additions (minimal):

.knowledgeconfigpipeline.yml
sources:
reddit: enabled: true
github: enabled: true
twitter: enabled: true
... others toggled onoff

trust:
domain_allowlist: optional for news
arxiv_bonus_per_version: 0.02
github: star_breakpoints:

prioritizer:
weights: novelty: 0.35, recency: 0.2, engagement: 0.15, length_quality: 0.1, topic: 0.2
max_parallel: 4

packing:
mode: topic
sessions: 3
evidence_quotes: true

8) What changes because of your bigger menu?

Nothing fundamental breaks. We add adapters and trustutility so the same pipelines (HSum, CRAG, timeline, RAG) operate across heterogeneous sources.

Quality control improves. Low-trust sources dont dominate final synthesis unless corroborated. High-trust sources get preferential weight.

Throughput stays high. Cheap passes (doc summaries, dedup, embeddings) run first; heavier steps are batched and scheduled.

You keep optionality. Everything remains offline-first, file-system driven, and modular.

If you want, I can generate stubs for the top 4 adapters youll hit first (Reddit saved, GitHub repo, Twitter export, arXiv PDFs) and wire TrustUtility orchestrator rules so you can drop dumps into a folder and let it churn.

You said:
How about capturing and preserving the context of what matters to me (one human user among many potentially) for the various data sources? e.g. my personal notes, my reddit comments and saved posts, tweets, twitter favorites, etc etc - those are different than the more general data of what's fed in. though even the general data has somewhat more meaning than anything random due to my selection bias too. how would this system account for all that, while also being general enough to be able to do the same for more users in the future andor to make its own objective selections minimizing influence
ChatGPT said:
Thought for 17s

Totallythis is where you add a personalization layer on top of the global GraphRAGCRAG engine, but keep it switchable so you can run in Personal, Balanced, or Objective modes. Heres a tight plan that captures your signals, preserves your intent, and stays multi-user friendly.

Goals

Preserve what matters to me (saves, likes, comments, notes, chats) as first-class signal.

Stay general so multiple users can coexist later (or the system can run objective).

Remain offline-first and modular; everything is explainable auditable.

Layers (what we add)

User Profile Layer (per user):

A canonical UserEvent log: I savedlikedrepliedhighlightedread across Reddit, X, DiscordSignal, GH stars, HN upvotes, notes, etc.

Derived interest embeddings (topic vectors) from your interactions (centroids with time decay).

A light preference schema: topics, sources, styles (e.g., longform, tutorials), recency bias, and avoid lists.

Personal Graph Overlay:

Over the global graph, build an ego-graph: nodes and edges youve touched, plus nearest neighbors.

Keep weights for touched by user, often co-saved, same authorsubreddit, etc.

Scoring Modes:

Rank items by a transparent formula, then pick a mode:

Score Trust Utility PersonalRelevance DiversityBoost DupPenalty


Personal: high (e.g., 0.45), moderate

Balanced: moderate (0.2) with stronger diversity

Objective: 0; (trust) and diversity up; also counter-bias sampling

Always expose why an item was chosen: top contributing terms, source type, trust band, personal signals.

What to capture (signals)

Normalize everything into a single UserEvent schema (JSONL):

user_id, timestamp, source_type, source_id, event_type, weight,
meta subredditrepohandle, url, author, title, tags , device?...


event_type: saved, upvoted, liked, starred, replied, quoted, highlighted, read_dwell, note_edit

weight: default 1.0; upweight comments you wrote; time-decay older events

We store this per user at:

.knowledgeusersuser_ideventsevents.jsonl

Modeling (simple, effective)

Interest vectors: for each user, maintain one or more centroids (topic clusters) as EMA over embeddings of interacted docsquotes. Keep:

Global interest vector

Per-domain vectors (reddit, code, papers)

Recency-biased vector for current focus

PersonalRelevance(doc) max cosine(doc_emb, interest_vectors) with decay for stale topics.

DiversityBoost MMRnovelty w.r.t. what youve recently readseen.

Counter-bias meter: KL divergence between your interest distribution and the corpus distribution; we can surface areas you under-sample.

How retrieval synthesis use it

Two-stage retrieval:

Global hybrid (denseBM25), then re-rank by PersonalRelevance unless modeObjective.

If CRAG says IncorrectAmbiguous, expand scope via graph neighbors prioritized by ego-graph proximity (PersonalBalanced) or by trust (Objective).

Context budget allocation:

Personal: allocate, say, 60 budget to high PersonalRelevance across clusters; reserve 40 for high-Trustnovel content.

Balanced: 4060 split.

Objective: 0100; plus anti-filter bubble slot that forces in counter-examples.

Generation prompts:

Add a persona frame: Weight sources you have engaged with, but report dissenting high-trust evidence.

Objective mode prompt removes the persona and asks for minimize personalization; maximize trust diversity.

Multi-user generality

Per-user dirs:

.knowledgeusersuser_id
profile.yml declared prefs (optional)
eventsevents.jsonl raw interaction log
vectors.npy interest vectors (globaldomainrecency)
overlaysego_graph. user overlay on the global graph
caches... cached re-ranks summaries


The global corpus lives as it does now. User overlays dont mutate it.

Switching users is just switching the user_id in CLI flags or env.

Objective selections (minimize influence)

Mode Objective:

0; stronger (Trust) and (Diversity).

Disallow user-specific boosts; use domain allowlist and venue priors only.

Report exposure audit: distribution of sources and stances used.

Optionally dual runs: produce personalized and objective answers side-by-side with different evidence packs.

Where it fits in your repo (concrete)

New modules:

knprofile.py loadsave profile.yml, manage user_id

knpersonalizer.py computeupdate interest vectors; compute PersonalRelevance

knego_graph.py buildupdate user overlay graph

kntrust.py (already planned) trust bands per source; expose defaults per mode

knorchestrator.py add mode-aware routing budget allocation

knmmr.py noveltydiversity utilities

Config additions:

.knowledgeconfigpipeline.yml
users:
default_user: "me"
modes:
personal:
alpha: 0.25 Trust
beta: 0.25 Utility
gamma: 0.45 PersonalRelevance
delta: 0.15 Diversity
balanced:
alpha: 0.30
beta: 0.30
gamma: 0.20
delta: 0.25
objective:
alpha: 0.45
beta: 0.35
gamma: 0.00
delta: 0.25
interest:
decay_half_life_days: 45
per_domain_vectors: true
recent_window_days: 14
retrieval_budget:
personal: personal_pct: 0.6, trust_pct: 0.4
balanced: personal_pct: 0.4, trust_pct: 0.6
objective: personal_pct: 0.0, trust_pct: 1.0


CLI knobs (examples):

Use my profile in personal mode
python binquery_rag.py --q "What should I read next about X?" --user me --mode personal

Balanced synthesis over my Reddit saves papers
python binexport_monofile.py --q "State of topic Y" --user me --mode balanced --pack-mode topic

Objective brief (no personalization) from papersnews only
python binexport_monofile.py --q "Objective summary of Z" --mode objective --sources papers,news

Privacy audit

Personal events never leave disk; PII plugin runs on both global and user artifacts.

Every export includes an evidence ledger (sources why they were included).

Optional encryption-at-rest for .knowledgeusersuser_id if you want it.

Defaults Id start with

Half-life 45 days for interest decay; 3 interest vectors (globaldomainrecent).

Personal mode: 6040 budget split; Balanced: 4060; Objective: 0100.

Dual-answer option for high-stakes queries: return personalized objective with evidence lists.

If you like, I can draft the profile.yml format, the UserEvent JSONL schema, and a tiny binuser_events_import.py (for Reddit savesTwitter likesDiscord exports) so your profile builds itself the moment you drop those files in.

Hierarchical Summarization Metadata Roadmap

1. Chunking Foundations (v1)
- Upgrade knchunking.py to capture structure: detect headingsTOC, bullet hierarchies; store section_id, parent_section_id, level, title, breadcrumbs.
- Persist per-document structure files (e.g., .knowledgeindexesstructuredoc_id.json) to describe the tree and link to chunk IDs.
- Tailor chunking policies by document type (code vs. prose vs. scanned PDFs) with pluggable detectors.

2. Map-Reduce Pipeline (v1)
- Ensure chunk-summary runs automatically on ingestupdate; extend to chunk-level attributes (keywords, glossary, risk flags).
- Generalize doc_reduce into reusable reducers per attribute type; each reducer consumes ordered chunk outputs, emits doc-level JSON, records provenance timestamps.
- Add dashboard controls to trigger map-reduce batches, display chunk coverage, and surface doc-level aggregates.

3. Multi-Level Summaries (v2)
- Introduce section-level reducers (chunk section doc). Support recursive summarization for deep documents.
- Implement corpus-level reducers to summarize domainsrepos using doc summaries (e.g., domain overview, changelog).
- Auto-refresh stale sections by comparing chunkdoc mtimes; trigger incremental rebuilds.

4. Attribute Aggregation (v2)
- Refactor doc-level attributes to operate over chunk outputs (merge per-section topic tags, glossary terms, requirements, etc.).
- Support incremental updates: when a subset of chunks change, recompute affected sectionsdoc aggregates only.
- Store merge heuristics (frequency, confidence, section coverage) so outputs remain explainable.

5. Repository-Level Hierarchies (v3)
- Maintain multiple virtual hierarchies: original file tree, logical domains, priority tiers, temporal views.
- Compute rollups for each hierarchy node (summary, highlights, metrics) using sectiondoc outputs.
- Allow dashboardagents to switch hierarchies and retrieve the corresponding summariesmetadata.

6. Retrieval Storage (v3)
- Index chunksection summaries alongside embeddings; include hierarchy metadata so retrieval can surface context at the right depth.
- Provide APIs for hierarchy traversal (e.g., apihierarchyname returning nodes, summaries, children).
- Cache LLM inputsoutputs for chunk section stages to avoid redundant calls; record latencystats for tuning.

7. Guardrails Tooling (ongoing)
- Enforce strict token budgets per stage; re-chunk or flag when limits are exceeded.
- Add CLI utilities for hierarchy buildsstatus (binhierarchy_build.py, binhierarchy_status.py) and regression tests.
- Monitor coverage metrics (chunk summaries present, doc reduces up-to-date, hierarchy rollups fresh).

8. Future Directions
- Multi-perspective hierarchies (importance, recency, author ownership, user-defined playlists).
- Integration with trainingfinetuning pipelines (export structured summaries, provide grounding traces).
- Agent planner that uses hierarchy metadata to decide reading order, recommend updates, or trigger targeted re-summarization.

.setup.ps1
param(
RepoPath ".FAE",
Model "llama-3.1-8b-instruct"
)

if (-not (Test-Path ".venvScriptspython.exe"))
python -m venv .venv
else
Write-Host "Virtual environment already exists; skipping creation."


if (-not (Test-Path ".venvScriptsActivate.ps1"))
throw "Virtual environment missing activation script; rerun without workers or recreate manually."


. ..venvScriptsActivate.ps1

if (Test-Path "requirements-win.txt")
python -m pip install -r requirements-win.txt
else
python -m pip install -r requirements.txt

env:PYTHONPATH PSScriptRoot

try
resp Invoke-RestMethod -Method Get -Uri "http:localhost:1234v1models" -TimeoutSec 5
catch Write-Warning "LM Studio local server not reachable."

"
OPENAI_BASE_URLhttp:10.5.0.2:12345v1
OPENAI_API_KEYlm-studio
OPENAI_MODELModel
EMBED_MODELBAAIbge-small-en-v1.5
REPO_PATHRepoPath
KN_ROOT.knowledge
OCR_ENABLEDfalse
" Set-Content .env -Encoding UTF8

python .biningest_build_graph.py --repo RepoPath --full

.biningest_build_graph.py
import argparse, pathlib
from kn.config import load_configs
from kn.file_index import initial_scan
from kn.chunking import chunk_repo
from kn.embeddings import embed_chunks
from kn.graph_nx import build_or_update_graph, summarize_communities

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--repo", requiredTrue)
ap.add_argument("--full", action"store_true")
args ap.parse_args()
cfg load_configs()
repo pathlib.Path(args.repo).resolve()
print(" scanning repo...")
docs initial_scan(repo, cfg)
print(f" len(docs) docs found")
print(" building chunks...")
chunks chunk_repo(docs, cfg, repo_rootrepo)
print(" embedding chunks...")
embed_chunks(chunks, cfg)
print(" updating graph...")
build_or_update_graph(chunks, cfg)
print(" community summaries...")
summarize_communities(cfg)
print(" ")

.binwatch_daemon.py
import argparse, pathlib
from kn.config import load_configs
from kn.file_index import watch_changes

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--repo", requiredTrue, help"Path to watch")
args ap.parse_args()
cfg load_configs()
repo pathlib.Path(args.repo).resolve()
print(f" monitoring repo ... (CtrlC to quit)")
watch_changes(repo, cfg)

.binenrich_worker.py
import argparse, time, json, threading, pathlib
from kn.config import load_configs
from kn.jobs_sqlite import (
ensure_db,
dequeue_batch,
ack_job,
iter_docs_for_jobs,
fail_and_requeue_job,
list_pending_plugins,
try_acquire,
release,
set_limit,
reset_status,
reset_running_jobs,
reset_counter,
)


ATTR_ROOT pathlib.Path('.knowledgeindexesattributes')


def _job_output_exists(plugin: str, job: dict) - bool:
payload job.get('payload') or
if payload.get('overwrite') or payload.get('force'):
return False
doc_id job.get('doc_id')
plugin_dir ATTR_ROOT plugin
if plugin 'chunk-summary':
chunk_id payload.get('chunk_id')
if not chunk_id:
return False
return plugin_dir.joinpath(f"chunk_id.json").exists()
if plugin 'summaries':
mode payload.get('mode')
if not mode or not doc_id:
return False
return plugin_dir.joinpath(f"doc_id_mode.json").exists()
if not doc_id:
return False
return plugin_dir.joinpath(f"doc_id.json").exists()

def run_once(plugins, cfg, batch_size16):
ensure_db(cfg)
If no jobs for requested plugins, peek pending list and suggest available
jobs dequeue_batch(cfg, wanted_pluginsplugins, limitbatch_size)
if not jobs:
try to broaden to any pending plugins if requested plugins empty
pend list_pending_plugins(cfg)
avail
if not avail:
return 0
jobs dequeue_batch(cfg, wanted_pluginsavail, limitbatch_size)
if not jobs:
return 0
docs iter_docs_for_jobs(jobs)
def _load_chunk_by_id(chunk_id: str):
import json as _json, pathlib as _pathlib
p _pathlib.Path('.knowledgeindexeschunks')f"chunk_id.json"
if p.exists():
try:
return _json.loads(p.read_text(encoding'utf-8'))
except Exception:
return None
return None
import subprocess, pathlib, sys
by_plugin
for j in jobs:
by_plugin.setdefault(j , ).append(j)
for plugin, items in by_plugin.items():
remaining
for j in items:
if _job_output_exists(plugin, j):
ack_job(cfg, j )
print(f" plugin: doc j.get('doc_id') already has output; skipping (no overwrite)")
else:
remaining.append(j)
if not remaining:
continue
acquired try_acquire(cfg, 'llm_concurrency')
if not acquired:
print(f" plugin: concurrency limit reached, skipping batch")
continue
fs_name plugin.replace('-', '_')
pypath pathlib.Path(f"pluginsattributesfs_name.py")
if not pypath.exists():
print(f" plugin not found: plugin")
for j in items:
fail_and_requeue_job(cfg, j , error_message"plugin not found", back_to_pendingFalse)
continue
inp_lines
doc_ids
for j in remaining:
payload j.get("payload") or
doc None
if payload.get('chunk_id'):
doc _load_chunk_by_id(payload )
if doc is None:
doc docs.get(j ) fallback
if doc:
merged dict(doc)
merged payload
inp_lines.append(json.dumps(merged, ensure_asciiFalse))
doc_ids.append(j )
if not inp_lines:
for j in remaining:
fail_and_requeue_job(cfg, j , error_message"no input doc", back_to_pendingFalse)
continue
try:
per-plugin process timeout (seconds)
pcfg (cfg.get('plugins') or ).get(plugin) or
proc_timeout pcfg.get('process_timeout') if isinstance(pcfg, dict) else None
if not isinstance(proc_timeout, (int, float)):
proc_timeout 600 if plugin in ('multi-basic','doc-skeleton') else 300
summary_docs ",".join(doc_ids )
if len(doc_ids) 4:
summary_docs ",..."
if summary_docs:
print(f" pluginplugin docssummary_docs")
proc subprocess.Popen( , stdinsubprocess.PIPE,
stdoutsubprocess.PIPE, stderrsubprocess.STDOUT, textTrue, bufsize1)

def _pump_stdout():
try:
for raw_line in proc.stdout:
print(f" raw_line.rstrip()" )
except Exception:
pass

reader threading.Thread(target_pump_stdout, daemonTrue)
reader.start()

input_blob "n".join(inp_lines) "n"
try:
proc.stdin.write(input_blob)
proc.stdin.close()
except Exception:
pass

try:
proc.wait(timeoutproc_timeout)
except subprocess.TimeoutExpired:
print(f" plugin: timeout, requeueing batch")
try:
proc.kill()
except Exception:
pass
try:
proc.communicate(timeout1)
except Exception:
pass
for j in remaining:
fail_and_requeue_job(cfg, j , error_message"timeout", back_to_pendingTrue)
continue
finally:
reader.join(timeout0.5)

if proc.returncode and proc.returncode ! 0:
msg f"exit code proc.returncode"
print(f" plugin: msg, requeueing batch")
for j in remaining:
fail_and_requeue_job(cfg, j , error_messagemsg, back_to_pendingTrue)
else:
print(f" plugin: len(remaining) docs processed")
for j in remaining:
ack_job(cfg, j )
except Exception as e:
msg str(e)
print(f" plugin: error msg")
back_to_pending True
if 'Context too large' in msg or 'context too large' in msg:
back_to_pending False
for j in remaining:
fail_and_requeue_job(cfg, j , error_messagemsg, back_to_pendingback_to_pending)
finally:
if acquired:
release(cfg, 'llm_concurrency')
print(f" pluginplugin")
return len(jobs)

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--plugins", requiredTrue, help"comma-separated plugin names or '' for any pending")
ap.add_argument("--watch", action"store_true")
ap.add_argument("--batch", typeint, default16)
ap.add_argument("--any-pending", action"store_true", dest"any_pending")
ap.add_argument("--max-inflight", typeint, default2, help"global concurrent LLM calls across workers")
args ap.parse_args()
cfg load_configs()
plugins
any_pending args.any_pending or (len(plugins)1 and plugins in ("","any"))
set concurrency limit at start
set_limit(cfg, 'llm_concurrency', max(1, int(args.max_inflight)))
reset_running_jobs(cfg, status'pending')
reset_counter(cfg, 'llm_concurrency')
while True:
use_plugins plugins
if any_pending:
dyn list_pending_plugins(cfg)
use_plugins dyn if dyn else
n run_once(use_plugins, cfg, batch_sizeargs.batch)
if not args.watch:
break
if n 0:
time.sleep(2)

.binquery_rag.py
import argparse
from kn.retrieval import answer_query
from kn.config import load_configs

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--q", requiredTrue)
ap.add_argument("--scope", defaultNone)
ap.add_argument("--topk", typeint, default12)
args ap.parse_args()
cfg load_configs()
print(answer_query(args.q, cfg, scopeargs.scope, topkargs.topk))

.binexport_monofile.py
import argparse, pathlib
from kn.exporter import export_monofile
from kn.config import load_configs

if __name__ "__main__":
ap argparse.ArgumentParser()
ap.add_argument("--q", requiredTrue)
ap.add_argument("--budget", typeint, defaultNone)
ap.add_argument("--scope", defaultNone)
ap.add_argument("--no-meta", action"store_true", help"Exclude per-chunk headerscitations")
ap.add_argument("--out", requiredTrue)
args ap.parse_args()
cfg load_configs()
out pathlib.Path(args.out)
out.parent.mkdir(parentsTrue, exist_okTrue)
export_monofile(args.q, cfg, out, budgetargs.budget, scopeargs.scope, include_metanot args.no_meta)
print(f" wrote out")

.binreport_attrs.py
import json, csv, pathlib

def main():
base pathlib.Path('.knowledgeindexesattributes')
out pathlib.Path('.knowledgeexportsreports')
out.mkdir(parentsTrue, exist_okTrue)

def load(name: str):
result
p base name
if p.exists():
for f in p.glob('.json'):
try:
rec json.loads(f.read_text(encoding'utf-8'))
result rec
except Exception:
continue
return result

summaries load('summary-20w')
tags load('topic-tags')
pii load('pii-scan')
ids sorted(set(summaries) set(tags) set(pii))

rows
for i in ids:
rows.append(
'doc_id': i,
'summary': summaries.get(i, ).get('value', ''),
'tags': tags.get(i, ).get('value', ''),
'pii_risk': pii.get(i, ).get('risk', ''),
)

csv_path out 'attributes_catalog.csv'
with csv_path.open('w', newline'', encoding'utf-8') as f:
writer csv.DictWriter(f, fieldnames )
writer.writeheader()
writer.writerows(rows)
print(f" wrote csv_path")

if __name__ '__main__':
main()




.binreport_communities.py
import json, pathlib

def main():
src pathlib.Path('.knowledgeindexessummariescommunities.jsonl')
out_dir pathlib.Path('.knowledgeexportsreports')
out_dir.mkdir(parentsTrue, exist_okTrue)
lines src.read_text(encoding'utf-8').splitlines() if src.exists() else
md
for l in lines :
try:
c json.loads(l)
md.append(f"- c (sizec ): c ")
except Exception:
continue
out out_dir 'communities.md'
out.write_text('n'.join(md), encoding'utf-8')
print(f" wrote out")

if __name__ '__main__':
main()



.binexport_graph.py
import json, pathlib, networkx as nx

def main():
graph_path pathlib.Path('.knowledgeindexesgraphgraph.jsonl')
out_dir pathlib.Path('.knowledgeexportsgraph')
out_dir.mkdir(parentsTrue, exist_okTrue)
G nx.Graph()
if graph_path.exists():
for l in graph_path.read_text(encoding'utf-8').splitlines():
try:
d json.loads(l)
G.add_edge(d , d , weightd.get('weight', 1))
except Exception:
continue
nx.write_gexf(G, str(out_dir 'graph.gexf'))
nx.write_graphml(G, str(out_dir 'graph.graphml'))
print(f" wrote out_dir 'graph.gexf' and out_dir 'graph.graphml'")

if __name__ '__main__':
main()



.binplanner_llm.py
import argparse, json, pathlib, collections
from kn.config import load_configs
from kn.jobs_sqlite import ensure_db, enqueue
from kn.utils.llm_client import chat

CHUNK_DIR pathlib.Path('.knowledgeindexeschunks')
COMM_PATH pathlib.Path('.knowledgeindexessummariescommunities.jsonl')

def collect_docs_and_domains():
doc_to_domain: dict
domain_counts collections.Counter()
for p in CHUNK_DIR.glob('.json'):
try:
rec json.loads(p.read_text(encoding'utf-8'))
except Exception:
continue
doc_id rec.get('doc_id')
meta rec.get('meta', )
domain meta.get('domain', 'root') or 'root'
if doc_id not in doc_to_domain:
doc_to_domain domain
domain_counts 1
return doc_to_domain, domain_counts

def load_community_summaries(limit8):
out
if COMM_PATH.exists():
for line in COMM_PATH.read_text(encoding'utf-8').splitlines() :
try:
c json.loads(line)
out.append(f"c.get('id') sizec.get('size'): c.get('summary')")
except Exception:
continue
return out

SYS_PROMPT (
"You are a planning agent for repository analysis. Based on the context, propose high-value attribute jobs.n"
"Available plugins: .n"
"Scopes you may use: 'all' or a list of domains from the provided list.n"
"Return STRICT JSON: "jobs": .n"
"Choose at most 6 jobs. Prefer targeted scopes over 'all'."
)

def build_planner_context():
doc_to_domain, domain_counts collect_docs_and_domains()
domains_list ', '.join( ) or 'root(0)'
comms load_community_summaries()
ctx
"Context:",
f"Domains: domains_list",
"Top communities:",

return "n".join(ctx), doc_to_domain

def expand_scope(scope: str, doc_to_domain: dict ) - list :
if scope.strip().lower() 'all':
return list(doc_to_domain.keys())
if scope.startswith('domain:'):
want scope.split(':',1) .strip().lower()
return
Unknown scope no docs
return

def main():
ap argparse.ArgumentParser()
ap.add_argument('--max-jobs', typeint, default64)
args ap.parse_args()

cfg load_configs()
ensure_db(cfg)
context, doc_to_domain build_planner_context()

prompt SYS_PROMPT "nn" context "nnReturn JSON now."
try:
raw chat(prompt, max_tokens400, temperature0.2)
plan json.loads(raw)
jobs plan.get('jobs', ) if isinstance(plan, dict) else
except Exception:
jobs

enq 0
for job in jobs:
plugin str(job.get('plugin','')).strip()
scope str(job.get('scope','all')).strip()
limit job.get('limit')
docs expand_scope(scope, doc_to_domain)
if limit and isinstance(limit, int):
docs docs
for doc_id in docs:
enqueue(cfg, plugin, doc_id, payload)
enq 1
if enq args.max_jobs:
break
if enq args.max_jobs:
break
print(f" enqueued enq jobs from LLM plan")

if __name__ '__main__':
main()



.binjobs_status.py
import argparse
import collections
import datetime as dt
import json
import os
import pathlib
import sqlite3
import sys

Ensure project root is on sys.path when invoked from subdirectories
ROOT pathlib.Path(__file__).resolve().parents
if str(ROOT) not in sys.path:
sys.path.insert(0, str(ROOT))

from dotenv import load_dotenv
import requests


CHUNKS_DIR pathlib.Path('.knowledgeindexeschunks')
ATTR_DIR pathlib.Path('.knowledgeindexesattributes')
DB_PATH pathlib.Path('.knowledgequeuesjobs.sqlite')


def _unique_doc_ids():
seen set()
if not CHUNKS_DIR.exists():
return seen
for p in CHUNKS_DIR.glob('.json'):
try:
doc_id p.stem.split('-')
seen.add(doc_id)
except Exception:
continue
return seen


def _attribute_coverage(plugins, doc_ids):
coverage
for plugin in plugins:
p ATTR_DIR plugin
have 0
missing
if p.exists():
existing f.stem for f in p.glob('.json')
for d in doc_ids:
if d in existing:
have 1
else:
missing.append(d)
else:
missing list(doc_ids)
total len(doc_ids)
coverage
'total_docs': total,
'have': have,
'pct': round(100.0 (have total), 1) if total else 0.0,
'missing_examples': missing ,

return coverage


def _domains_breakdown():
from json import loads
dom collections.Counter()
for p in CHUNKS_DIR.glob('.json'):
try:
rec loads(p.read_text(encoding'utf-8'))
d (rec.get('meta') or ).get('domain') or 'root'
doc_id rec.get('doc_id')
count per-doc by domain (first chunk only)
dom 1
except Exception:
continue
out collections.Counter()
for (_, d), _ in dom.items():
out 1
return


def _db_summary(minutes_recent60):
if not DB_PATH.exists():
return
'present': False,
'total': 0,
'by_status': ,
'by_plugin_status': ,
'oldest_pending_min': None,
'recent_created': ,

con sqlite3.connect(DB_PATH)
con.row_factory sqlite3.Row
cur con.cursor()
out 'present': True
try:
by_status k: v for k, v in cur.execute(
"select status, count() from jobs group by status").fetchall()
out by_status
total sum(by_status.values()) if by_status else 0
out total

by plugin, status
plugin_status
for row in cur.execute(
"select plugin, status, count() as n from jobs group by plugin, status"):
plugin_status.setdefault(row , ) row
out plugin_status

durations per plugin (avg seconds)
try:
durations
for row in cur.execute(
"select plugin, avg(strftime('s',coalesce(completed_at, created_at)) - strftime('s',created_at)) as avg_s, count() as n "
"from jobs where status'done' and completed_at is not null group by plugin"):
durations 'avg_s': round(row or 0, 1), 'n': row
except Exception:
durations
out durations

oldest pending age (minutes)
row cur.execute(
"select created_at from jobs where status'pending' order by id asc limit 1").fetchone()
if row and row :
try:
created dt.datetime.fromisoformat(str(row ))
age_min (dt.datetime.now() - created).total_seconds() 60.0
except Exception:
age_min None
else:
age_min None
out round(age_min, 1) if age_min is not None else None

recent created by plugin within window
recent
try:
cutoff dt.datetime.now() - dt.timedelta(minutesminutes_recent)
cutoff_iso cutoff.isoformat(sep' ')
for row in cur.execute(
"select plugin, count() as n from jobs where created_at ? group by plugin",
(cutoff_iso,)):
recent row
except Exception:
recent
out recent

recent throughput and ETA
try:
cutoff dt.datetime.now() - dt.timedelta(minutesminutes_recent)
cutoff_iso cutoff.isoformat(sep' ')
row cur.execute(
"select count() as n from jobs where status'done' and completed_at ?",
(cutoff_iso,)
).fetchone()
recent_done_total row if row else 0
window_sec float(minutes_recent) 60.0 if minutes_recent else 1.0
overall_dps recent_done_total window_sec
plugin_dps
for row in cur.execute(
"select plugin, count() as n from jobs where status'done' and completed_at ? group by plugin",
(cutoff_iso,)
):
plugin_dps row window_sec
out
'window_min': minutes_recent,
'overall_dps': round(overall_dps, 4),
'plugin_dps': k: round(v, 4) for k, v in plugin_dps.items(),

pending_total sum((plugin_status.get(k, ).get('pending', 0) for k in plugin_status.keys()))
eta_overall (pending_total overall_dps) if overall_dps 0 else None
durations out.get('durations', )
plugin_eta
for plug, stat in plugin_status.items():
pend stat.get('pending', 0)
dps plugin_dps.get(plug, 0.0)
if dps 0:
plugin_eta pend dps
else:
avg_s (durations.get(plug) or ).get('avg_s') or 0
plugin_eta (pend avg_s) if avg_s 0 else None
out
'overall_sec': round(eta_overall, 1) if eta_overall is not None else None,
'plugin_sec': k: (round(v, 1) if v is not None else None) for k, v in plugin_eta.items(),

except Exception:
out 'window_min': minutes_recent, 'overall_dps': 0, 'plugin_dps':
out 'overall_sec': None, 'plugin_sec':
finally:
con.close()
return out


def _llm_health():
load_dotenv(overrideFalse)
base os.getenv('OPENAI_BASE_URL', 'http:localhost:1234v1')
try:
r requests.get(base.rstrip('') 'models', timeout5)
ok r.status_code 200
models r.json().get('data', ) if ok else
return 'reachable': ok, 'endpoint': base, 'models':
except Exception:
return 'reachable': False, 'endpoint': base, 'models':


def main():
ap argparse.ArgumentParser()
ap.add_argument('--recent-mins', typeint, default60)
ap.add_argument('--json', action'store_true')
args ap.parse_args()

plugins
doc_ids _unique_doc_ids()
data
'docs_total': len(doc_ids),
'domains': _domains_breakdown(),
'attributes_coverage': _attribute_coverage(plugins, doc_ids),
'queue': _db_summary(args.recent_mins),
'llm': _llm_health(),


if args.json:
print(json.dumps(data, ensure_asciiFalse, indent2))
return

Pretty print
print(f"docs_total: data ")
print("domains:")
for d in data :
print(f" - d : d ")
print("attributes coverage:")
for k, v in data .items():
print(f" - k: v v (v )")
print("queue:")
q data
print(f" present: q ")
if q :
print(f" total: q.get('total',0) by_status: q.get('by_status',)")
print(f" oldest_pending_min: q.get('oldest_pending_min')")
if q.get('recent_created'):
print(f" recent_created(args.recent_minsm): q ")
print(" by_plugin_status:")
for plug, m in q.get('by_plugin_status',).items():
print(f" - plug: m")
print("llm:")
llm data
print(f" reachable: llm endpoint: llm ")
if llm :
print(f" models: ', '.join(llm )")


if __name__ '__main__':
main()



.binrender_report.py
import json, pathlib, html, datetime as dt, sqlite3

CHUNKS_DIR pathlib.Path('.knowledgeindexeschunks')
ATTR_DIR pathlib.Path('.knowledgeindexesattributes')
COMM_PATH pathlib.Path('.knowledgeindexessummariescommunities.jsonl')
OUT_HTML pathlib.Path('.knowledgeexportsreportsindex.html')
GRAPH_PATH pathlib.Path('.knowledgeindexesgraphgraph.jsonl')

PLUGINS

def _iter_docs():
seen
for p in CHUNKS_DIR.glob('.json'):
try:
rec json.loads(p.read_text(encoding'utf-8'))
except Exception:
continue
doc_id rec.get('doc_id')
if doc_id in seen:
continue
meta rec.get('meta') or
seen
'doc_id': doc_id,
'rel': meta.get('rel') or meta.get('path') or '',
'domain': meta.get('domain') or 'root',

return list(seen.values())

def _load_attr(plugin, doc_id):
p ATTR_DIR plugin f'doc_id.json'
if not p.exists():
return None
try:
return json.loads(p.read_text(encoding'utf-8'))
except Exception:
return None

def _load_comms(max_items100):
out
if COMM_PATH.exists():
for i, line in enumerate(COMM_PATH.read_text(encoding'utf-8').splitlines()):
if i max_items: break
try:
c json.loads(line)
out.append(c)
except Exception:
continue
return out

def _esc(s):
return html.escape(str(s or ''))

def _render_doc_card(d):
doc_id d
parts )'div class'hdr'span class'rel'_esc(d )spanspan class'meta'doc:doc_id domain:_esc(d )spandiv"
attributes
def sec(title, txt):
if not txt: return
parts.append(f"div class'sec'div class'stit'_esc(title)divdiv class'sbody'pre_esc(txt)predivdiv")

summary
s _load_attr('summary-20w', doc_id); sec('Summary (20w)', s and s.get('value'))
t _load_attr('topic-tags', doc_id); sec('Tags', t and t.get('value'))
g _load_attr('glossary', doc_id); sec('Glossary', g and g.get('value'))
r _load_attr('requirements', doc_id);sec('Requirements', r and r.get('value'))
f _load_attr('faq-pairs', doc_id); sec('FAQ', f and f.get('value'))
td _load_attr('todo-items', doc_id); sec('TODOs', json.dumps(td.get('value'), ensure_asciiFalse, indent2) if td else '')
pii _load_attr('pii-scan', doc_id)
if pii:
sec('PII risk', f"riskpii.get('risk') findingslen(pii.get('value') or )")
parts.append("div")
return "n".join(parts)

def main():
docs sorted(_iter_docs(), keylambda x: x .lower())
comms _load_comms()
OUT_HTML.parent.mkdir(parentsTrue, exist_okTrue)
now dt.datetime.now().strftime('Y-m-d H:M')
attribute coverage rollups
doc_ids d for d in docs
attr_cov
attr_examples_missing
for plugin in PLUGINS:
have set()
p ATTR_DIR plugin
if p.exists():
for f in p.glob('.json'):
have.add(f.stem)
attr_cov 'have': len(have doc_ids), 'total': len(doc_ids)
missing list(doc_ids - have)
attr_examples_missing missing
PII rollup
pii_counts 'high':0,'any':0
pii_dir ATTR_DIR'pii-scan'
if pii_dir.exists():
for f in pii_dir.glob('.json'):
try:
d json.loads(f.read_text(encoding'utf-8'))
r float(d.get('risk') or 0)
if r0: pii_counts 1
if r0.8: pii_counts 1
except Exception:
continue
TODO rollup
todo_nonempty 0
todo_dir ATTR_DIR'todo-items'
if todo_dir.exists():
for f in todo_dir.glob('.json'):
try:
d json.loads(f.read_text(encoding'utf-8'))
if d.get('value'): todo_nonempty 1
except Exception:
continue
requirements presentmissing
req_dir ATTR_DIR'requirements'
req_have set()
if req_dir.exists():
req_have f.stem for f in req_dir.glob('.json')
req_missing list((doc_ids - req_have))
queue summary (best effort)
queue 'present': (pathlib.Path('.knowledgequeuesjobs.sqlite').exists())
if queue :
try:
con sqlite3.connect('.knowledgequeuesjobs.sqlite')
cur con.cursor()
by_status k: v for k, v in cur.execute("select status,count() from jobs group by status")
queue by_status; queue sum(by_status.values())
con.close()
except Exception:
pass
prepare graph data (nodeslinks), limited for performance
graph_nodes
graph_links
if GRAPH_PATH.exists():
Build nodeslinks
name_to_idx
degree
edges
for i, line in enumerate(GRAPH_PATH.read_text(encoding'utf-8').splitlines()):
try:
e json.loads(line)
except Exception:
continue
edges.append((e.get('src'), e.get('dst'), int(e.get('weight', 1))))
if len(edges) 1500: cap edges to keep UI responsive
break
for u, v, w in edges:
for n in (u, v):
if n not in name_to_idx:
idx len(graph_nodes)
name_to_idx idx
graph_nodes.append('id': n, 'deg': 0, 'idx': idx)
degree degree.get(u, 0) 1
degree degree.get(v, 0) 1
graph_links.append('source': name_to_idx , 'target': name_to_idx , 'weight': w)
for n in graph_nodes:
n degree.get(n , 0)
simple HTML
graph_json json.dumps('nodes': graph_nodes, 'links': graph_links, ensure_asciiFalse)
head f"""
!doctype html
htmlheadmeta charset'utf-8'
titleKnowledge Reporttitle
style
bodyfont-family:Segoe UI,Arial,sans-serif;margin:0;background:fafafa;color:222
headerbackground:0f62fe;color:fff;padding:12px 16px;display:flex;align-items:center;gap:16px
header h1font-size:18px;margin:0
.subopacity:.9;font-size:12px
.wrappadding:14px 16px
.rowdisplay:flex;gap:16px;flex-wrap:wrap
.colflex:1 1 380px;min-width:320px
.cardbackground:fff;border:1px solid eee;border-radius:8px;margin:10px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)
.hdrdisplay:flex;justify-content:space-between;gap:12px;padding:10px 12px;border-bottom:1px solid f0f0f0
.relfont-weight:600
.metafont-size:12px;color:666
.secpadding:10px 12px;border-top:1px dashed f2f2f2
.stitfont-size:12px;color:555;margin-bottom:6px;text-transform:uppercase;letter-spacing:.02em
.sbody prewhite-space:pre-wrap;margin:0;font-family:ui-monospace,Consolas,monospace;font-size:13px;line-height:1.35
.pilldisplay:inline-block;background:eef5ff;color:0f62fe;border-radius:999px;padding:2px 8px;margin:2px;font-size:12px
qwidth:100;padding:10px;border:1px solid ddd;border-radius:8px;margin:10px 0
style
headbody
scriptwindow.GRAPH_DATA graph_json;script
headerh1Knowledge Reporth1div class'sub'generated nowdivheader
div class'wrap'
input id'q' placeholder'Filter by file, domain or text... (client-side)'
div class'row'
div class'col'
div class'card'
div class'hdr'span class'rel'Communitiesspanspan class'meta'len(comms) itemsspandiv
div class'sec'
''.join(f"div class'pill'"_esc(c.get('id'))f" size_esc(c.get('size'))div" for c in comms)
div
div class'sec'
div class'stit'Top community keywordsdiv
div class'sbody'
ul
''.join(f"li- _esc(c.get('id')) (size _esc(c.get('size')) ): _esc(c.get('summary')) li" for c in comms)
ul
div
div
div
div
div class'col'
div class'card'
div class'hdr'span class'rel'Coveragespanspan class'meta'len(doc_ids) docsspandiv
div class'sec'
''.join(f"div style'margin:6px 0'div class'stit'_esc(k)divdiv style'background:eee;border-radius:6px;overflow:hidden'div style'height:8px;width: (attr_cov 100max(1,attr_cov )):.1f;background:0f62fe'divdivdiv style'font-size:12px;color:666'attr_cov attr_cov " (" missing: "', '.join(f"a hrefdoc-_esc(m)_esc(m)a" for m in attr_examples_missing ) if attr_examples_missing else '') "divdiv" for k in PLUGINS)
div
div class'sec'
div class'stit'PII TODOdiv
div class'sbody' style'font-size:12px;color:444'PII high-risk: pii_counts PII any: pii_counts Files with TODOs: todo_nonemptydiv
div
div class'sec'
div class'stit'Requirementsdiv
div class'sbody' style'font-size:12px;color:444'present: len(req_have) missing: len(req_missing) (' e.g. '', '.join(f"a hrefdoc-_esc(m)_esc(m)a" for m in req_missing )) if req_missing else ''div
div
div class'sec'
div class'stit'Queuediv
div class'sbody' style'font-size:12px;color:444'present: queue.get('present') total: queue.get('total','-') by_status: _esc(queue.get('by_status','-')) div
div
div
div
div class'col'
div class'card'
div class'hdr'span class'rel'Graph (preview)spanspan class'meta'len(graph_nodes) nodes len(graph_links) edgesspandiv
div class'sec'
div id'graphwrap' style'width:100;height:380px;'
svg id'graph' width'100' height'360'svg
div
div style'font-size:12px;color:666'Drag to move. Hover to highlight neighborhood. Filter: min degree input type'range' id'mindeg' min'0' max'10' value'0' span id'mindegv'0spandiv
div
div
div
div
div id'docs'
"""
cards
tail """
divdiv
script
Try local d3 first, then CDN; if all fail, render static graph
(function()
function onReady() if (typeof d3!'undefined') initGraph(); else renderStaticGraph();
function tryCdn()
var sdocument.createElement('script'); s.src'https:cdnjs.cloudflare.comajaxlibsd37.9.0d3.min.js';
s.onloadonReady;
s.onerrorfunction()
var s2document.createElement('script'); s2.src'https:unpkg.comd37distd3.min.js'; s2.onloadonReady; s2.onerroronReady; document.head.appendChild(s2);
;
document.head.appendChild(s);

var sldocument.createElement('script'); sl.src'd3.min.js'; sl.onloadonReady; sl.onerrortryCdn; document.head.appendChild(sl);
window.addEventListener('load', onReady);
)();

guard flags to prevent double rendering
window.__graphInited window.__graphInited false;
window.__staticRendered window.__staticRendered false;

const GRAPH_DATA (window.GRAPH_DATA window.GRAPH_DATA.nodes) ? window.GRAPH_DATA : nodes: , links: ;

function initGraph()
if (window.__graphInited) return; window.__graphInited true;
const svg d3.select('graph');
if (!svg.node()) return;
if (GRAPH_DATA.nodes.length0)
const wdocument.getElementById('graphwrap');
if(w) w.innerHTML "div style'padding:8px;color:666;font-size:12px'No graph data available.div";
return;

clear any prior content (e.g., static fallback)
svg.selectAll('').remove();
const wrap document.getElementById('graphwrap');
const width (wrap wrap.clientWidth) ? wrap.clientWidth : 640;
const height 360;
svg.attr('viewBox', 0 0 width height).attr('width', width).attr('height', height);
const g svg.append('g');
const zoom d3.zoom().on('zoom', (ev) g.attr('transform', ev.transform));
svg.call(zoom);

const color d3.scaleOrdinal(d3.schemeCategory10);
const deg GRAPH_DATA.nodes.map(nn.deg);
const maxDeg Math.max(1, d3.max(deg));
const rscale d3.scaleSqrt().domain( ).range( );

const nodes GRAPH_DATA.nodes.map(dObject.assign(, d));
const rawLinks GRAPH_DATA.links.map(dObject.assign(, d));
Build link objects referencing node objects by index to avoid id mismatches
const linkData rawLinks.map(l(source: nodes , target: nodes , weight: l.weight));

Seed positions in a circle for visibility before forces settle
const R Math.min(width, height) 0.45;
nodes.forEach((d,i) const a (2Math.PIi)nodes.length; d.x width2 RMath.cos(a); d.y height2 RMath.sin(a); );

const link g.append('g').attr('stroke', '999').attr('stroke-opacity', 0.35)
.selectAll('line').data(linkData).join('line').attr('stroke-width', dMath.max(1, Math.log(1(d.weight1))));
const node g.append('g').attr('stroke', 'fff').attr('stroke-width', 1)
.selectAll('circle').data(nodes).join('circle')
.attr('r', drscale(d.deg)).attr('fill', (d,i) color(i10)).call(drag(sim()));
const label g.append('g').selectAll('text').data(nodes).join('text')
.text(dd.id).attr('font-size', '10px').attr('fill', '444').attr('pointer-events', 'none');

function sim()
const simulation d3.forceSimulation(nodes)
.force('link', d3.forceLink(linkData).distance(40).strength(0.2))
.force('charge', d3.forceManyBody().strength(-80))
.force('center', d3.forceCenter(width2, height2))
.alpha(1).on('tick', ticked);
function ticked()
link.attr('x1', dd.source.x).attr('y1', dd.source.y).attr('x2', dd.target.x).attr('y2', dd.target.y);
node.attr('cx', dd.x).attr('cy', dd.y);
label.attr('x', dd.x6).attr('y', dd.y3);

return simulation;


function drag(simulation)
function dragstarted(event, d) if (!event.active) simulation.alphaTarget(0.3).restart(); d.fxd.x; d.fyd.y;
function dragged(event, d) d.fxevent.x; d.fyevent.y;
function dragended(event, d) if (!event.active) simulation.alphaTarget(0); d.fxnull; d.fynull;
return d3.drag().on('start', dragstarted).on('drag', dragged).on('end', dragended);


node.on('mouseover', (_, d)
const neigh new Set( );
linkData.forEach(l if(l.source.indexd.index) neigh.add(l.target.index); if(l.target.indexd.index) neigh.add(l.source.index); );
node.attr('opacity', n neigh.has(n.index)?1:0.15);
link.attr('opacity', l l.source.indexd.indexl.target.indexd.index?0.8:0.1);
label.attr('opacity', n neigh.has(n.index)?1:0.1);
).on('mouseout', ()
node.attr('opacity', 1); link.attr('opacity', 0.35); label.attr('opacity', 1);
);

const slider document.getElementById('mindeg');
const sv document.getElementById('mindegv');
if (slider)
slider.max String(Math.max(10, Math.floor(maxDeg)));
slider.addEventListener('input', ()
sv.textContent slider.value;
const minDeg slider.value;
node.attr('display', d d.degminDeg? null : 'none');
label.attr('display', d d.degminDeg? null : 'none');
link.attr('display', l (l.source.degminDeg l.target.degminDeg)? null : 'none');
);

Fallback: if nothing rendered for any reason, draw static
do not auto-render static here; static is reserved for no-d3 case


function renderStaticGraph()
if (window.__graphInited window.__staticRendered) return; window.__staticRendered true;
const svg document.getElementById('graph');
const wrap document.getElementById('graphwrap');
if (!svg !wrap) return;
const width (wrap wrap.clientWidth) ? wrap.clientWidth : 640;
const height 360;
svg.setAttribute('viewBox', 0 0 width height);
const ns 'http:www.w3.org2000svg';
Clear
while (svg.firstChild) svg.removeChild(svg.firstChild);
const nodes GRAPH_DATA.nodes.slice(0, 500); cap
const links GRAPH_DATA.links.slice(0, 1500);
const R Math.min(width, height) 0.42;
nodes.forEach((d,i) const a (2Math.PIi)nodes.length; d.x width2 RMath.cos(a); d.y height2 RMath.sin(a); );
draw links
links.forEach(l
const s nodes ; const t nodes ; if(!s!t) return;
const line document.createElementNS(ns, 'line');
line.setAttribute('x1', s.x); line.setAttribute('y1', s.y);
line.setAttribute('x2', t.x); line.setAttribute('y2', t.y);
line.setAttribute('stroke', 'bbb'); line.setAttribute('stroke-opacity', '0.5'); line.setAttribute('stroke-width', String(Math.max(1, Math.log(1(l.weight1)))));
svg.appendChild(line);
);
draw nodes labels
nodes.forEach((d,i)
const circ document.createElementNS(ns, 'circle'); circ.setAttribute('cx', d.x); circ.setAttribute('cy', d.y); circ.setAttribute('r', String(3 Math.min(12, (d.deg0)2))); circ.setAttribute('fill', '0f62fe'); circ.setAttribute('stroke', 'fff'); circ.setAttribute('stroke-width','1'); svg.appendChild(circ);
if (i 400) const tx document.createElementNS(ns, 'text'); tx.setAttribute('x', d.x6); tx.setAttribute('y', d.y3); tx.setAttribute('font-size','10'); tx.setAttribute('fill','444'); tx.textContent d.id; svg.appendChild(tx);
);


const q document.getElementById('q');
q.addEventListener('input', ()
const term q.value.toLowerCase();
document.querySelectorAll('docs .card').forEach(card
card.style.display card.innerText.toLowerCase().includes(term) ? '' : 'none';
);
);
script
bodyhtml
"""
html_out head.replace('__NODES__', json.dumps(graph_nodes, ensure_asciiFalse))
.replace('__LINKS__', json.dumps(graph_links, ensure_asciiFalse))
"n".join(cards) tail
OUT_HTML.write_text(html_out, encoding'utf-8')
print(f" wrote OUT_HTML")

if __name__ '__main__':
main()



.bintest_lmstudio.py
import argparse
import os
import sys
import textwrap
from typing import Iterable

import requests

try:
from kn.config import load_configs
except Exception: noqa: BLE001
load_configs None type: ignore

DEFAULT_PROMPT "This is a connectivity test. Reply with a short acknowledgement."


def _print_header(title: str) - None:
print("n" "" len(title))
print(title)
print("" len(title))


def list_models(base_url: str, timeout: int 5) - list :
url base_url.rstrip('') 'models'
resp requests.get(url, timeouttimeout)
resp.raise_for_status()
data resp.json()
models ) if m.get('id')
return models


def ping_model(base_url: str, model: str, prompt: str, timeout: int 30, max_tokens: int 32,
temperature: float None None) - tuple :
url base_url.rstrip('') 'chatcompletions'
payload
'model': model,
'messages': ,
'max_tokens': max_tokens,

if temperature is not None:
payload temperature
resp requests.post(url, jsonpayload, timeouttimeout)
status resp.status_code
content None
if resp.ok:
try:
content resp.json()
except Exception as exc: noqa: BLE001
content f"Failed to parse response: exc" type: ignore
else:
try:
content resp.text
except Exception: noqa: BLE001
content None
return status, content


def check_dashboard(base_url: str, timeout: int 5) - dict :
"""Ping key dashboard endpoints and return endpoint: (status, details)."""
results: dict
base base_url.rstrip('')

def _record(name: str, response: requests.Response, body_preview: str None None) - None:
snippet ""
if body_preview is not None:
snippet body_preview
else:
try:
snippet textwrap.shorten(response.text or '', width120, placeholder'...')
except Exception: noqa: BLE001
snippet "no body"
results (response.status_code, snippet)

try:
resp requests.get(f"baseapistatus", timeouttimeout)
if resp.ok:
body_preview textwrap.shorten(resp.text, width120, placeholder'...')
else:
body_preview resp.text
_record('GET apistatus', resp, body_preview)
except Exception as exc: noqa: BLE001
results (0, f"exc")

try:
resp requests.get(f"baseapiqueuelist?limit5", timeouttimeout)
_record('GET apiqueuelist', resp)
except Exception as exc: noqa: BLE001
results (0, f"exc")

try:
payload 'mode': 'reset-running'
resp requests.post(f"baseapiqueueclear", jsonpayload, timeouttimeout)
_record('POST apiqueueclear', resp)
except Exception as exc: noqa: BLE001
results (0, f"exc")

try:
resp requests.get(f"baseapiworkers", timeouttimeout)
_record('GET apiworkers', resp)
except Exception as exc: noqa: BLE001
results (0, f"exc")

return results


def summarize_results(title: str, items: Iterable ) - None:
_print_header(title)
for name, (status, detail) in items:
prefix 'OK' if status 200 else 'ERR'
print(f" name - statusstatus detaildetail")


def main(argv: list None None) - int:
parser argparse.ArgumentParser(description"Check LM Studio availability and dashboard endpoints.")
parser.add_argument('--base-url', defaultos.getenv('OPENAI_BASE_URL', 'http:127.0.0.1:12345v1'),
help'LM Studio base URL (default: (default)s or OPENAI_BASE_URL env)')
parser.add_argument('--dashboard-url', default'http:127.0.0.1:5051',
help'Dashboard base URL (default: (default)s)')
parser.add_argument('--models', nargs'', defaultNone,
help'Specific model IDs to probe. If absent, lists models only.')
parser.add_argument('--prompt', defaultDEFAULT_PROMPT, help'Prompt text for chat check.')
parser.add_argument('--max-tokens', typeint, default32, help'max_tokens for chat check (default: (default)s)')
parser.add_argument('--temperature', typefloat, defaultNone,
help'Optional temperature value for chat checks.')
parser.add_argument('--timeout', typeint, default30, help'HTTP timeout in seconds (default: (default)s)')
parser.add_argument('--skip-dashboard', action'store_true', help'Skip dashboard API checks.')
parser.add_argument('--plugins', nargs'', defaultNone,
help'Plugin names (from config) to resolve into model IDs for chat checks.')
parser.add_argument('--show-response', action'store_true', help'Print the full model response instead of a preview.')
args parser.parse_args(argv)

base args.base_url.rstrip('')
_print_header(f"LM Studio Diagnostics base")

try:
models list_models(base, timeoutargs.timeout)
except requests.RequestException as exc:
print(f" Could not reach base 'models': exc")
return 1

if not models:
print(" No models returned by the server.")
else:
print("Available models:")
for model in models:
print(f" - model")

plugin_models: list
if args.plugins:
if load_configs is None:
print(" kn.config.load_configs not available; skipping plugin resolution.")
else:
cfg load_configs()
for plugin in args.plugins:
p cfg.get('plugins', ).get(plugin)
if not isinstance(p, dict):
print(f" Plugin 'plugin' not found in config; skipping")
continue
llm_cfg p.get('llm') if isinstance(p.get('llm'), dict) else
model_id llm_cfg.get('model') or cfg.get('llm', ).get('model')
if not model_id:
print(f" Plugin 'plugin' has no model configured; skipping")
continue
plugin_models.append(model_id)
print(f"Resolved plugin 'plugin' - model 'model_id'")

models_to_check
seen set()
for model in ), plugin_models:
m model.strip()
if not m or m in seen:
continue
seen.add(m)
models_to_check.append(m)

if models_to_check:
_print_header("Chat Checks")
for model in models_to_check:
model model.strip()
if not model:
continue
print(f"Testing model: model")
try:
status, content ping_model(base, model, promptargs.prompt,
timeoutargs.timeout, max_tokensargs.max_tokens,
temperatureargs.temperature)
except requests.RequestException as exc:
print(f" Request failed: exc")
continue

if status 200:
if args.show_response:
print(f" status200 responsecontent!r")
else:
preview textwrap.shorten(content or '', width120, placeholder'...')
print(f" status200 responsepreview!r (use --show-response to display full text)")
else:
snippet textwrap.shorten(content or '', width120, placeholder'...')
print(f" statusstatus bodysnippet!r")
else:
print("nNo specific models requested for chat check. Use --models or --plugins to verify.")

if not args.skip_dashboard:
dashboard args.dashboard_url.rstrip('')
results check_dashboard(dashboard, timeoutargs.timeout)
summarize_results(f"Dashboard Diagnostics dashboard", sorted(results.items()))

return 0


if __name__ '__main__': pragma: no cover
raise SystemExit(main())


.bintest_llm_gateway.py
import argparse
import threading
import time
import sys

from kn.config import load_configs
from kn.llm_gateway.runner import run_gateway_service
from kn.llm_gateway.client import submit_chat_request
from kn.llm_gateway.errors import GatewayError, QueueTimeoutError, RequestRejectedError


def _run_gateway(service: str, cfg):
run_gateway_service(service, loop_foreverTrue, cfgcfg)


def main(argvNone):
parser argparse.ArgumentParser(description"Smoke-test the LLM gateway by issuing a simple chat request.")
parser.add_argument("prompt", nargs"?", default"Reply with OK.", help"Prompt to send through the gateway")
parser.add_argument("--service", defaultNone, help"Service name to target (defaults to llm.service from config)")
parser.add_argument("--model", defaultNone, help"Override model name for the request")
parser.add_argument("--boot-delay", typefloat, default0.5, help"Seconds to wait after starting the gateway thread")
args parser.parse_args(argv)

cfg load_configs()
service args.service or cfg.get("llm", ).get("service", "lmstudio")

gateway_thread threading.Thread(target_run_gateway, args(service, cfg), daemonTrue)
gateway_thread.start()
time.sleep(max(args.boot_delay, 0))

overrides "model": args.model if args.model else None
try:
response submit_chat_request(
args.prompt,
overridesoverrides,
cfgcfg,
)
except (QueueTimeoutError, RequestRejectedError, GatewayError) as err:
print(f"Gateway call failed: err")
return 1
print(response)
return 0


if __name__ "__main__":
sys.exit(main())








.binllm_gateway_service.py
import argparse
import logging
import sys

from kn.config import load_configs
from kn.llm_gateway.runner import run_gateway_service


def main(argvNone):
parser argparse.ArgumentParser(description"Run the LLM gateway service loop")
parser.add_argument("--service", defaultNone, help"Service name (defaults to llm.service from config)")
parser.add_argument("--log-level", default"INFO", help"Logging level (INFO, DEBUG, ...)")
parser.add_argument("--once", action"store_true", help"Process a single request and exit")
args parser.parse_args(argv)

logging.basicConfig(levelgetattr(logging, args.log_level.upper(), logging.INFO), format" (levelname)s (name)s: (message)s")

cfg load_configs()
service args.service or cfg.get("llm", ).get("service", "lmstudio")
run_gateway_service(service_nameservice, loop_forevernot args.once, cfgcfg)


if __name__ "__main__":
sys.exit(main())


.binjobs_reset.py
import argparse
import sys

from kn.config import load_configs
from kn.jobs_sqlite import reset_running_jobs, reset_counter, get_counter


def main(argvNone):
parser argparse.ArgumentParser(description"Reset job status and counters for the enrichment queue")
parser.add_argument("--status", default"pending", help"Status to set for running jobs (default: pending)")
parser.add_argument("--counter", default"llm_concurrency", help"Counter name to reset (default: llm_concurrency)")
parser.add_argument("--no-counter", action"store_true", help"Skip resetting the counter")
args parser.parse_args(argv)

cfg load_configs()

ids reset_running_jobs(cfg, statusargs.status)
print(f"Reset len(ids) jobs from 'running' to 'args.status'")

if not args.no_counter:
before get_counter(cfg, args.counter)
reset_counter(cfg, args.counter)
after get_counter(cfg, args.counter)
print(f"Counter 'args.counter' reset (was before, now after)")


if __name__ "__main__":
sys.exit(main())


.binplan_enqueue.py
import argparse, pathlib, time, json
from kn.config import load_configs
from kn.jobs_sqlite import ensure_db, enqueue

ATTR_ROOT pathlib.Path('.knowledgeindexesattributes')
CHUNKS_DIR pathlib.Path('.knowledgeindexeschunks')

def _plugin_dir_name(plugin: str) - str:
if plugin 'summaries':
return 'summaries'
return plugin.replace('_', '-')


def already_has(plugin: str, doc_id: str) - bool:
dir_name _plugin_dir_name(plugin)
if dir_name 'summaries':
pdir ATTR_ROOT 'summaries'
if not pdir.exists():
return False
return any(pp.name.startswith(f"doc_id_") for pp in pdir.glob(f"doc_id_.json"))
p ATTR_ROOT dir_name f"doc_id.json"
return p.exists()

def iter_doc_ids():
seen set()
for p in CHUNKS_DIR.glob('.json'):
doc_id p.stem.split('-')
if doc_id not in seen:
seen.add(doc_id)
yield doc_id

def latest_mtime_for_doc(doc_id: str) - float:
mt 0.0
for p in CHUNKS_DIR.glob(f"doc_id-.json"):
mt max(mt, p.stat().st_mtime)
return mt

def main():
ap argparse.ArgumentParser()
ap.add_argument('--plugins', requiredFalse, default'glossary,requirements,todo-items,faq-pairs', help'comma-separated plugin names')
ap.add_argument('--limit', typeint, default0, help'max jobs to enqueue in total (0 unlimited)')
ap.add_argument('--only-missing', action'store_true', help'enqueue only if attribute output file missing')
ap.add_argument('--changed-since-min', typeint, default0, help'only enqueue docs whose chunks changed in last N minutes (0 ignore)')
ap.add_argument('--summaries-modes', default'short,medium', help'for plugin "summaries", comma list of modes (short,medium,long,outline)')
ap.add_argument('--payload-json', defaultNone, help'optional JSON to attach as payload to each job (e.g., "llm":"model":"qwen2.5-7b-instruct","timeout":60)')
ap.add_argument('--map-reduce', action'store_true', help'enqueue chunk map (chunk-summary) then doc reduce (doc-reduce)')
ap.add_argument('--doc-ids', defaultNone, help'comma-separated list of doc ids; if provided, only these docs are enqueued')
args ap.parse_args()

cfg load_configs()
ensure_db(cfg)
plugins
sum_modes if 'summaries' in plugins else
try:
base_payload json.loads(args.payload_json) if args.payload_json else
except Exception:
base_payload

cutoff None
if args.changed_since_min and args.changed_since_min 0:
cutoff time.time() - args.changed_since_min 60

if args.doc_ids:
doc_list list(d.strip(): None for d in args.doc_ids.split(',') if d.strip().keys())
else:
doc_list list(iter_doc_ids())

count 0
for doc_id in doc_list:
if cutoff is not None and latest_mtime_for_doc(doc_id) cutoff:
continue
for plugin in plugins:
if args.only_missing and already_has(plugin, doc_id):
continue
if plugin 'summaries' and sum_modes:
for mode in sum_modes:
pl dict(base_payload); pl mode
enqueue(cfg, plugin, doc_id, payloadpl)
count 1
else:
enqueue(cfg, plugin, doc_id, payloadbase_payload)
count 1
if args.map_reduce:
enqueue chunk map jobs for all chunks of this doc, then doc reduce
from pathlib import Path
cdir Path('.knowledgeindexeschunks')
chunks sorted(cdir.glob(f"doc_id-.json"))
for cp in chunks:
chk_id cp.stem
pl dict(base_payload); pl chk_id
enqueue(cfg, 'chunk-summary', doc_id, payloadpl)
count 1
enqueue(cfg, 'doc-reduce', doc_id, payloadbase_payload)
count 1
if args.limit and count args.limit:
print(f" enqueued count jobs (limit reached)")
return
print(f" enqueued count jobs for plugins: ', '.join(plugins)")

if __name__ '__main__':
main()



.bindashboard_server.py
import copy
import json, pathlib, threading, time, os, sqlite3, collections, datetime as dt, subprocess, sys
import yaml
from flask import Flask, send_from_directory, jsonify, request

from kn.config import load_configs
from kn.jobs_sqlite import ensure_db, enqueue
from kn.llm_gateway.client import submit_generic_request
from kn.llm_gateway.storage import QueueStorage

ROOT pathlib.Path('.knowledge')
BASE_DIR pathlib.Path(__file__).resolve().parent
STATIC_DIR BASE_DIR 'dashboard_static'

app Flask(__name__, static_folderstr(STATIC_DIR))


from collections import deque
import itertools, datetime
_WORKER_ID_GEN itertools.count(1)
WORKERS: dict
GATEWAY
'proc': None,
'log': deque(maxlen500),
'service': None,
'started_at': None,



def _gateway_running() - bool:
proc GATEWAY.get('proc')
return proc is not None and proc.poll() is None


def _spawn_gateway(service: str None None, log_level: str 'INFO') - bool:
if _gateway_running():
return False
cfg load_configs()
service_name service or cfg.get('llm', ).get('service', 'lmstudio')
args
if log_level:
args.extend( )
if service_name:
args.extend( )
env os.environ.copy()
env str(pathlib.Path.cwd())
proc subprocess.Popen(args, stdoutsubprocess.PIPE, stderrsubprocess.STDOUT, textTrue, envenv)
GATEWAY proc
GATEWAY service_name
GATEWAY datetime.datetime.now().isoformat(timespec'seconds')
GATEWAY .clear()

def _reader():
try:
for line in proc.stdout:
GATEWAY .append(line.rstrip())
except Exception:
pass

threading.Thread(target_reader, daemonTrue).start()
return True


def _stop_gateway() - bool:
proc GATEWAY.get('proc')
if not proc:
return False
if proc.poll() is None:
try:
proc.terminate()
except Exception:
pass
GATEWAY None
GATEWAY None
return True


def _gateway_status() - dict:
cfg load_configs()
service_name GATEWAY.get('service') or cfg.get('llm', ).get('service', 'lmstudio')
queue_cfg cfg.get('llm_gateway', )
queue_path pathlib.Path(queue_cfg.get('queue_path', str(ROOT 'queues' 'llm_gateway.sqlite')))
stats
try:
storage QueueStorage(queue_path)
stats storage.stats(service_name if service_name else None)
except Exception:
stats
return
'running': _gateway_running(),
'service': service_name,
'started_at': GATEWAY.get('started_at'),
'queue': stats,
'log_tail': list(GATEWAY ) ,


CHUNKS_DIR pathlib.Path('.knowledgeindexeschunks')
ATTR_DIR pathlib.Path('.knowledgeindexesattributes')
DB_PATH pathlib.Path('.knowledgequeuesjobs.sqlite')
DOC_ATTR_PLUGINS
'summary-20w','topic-tags','pii-scan','glossary','requirements',
'todo-items','faq-pairs','keyphrases','bridge-candidates','risk-scan',
'recent-summary','summary-short','summary-medium','summary-long','summary-outline'



def _handle_worker_line(wid: int, line: str):
w WORKERS.get(wid)
if not w:
return
if line.startswith(' '):
expected format: pluginfoo docsdoc1,doc2,...
try:
rest line.split('', 1) .strip()
parts
for part in rest.split():
if '' in part:
k, v part.split('', 1)
parts v
w
'plugin': parts.get('plugin'),
'docs': parts.get('docs'),
'since': datetime.datetime.now().isoformat(timespec'seconds'),

except Exception:
w 'raw': line
elif line.startswith(' '):
w None

def _unique_doc_ids():
seen set()
if not CHUNKS_DIR.exists():
return seen
for p in CHUNKS_DIR.glob('.json'):
try:
doc_id p.stem.split('-')
seen.add(doc_id)
except Exception:
continue
return seen

def _attribute_coverage(plugins, doc_ids):
coverage
for plugin in plugins:
p ATTR_DIR plugin
have 0
missing
failed 0
failed_examples
if p.exists():
existing f.stem for f in p.glob('.json')
for d in doc_ids:
if d in existing:
fpath p f"d.json"
try:
data json.loads(fpath.read_text(encoding'utf-8'))
except Exception:
failed 1
if len(failed_examples) 5:
failed_examples.append('doc_id': d, 'error': 'invalid json')
continue
value data.get('value')
error data.get('error')
unavailable isinstance(value, str) and 'unavailable' in value.lower()
if error or unavailable:
failed 1
if len(failed_examples) 5:
failed_examples.append('doc_id': d, 'error': error or value)
else:
have 1
else:
missing.append(d)
else:
missing list(doc_ids)
total len(doc_ids)
coverage
'total_docs': total,
'have': have,
'failed': failed,
'pct': round(100.0 (have total), 1) if total else 0.0,
'missing_examples': missing ,
'failed_examples': failed_examples ,

return coverage

def _domains_breakdown():
from json import loads
dom collections.Counter()
for p in CHUNKS_DIR.glob('.json'):
try:
rec loads(p.read_text(encoding'utf-8'))
d (rec.get('meta') or ).get('domain') or 'root'
doc_id rec.get('doc_id')
dom 1
except Exception:
continue
out collections.Counter()
for (_, d), _ in dom.items():
out 1
return

def _db_summary(minutes_recent60):
if not DB_PATH.exists():
return
'present': False,
'total': 0,
'by_status': ,
'by_plugin_status': ,
'oldest_pending_min': None,
'recent_created': ,
'recent_failed': ,

con sqlite3.connect(DB_PATH)
con.row_factory sqlite3.Row
cur con.cursor()
out 'present': True
try:
by_status k: v for k, v in cur.execute(
"select status, count() from jobs group by status").fetchall()
out by_status
total sum(by_status.values()) if by_status else 0
out total
plugin_status
for row in cur.execute(
"select plugin, status, count() as n from jobs group by plugin, status"):
plugin_status.setdefault(row , ) row
out plugin_status
row cur.execute(
"select created_at from jobs where status'pending' order by id asc limit 1").fetchone()
if row and row :
try:
created dt.datetime.fromisoformat(str(row ))
age_min (dt.datetime.now() - created).total_seconds() 60.0
except Exception:
age_min None
else:
age_min None
out round(age_min, 1) if age_min is not None else None
recent
try:
cutoff dt.datetime.now() - dt.timedelta(minutesminutes_recent)
cutoff_iso cutoff.isoformat(sep' ')
for row in cur.execute(
"select plugin, count() as n from jobs where created_at ? group by plugin",
(cutoff_iso,)):
recent row
except Exception:
recent
out recent

recent failed per plugin
recent_failed
try:
cutoff dt.datetime.now() - dt.timedelta(minutesminutes_recent)
cutoff_iso cutoff.isoformat(sep' ')
for row in cur.execute(
"select plugin, count() as n from jobs where status'failed' and completed_at ? group by plugin",
(cutoff_iso,)):
recent_failed row
except Exception:
recent_failed
out recent_failed
finally:
con.close()
return out

LLM_MODELS_CACHE 'data': None, 'ts': 0

def _llm_health(force: boolFalse, ttl_sec: int600):
now time.time()
if not force and LLM_MODELS_CACHE and (now - LLM_MODELS_CACHE ttl_sec):
return copy.deepcopy(LLM_MODELS_CACHE )

cfg load_configs()
service cfg.get('llm', ).get('service', 'lmstudio')
service_cfg (cfg.get('llm_services') or ).get(service, )
model service_cfg.get('default_model') or cfg.get('llm', ).get('model')
try:
wait_timeout float((cfg.get('llm_gateway') or ).get('health_timeout_sec', 15))
resp submit_generic_request(
serviceservice,
payload'action': 'health', 'model': model,
metadata'source': 'dashboard.health',
cfgcfg,
waitTrue,
timeout_overridewait_timeout,
)
raw resp.raw or
data
'reachable': resp.success,
'endpoint': service_cfg.get('base_url') or cfg.get('llm', ).get('base_url'),
'models': (raw.get('available_models') or ) ,

if not resp.success:
data resp.error or raw.get('error')
LLM_MODELS_CACHE data
LLM_MODELS_CACHE now
return data
except Exception as exc:
data
'reachable': False,
'endpoint': service_cfg.get('base_url') or cfg.get('llm', ).get('base_url'),
'models': ,
'error': str(exc),

LLM_MODELS_CACHE data
LLM_MODELS_CACHE now
return data

def _read_jobs_status():
plugins DOC_ATTR_PLUGINS
doc_ids _unique_doc_ids()
data
'docs_total': len(doc_ids),
'domains': _domains_breakdown(),
'attributes_coverage': _attribute_coverage(plugins, doc_ids),
'queue': _db_summary(60),
'llm': _llm_health(),
'gateway': _gateway_status(),

return data


app.post('apienqueue')
def api_enqueue():
body request.get_json(forceTrue, silentTrue) or
plugins body.get('plugins') or
doc_ids body.get('doc_ids') or
payload body.get('payload')
force body.get('force')
overwrite body.get('overwrite')
if payload is None:
payload
if force is True:
payload.setdefault('force', True)
if overwrite is True:
payload True
cfg load_configs(); ensure_db(cfg)
count 0
for d in doc_ids:
for p in plugins:
enqueue(cfg, p, d, dict(payload))
count 1
return jsonify('enqueued': count)

app.post('apiplan')
def api_plan():
spawn plan_enqueue.py as a subprocess to avoid importpackage issues
args request.get_json(forceTrue, silentTrue) or
cmd
if 'plugins' in args:
cmd )
if args.get('only_missing'):
cmd
if args.get('limit'):
cmd )
if args.get('changed_since_min'):
cmd )
if args.get('summaries_modes'):
cmd )
if args.get('map_reduce'):
cmd
if args.get('doc_ids'):
cmd )
if args.get('payload_json'):
cmd
env os.environ.copy(); env str(pathlib.Path.cwd())
subprocess.run(cmd, envenv)
return jsonify('ok': True)

def _spawn_worker(plugins: list , batch: int, watch: boolTrue):
args
if watch:
args.append('--watch')
env os.environ.copy(); env str(pathlib.Path.cwd())
proc subprocess.Popen(args, stdoutsubprocess.PIPE, stderrsubprocess.STDOUT, textTrue, envenv)
wid next(_WORKER_ID_GEN)
wlog deque(maxlen500)
WORKERS
'proc': proc,
'plugins': plugins,
'batch': batch,
'watch': watch,
'started_at': datetime.datetime.now().isoformat(timespec'seconds'),
'log': wlog,
'current': None,

def _reader():
try:
for line in proc.stdout:
clean line.rstrip()
wlog.append(clean)
_handle_worker_line(wid, clean)
except Exception:
pass
threading.Thread(target_reader, daemonTrue).start()
return wid

app.post('apiworkerstart')
def api_worker_start():
body request.get_json(forceTrue, silentTrue) or
plugins body.get('plugins') or
batch int(body.get('batch') or 32)
wid _spawn_worker(plugins, batch, watchTrue)
return jsonify('started': True, 'worker_id': wid)

app.post('apiworkerstop')
def api_worker_stop():
body request.get_json(forceTrue, silentTrue) or
wid body.get('id') or body.get('worker_id')
if wid is None:
stop all
for wid2 in list(WORKERS.keys()):
_stop_worker_id(wid2)
return jsonify('stopped_all': True)
ok _stop_worker_id(int(wid))
return jsonify('stopped': ok, 'worker_id': wid)


app.post('apigatewaystart')
def api_gateway_start():
body request.get_json(forceTrue, silentTrue) or
service body.get('service')
log_level body.get('log_level', 'INFO')
started _spawn_gateway(serviceservice, log_levellog_level)
status _gateway_status()
status started
return jsonify(status)


app.post('apigatewaystop')
def api_gateway_stop():
stopped _stop_gateway()
status _gateway_status()
status stopped
return jsonify(status)

app.get('apiworkers')
def api_workers():
out
for wid, w in WORKERS.items():
proc w.get('proc')
out.append(
'id': wid,
'pid': proc.pid if proc else None,
'running': (proc is not None and proc.poll() is None),
'plugins': w.get('plugins'),
'batch': w.get('batch'),
'watch': w.get('watch'),
'started_at': w.get('started_at'),
)
return jsonify('workers': out)

app.get('apiqueuelist')
def api_queue_list():
try:
limit int(request.args.get('limit', '200'))
except Exception:
limit 200
status request.args.get('status') optional: pendingrunningdone
plugin request.args.get('plugin') optional
if not DB_PATH.exists():
return jsonify('items': )
con sqlite3.connect(DB_PATH)
con.row_factory sqlite3.Row
cur con.cursor()
q "select id, plugin, doc_id, status, created_at from jobs"
cond
args
if status:
cond.append("status?"); args.append(status)
if plugin:
cond.append("plugin?"); args.append(plugin)
if cond:
q " where " " and ".join(cond)
q " order by id asc limit ?"; args.append(limit)
rows
con.close()
return jsonify('items': rows)


app.get('apiqueuejobint:job_id')
def api_queue_job(job_id: int):
if not DB_PATH.exists():
return jsonify('error': 'queue database not found'), 404
con sqlite3.connect(DB_PATH)
con.row_factory sqlite3.Row
cur con.cursor()
row cur.execute(
"select id, plugin, doc_id, status, payload, retries, last_error, created_at, completed_at from jobs where id?",
(job_id,),
).fetchone()
con.close()
if not row:
return jsonify('error': 'job not found', 'id': job_id), 404
payload row
try:
payload json.loads(payload) if payload else None
except Exception:
payload 'raw': payload
return jsonify(
'id': row ,
'plugin': row ,
'doc_id': row ,
'status': row ,
'payload': payload,
'retries': row ,
'last_error': row ,
'created_at': row ,
'completed_at': row ,
)

app.post('apiqueueclear')
def api_queue_clear():
mode (request.get_json(forceTrue, silentTrue) or ).get('mode') or 'non-done'
app.logger.info(' modes', mode)
if not DB_PATH.exists():
return jsonify('ok': True, 'cleared': 0, 'mode': mode)
con sqlite3.connect(DB_PATH)
cur con.cursor()
cleared 0
try:
if mode 'all':
cur.execute('delete from jobs')
cleared cur.rowcount
elif mode in ('reset-running', 'reset-running-counter'):
cur.execute("update jobs set status'pending', retriescoalesce(retries,0)1 where status'running'")
cleared cur.rowcount
if mode 'reset-running-counter':
from kn.jobs_sqlite import reset_counter
reset_counter(load_configs(), 'llm_concurrency')
elif mode 'pending':
cur.execute("delete from jobs where status'pending'")
cleared cur.rowcount
elif mode 'non-done':
cur.execute("delete from jobs where status!'done'")
cleared cur.rowcount
else:
return jsonify('error': 'unknown mode', 'mode': mode), 400
con.commit()
finally:
con.close()
return jsonify('ok': True, 'cleared': int(cleared), 'mode': mode)


Plugin defaults (persisted to .knowledgeconfigmodels.yml under 'plugins')

Cache management
CACHE_DIR pathlib.Path('.knowledgecachellm')

app.get('apicachellmstats')
def api_cache_stats():
total 0; count 0
if CACHE_DIR.exists():
for p in CACHE_DIR.glob('.json'):
try:
total p.stat().st_size
count 1
except Exception:
continue
return jsonify('count': count, 'total_bytes': total)

app.get('apicachellmlist')
def api_cache_list():
try:
limit int(request.args.get('limit', '200'))
except Exception:
limit 200
items
if CACHE_DIR.exists():
files sorted(CACHE_DIR.glob('.json'), keylambda p: p.stat().st_mtime, reverseTrue)
for p in files :
try:
st p.stat()
items.append('name': p.name, 'size': st.st_size, 'mtime': dt.datetime.fromtimestamp(st.st_mtime).isoformat(sep' '))
except Exception:
continue
return jsonify('items': items)

app.post('apicachellmclear')
def api_cache_clear():
cleared 0
if CACHE_DIR.exists():
for p in CACHE_DIR.glob('.json'):
try:
p.unlink(); cleared 1
except Exception:
continue
return jsonify('ok': True, 'cleared': cleared)


--------------
Document attribute browsing APIs
--------------

def _first_chunk_for_doc(doc_id: str):
for p in CHUNKS_DIR.glob(f"doc_id-.json"):
try:
return json.loads(p.read_text(encoding'utf-8'))
except Exception:
continue
return None

app.get('apidocs')
def api_docs():
docs
total len(DOC_ATTR_PLUGINS)
for d in sorted(_unique_doc_ids()):
rec _first_chunk_for_doc(d) or
meta rec.get('meta') or
attrs _attr_paths_for_doc(d)
have sum(1 for plugin in DOC_ATTR_PLUGINS if isinstance(attrs.get(plugin), dict))
docs.append(
'doc_id': d,
'domain': meta.get('domain','root'),
'path': meta.get('path',''),
'coverage': 'have': have, 'total': total
)
return jsonify('docs': docs, 'coverage_total': total, 'runnable_plugins': DOC_ATTR_PLUGINS)

app.post('apiingest')
def api_ingest():
body request.get_json(forceTrue, silentTrue) or
repo body.get('repo') or body.get('path')
full bool(body.get('full', True))
if not repo:
return jsonify('error':'missing repo path'), 400
spawn ingest_build_graph.py
env os.environ.copy(); env str(pathlib.Path.cwd())
args
if full:
args.append('--full')
subprocess.Popen(args, envenv)
return jsonify('ok': True, 'started': True, 'repo': repo, 'full': full)

def _attr_paths_for_doc(doc_id: str):
out
if ATTR_DIR.exists():
for plugin_dir in ATTR_DIR.iterdir():
if not plugin_dir.is_dir():
continue
if plugin_dir.name 'summaries':
modes
for f in plugin_dir.glob(f"doc_id_.json"):
try:
mode f.stem.split('_', 1)
except Exception:
continue
try:
mt f.stat().st_mtime
except Exception:
mt None
modes.append('mode': mode, 'path': str(f), 'mtime': mt)
if modes:
out 'modes': modes, 'runnable': False
else:
f plugin_dir f"doc_id.json"
if f.exists():
try:
mt f.stat().st_mtime
except Exception:
mt None
out
'path': str(f),
'mtime': mt,
'runnable': plugin_dir.name in DOC_ATTR_PLUGINS,

return out

app.get('apidocdoc_id')
def api_doc(doc_id):
rec _first_chunk_for_doc(doc_id) or
meta rec.get('meta') or
attrs _attr_paths_for_doc(doc_id)
return jsonify('doc_id': doc_id, 'meta': meta, 'attributes': attrs)

app.get('apidocdoc_idattrplugin')
def api_doc_attr(doc_id, plugin):
if plugin 'summaries':
pdir ATTR_DIR 'summaries'
items
for f in sorted(pdir.glob(f"doc_id_.json")):
try:
data json.loads(f.read_text(encoding'utf-8'))
items.append(data)
except Exception:
continue
return jsonify('items': items)
p ATTR_DIR plugin f"doc_id.json"
if not p.exists():
return jsonify('error':'not found'), 404
try:
data json.loads(p.read_text(encoding'utf-8'))
except Exception:
data 'raw': p.read_text(errors'ignore')
return jsonify('item': data)

Chunk-level attributes per document
def _list_chunk_attrs_for_doc(doc_id: str, plugin: str):
items
pdir ATTR_DIR plugin
if not pdir.exists():
return
for f in sorted(pdir.glob('.json')):
try:
data json.loads(f.read_text(encoding'utf-8'))
except Exception:
continue
if data.get('doc_id') ! doc_id:
continue
chunk_id data.get('chunk_id')
seq None
try:
if chunk_id:
cpath CHUNKS_DIR f"chunk_id.json"
if cpath.exists():
crec json.loads(cpath.read_text(encoding'utf-8'))
seq ((crec.get('meta') or ).get('seq'))
except Exception:
pass
val data.get('value')
preview None
if isinstance(val, str):
preview val.strip().replace('n',' ')
items.append('chunk_id': chunk_id, 'seq': seq, 'path': str(f), 'preview': preview)
order by seq when available, else by chunk_id
def _key(it):
if isinstance(it.get('seq'), int):
return (0, it )
return (1, str(it.get('chunk_id') or ''))
items.sort(key_key)
return items

app.get('apidocdoc_idchunksattrplugin')
def api_doc_chunk_attrs(doc_id, plugin):
items _list_chunk_attrs_for_doc(doc_id, plugin)
return jsonify('doc_id': doc_id, 'plugin': plugin, 'items': items)

Serve existing report (if present) under report
REPORT_DIR ROOT 'exports' 'reports'

app.get('report')
def report_index():
idx REPORT_DIR 'index.html'
if not idx.exists():
return jsonify('error': 'report not found', 'path': str(idx)), 404
return send_from_directory(str(REPORT_DIR), 'index.html')

app.get('reportpath:path')
def report_static(path):
if not REPORT_DIR.exists():
return jsonify('error': 'report dir not found'), 404
return send_from_directory(str(REPORT_DIR), path)

Attribute-wise listing across documents
app.get('apiattrplugindocs')
def api_attr_docs(plugin):
pdir ATTR_DIR plugin
present
if pdir.exists() and pdir.is_dir():
for f in sorted(pdir.glob('.json')):
try:
data json.loads(f.read_text(encoding'utf-8'))
doc_id data.get('doc_id') or f.stem
val data.get('value')
preview None
if isinstance(val, str):
preview val.strip().replace('n',' ')
present.append('doc_id': doc_id, 'path': str(f), 'preview': preview)
except Exception:
doc_id f.stem
present.append('doc_id': doc_id, 'path': str(f))
all_docs _unique_doc_ids()
have i for i in present
missing sorted( )
return jsonify('plugin': plugin, 'present': present, 'missing': missing)

def _stop_worker_id(wid: int) - bool:
w WORKERS.get(wid)
if not w:
return False
proc w.get('proc')
if proc and (proc.poll() is None):
try:
proc.terminate()
except Exception:
pass
WORKERS.pop(wid, None)
return True

app.get('apistatus')
def api_status():
force_models bool(request.args.get('force_models'))
data _read_jobs_status()
data _gateway_status()
workers
for wid, w in WORKERS.items():
proc w.get('proc')
workers.append(
'id': wid,
'pid': proc.pid if proc else None,
'running': (proc is not None and proc.poll() is None),
'plugins': w.get('plugins'),
'batch': w.get('batch'),
'watch': w.get('watch'),
'started_at': w.get('started_at'),
'log_tail': list(w.get('log', deque())) ,
'current': w.get('current'),
)
data workers
if workers:
data workers
try:
cfg load_configs()
data cfg.get('plugins', )
except Exception:
data
data _llm_health(forceforce_models)
return jsonify(data)

app.get('')
def root():
index_path pathlib.Path(app.static_folder) 'index.html'
if not index_path.exists():
return jsonify('error':'index not found', 'path': str(index_path)), 500
return send_from_directory(app.static_folder, 'index.html')

app.get('path:path')
def static_proxy(path):
return send_from_directory(app.static_folder, path)

if __name__ '__main__':
app.run(host'0.0.0.0', port5051, debugFalse, threadedTrue)



.bindashboard_staticindex.html
!doctype html
html lang"en"
head
meta charset"utf-8"
meta name"viewport" content"widthdevice-width, initial-scale1"
titleGraphRAG Dashboardtitle
style
:root
--bg: 0b111b;
--header-bg: 0f172a;
--panel: 161f2e;
--panel-alt: 1e2737;
--border: 263342;
--text: e6edf7;
--subtle: 94a3b8;
--accent: 3b82f6;
--accent-strong: 60a5fa;
--danger: f87171;
--success: 34d399;
--input-bg: 0f172a;
--input-border: 1f2937;
--table-row: 101827;
--table-row-alt: 141d2c;

box-sizing: border-box;
body font-family: system-ui, sans-serif; margin: 0; padding: 0; background: var(--bg); color: var(--text);
header padding: 12px 16px; background: var(--header-bg); color: var(--text); display: flex; align-items: center; gap: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.3);
header h1 margin: 0; font-size: 16px; font-weight: 600;
main padding: 16px; display: grid; grid-template-columns: 320px 1fr; gap: 16px;
.panel border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: var(--panel); box-shadow: 0 2px 12px rgba(0,0,0,0.25);
.row display: flex; gap: 8px; align-items: center; flex-wrap: wrap;
.grid display: grid; grid-template-columns: repeat(auto-fit, minmax(160px, 1fr)); gap: 8px;
.btn padding: 6px 10px; border: 1px solid var(--input-border); border-radius: 6px; background: var(--input-bg); color: var(--text); cursor: pointer; transition: background 0.15s ease, border-color 0.15s ease;
.btn.small padding: 4px 6px; font-size: 12px;
.btn:hover background: var(--panel-alt); border-color: var(--accent);
.btn.primary background: var(--accent); color: 0b1220; border-color: var(--accent);
.btn.primary:hover background: var(--accent-strong); border-color: var(--accent-strong);
table width: 100; border-collapse: collapse;
th, td padding: 6px 8px; border-bottom: 1px solid rgba(148, 163, 184, 0.2); text-align: left; font-size: 13px;
thead th color: var(--subtle); font-weight: 500; text-transform: uppercase; letter-spacing: 0.02em; background: rgba(15, 23, 42, 0.6);
tbody tr background: var(--table-row);
tbody tr:nth-child(2n) background: var(--table-row-alt);
h3 margin: 0 0 8px 0; font-size: 14px; color: var(--text);
small color: var(--subtle);
ul margin: 6px 0 0 18px;
.badge background: rgba(59, 130, 246, 0.2); color: var(--accent-strong); padding:2px 6px; border-radius:10px; font-size:11px;
.mono font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
.ok color: var(--success);
.err color: var(--danger);
input, select, textarea background: var(--input-bg); color: var(--text); border: 1px solid var(--input-border); border-radius: 6px; padding: 4px 6px;
input::placeholder, textarea::placeholder color: rgba(148, 163, 184, 0.6);
details background: var(--panel-alt); border-radius: 8px; border: 1px solid var(--border); padding: 8px;
details summary cursor: pointer; color: var(--text);
pre background: var(--panel-alt); color: var(--text); border-radius: 6px; padding: 8px;
select min-height: 28px;
option background: var(--input-bg); color: var(--text);
select option , select option:checked
background-color: rgba(148, 163, 184, 0.28) !important;
color: var(--text) !important;

select:focus option , select:focus option:checked
background-color: rgba(59, 130, 246, 0.45) !important;
color: var(--text) !important;

a color: var(--accent-strong);
a:hover text-decoration: underline;
style
head
body
header
h1GraphRAG Dashboardh1
button class"btn" onclick"refresh()"Refreshbutton
header
main
section class"panel" id"controls"
h3Plannerh3
div class"row" style"margin-bottom:4px; align-items:center;"
labelPluginslabel
span style"flex:1"span
div class"row" style"gap:4px;"
button class"btn" type"button" onclick"multiSelectAll('plugins')"Allbutton
button class"btn" type"button" onclick"multiSelectNone('plugins')"Nonebutton
div
div
div class"row" style"margin-bottom:8px;"
select id"plugins" multiple size"10" style"width:100"
option value"multi-basic"multi-basic (batch)option
option value"doc-skeleton"doc-skeletonoption
option value"summary-20w"summary-20woption
option value"summary-short"summary-shortoption
option value"summary-medium"summary-mediumoption
option value"summary-long"summary-longoption
option value"summary-outline"summary-outlineoption
option value"topic-tags"topic-tagsoption
option value"keyphrases"keyphrasesoption
option value"glossary"glossaryoption
option value"requirements"requirementsoption
option value"todo-items"todo-itemsoption
option value"faq-pairs"faq-pairsoption
option value"bridge-candidates"bridge-candidatesoption
option value"risk-scan"risk-scanoption
option value"recent-summary"recent-summaryoption
option value"pii-scan"pii-scanoption
select
div
div class"row" style"margin-bottom:4px; align-items:center;"
labelDocslabel
span style"flex:1"span
div class"row" style"gap:4px;"
button class"btn" type"button" onclick"multiSelectAll('planner_docs')"Allbutton
button class"btn" type"button" onclick"multiSelectNone('planner_docs')"Nonebutton
div
div
div class"row" style"margin-bottom:8px;"
select id"planner_docs" multiple size"10" style"width:100"select
div
div class"row" style"margin-bottom:8px;"
labelPayload JSONlabel
input id"payload_json" placeholder'"llm":"model":"qwen2.5-7b-instruct","timeout":60' style"flex:1"
div
div class"row" style"margin-bottom:8px;"
labelChanged since (min)label
input id"changed_min" type"number" min"0" value"0" style"width:100px"
labelLimitlabel
input id"limit" type"number" min"0" value"0" style"width:100px"
labelinput id"map_reduce" type"checkbox" map-reducelabel
div
div class"row"
labelinput id"only_missing" type"checkbox" checked only-missinglabel
button class"btn primary" onclick"planEnqueue()"Plan Enqueuebutton
div

hr
h3Ingesth3
div class"row" style"margin-bottom:8px;"
input id"ingest_path" placeholder"repo path (e.g., G:LOKIpapers)" style"flex:1"
labelinput id"ingest_full" type"checkbox" checked fulllabel
div
div class"row"
button class"btn" onclick"startIngest()"Start Ingestbutton
div
hr
h3Direct Enqueueh3
div class"row" style"margin-bottom:8px;"
input id"doc_ids" placeholder"doc ids comma-separated (leave empty all)" style"flex:1"
div
div class"row"
button class"btn" onclick"enqueueSelected()"Enqueue for selected pluginsbutton
div
hr
h3Workerh3
div class"row" style"margin-bottom:8px;"
input id"worker_plugins" value"" style"flex:1"
labelbatchlabel
input id"worker_batch" type"number" value"8" style"width:90px"
labelmax-inflightlabel
input id"worker_max_inflight" type"number" value"2" style"width:90px"
div
div class"row"
button class"btn" onclick"startWorker()"Start Workerbutton
button class"btn" onclick"stopWorkerAll()"Stop Allbutton
button class"btn" onclick"testWorker()"Test Connectionbutton
div
hr
h3Gatewayh3
div class"row" style"margin-bottom:8px;"
input id"gateway_service" placeholder"service (default lmstudio)" style"flex:1"
labellog levellabel
input id"gateway_loglevel" value"INFO" style"width:90px"
div
div class"row"
button class"btn" onclick"startGateway()"Start Gatewaybutton
button class"btn" onclick"stopGateway()"Stop Gatewaybutton
div
pre id"gateway_status" class"mono" style"margin-top:8px;background:var(--panel-alt);color:var(--text);padding:8px;border-radius:6px;max-height:160px;overflow:auto;border:1px solid var(--border);"pre
pre id"worker_test_result" class"mono" style"margin-top:8px;background:var(--panel-alt);color:var(--text);padding:8px;border-radius:6px;max-height:160px;overflow:auto;border:1px solid var(--border);"pre
div id"workers_list" style"margin-top:8px;"div
section

section class"panel" id"status"
h3Statush3
div id"llm"div
div class"grid" id"queue"div
details style"margin:8px 0;"
summarybPlugin defaultsb small(per-plugin modeltimeout)smallsummary
div class"row" style"margin:6px 0;"
button class"btn" onclick"loadPluginDefaults()"Loadbutton
spansmallUse JSON: "summary-short": "llm": "model": "qwen2.5-7b-instruct", "timeout": 60 smallspan
div
textarea id"plugin_defaults_json" style"width:100;height:120px"textarea
div class"row" style"margin-top:6px;"
button class"btn" onclick"savePluginDefaults()"Savebutton
div
details
details style"margin:8px 0;"
summarybQueue listingb small(ordered by id asc)smallsummary
div class"row" style"margin:6px 0;"
labelStatuslabel
select id"queue_status"option value""(any)optionoptionpendingoptionoptionrunningoptionoptiondoneoptionselect
labelPluginlabel
input id"queue_plugin" placeholder"(any)"
labelLimitlabel
input id"queue_limit" type"number" value"200" style"width:90px"
button class"btn" onclick"loadQueueList()"Loadbutton
span style"flex:1"span
labelClearlabel
select id"queue_clear_mode"option value"non-done"non-doneoptionoption value"pending"pendingoptionoption value"reset-running"reset runningpendingoptionoption value"reset-running-counter"reset runningpending reset counteroptionoption value"all"alloptionselect
button class"btn" onclick"clearQueue()"Clear Queuebutton
div
div id"queue_list" style"max-height:240px; overflow:auto; border:1px solid var(--border); border-radius:6px; padding:6px; background:var(--panel-alt);"div
details
details style"margin:8px 0;"
summarybLLM cacheb small(prompt-level)smallsummary
div class"row" style"margin:6px 0;"
button class"btn" onclick"loadCacheStats()"Refreshbutton
div id"cache_stats" style"margin-left:8px"div
span style"flex:1"span
button class"btn" onclick"clearCache()"Clear cachebutton
div
div class"row" style"margin:6px 0;"
labelShowlabel
input id"cache_limit" type"number" value"200" style"width:90px"
button class"btn" onclick"loadCacheList()"Listbutton
div
div id"cache_list" style"max-height:240px; overflow:auto; border:1px solid var(--border); border-radius:6px; padding:6px; background:var(--panel-alt);"div
details
details style"margin:8px 0;"
summarybFailed Jobsbsummary
button class"btn" onclick"loadFailedJobs()"Refresh Failedbutton
div id"failed_jobs" style"margin-top:8px; max-height:200px; overflow:auto; border:1px solid var(--border); border-radius:6px; padding:6px; background:var(--panel-alt);"div
details
div class"row" style"justify-content:space-between;align-items:center;"
h3Attributes Coverageh3
divlabelinput id"auto_refresh" type"checkbox" checked auto-refreshlabel
select id"refresh_sec"option3optionoption selected5optionoption10optionoption30optionselectdiv
div
div id"coverage_overall"div
div id"coverage"div
h3 style"margin-top:16px;"Domainsh3
div id"domains"div
h3 style"margin-top:16px;"Worker Log (tail)h3
pre id"worker_log" style"background:var(--panel-alt);color:var(--text);padding:8px;border-radius:6px;max-height:200px;overflow:auto;border:1px solid var(--border);"pre

h3 style"margin-top:16px;"Documentsh3
div class"row" style"margin-bottom:8px;"
input id"doc_filter" placeholder"filter by doc_id or domain" oninput"renderDocs()" style"flex:1"
div
div id"docs"div
section
main

script
coverage open state persists across refreshes
window.__COV_OPEN__ window.__COV_OPEN__ new Set();
window.__COV_DOC_OPEN__ window.__COV_DOC_OPEN__ ; plugin - Set(doc_id)
window.__COV_DATA__ window.__COV_DATA__ ; plugin - present, missing
window.__ATTR_ITEM_CACHE__ window.__ATTR_ITEM_CACHE__ ; key: plugindoc - json
window.__PLANNER_SELECTED_PLUGINS__ window.__PLANNER_SELECTED_PLUGINS__ ;
window.__PLANNER_SELECTED_DOCS__ window.__PLANNER_SELECTED_DOCS__ ;
window.__RUNNABLE_PLUGINS__ window.__RUNNABLE_PLUGINS__ ;
window.__DOCS_COVERAGE_TOTAL__ window.__DOCS_COVERAGE_TOTAL__ 0;

async function refresh()
const r await fetch('apistatus');
const data await r.json();
render(data);
populatePlannerDocs();
ensurePlannerBindings();
restorePlannerSelections();

function selValues(id)
const s document.getElementById(id);
return Array.from(s.selectedOptions).map(oo.value);

function rememberSelection(id, key)
const sel document.getElementById(id);
if(!sel) return;
window Array.from(sel.selectedOptions).map(oo.value);

function applySelection(id, key)
const sel document.getElementById(id);
if(!sel) return;
const desired new Set(window );
Array.from(sel.options).forEach(opt opt.selected desired.has(opt.value); );

function restorePlannerSelections()
applySelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
applySelection('planner_docs', '__PLANNER_SELECTED_DOCS__');

function populatePlannerDocs()
const select document.getElementById('planner_docs');
if(!select) return;
const docs window.__DOCS__ ;
const scroll select.scrollTop;
select.innerHTML '';
docs.slice().sort((a,b)a.doc_id.localeCompare(b.doc_id)).forEach(doc
const opt document.createElement('option');
opt.value doc.doc_id;
const domain doc.domain ? (doc.domain) : '';
opt.textContent doc.doc_iddomain;
select.appendChild(opt);
);
applySelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
select.scrollTop scroll;
rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');

function multiSelectAll(id)
const sel document.getElementById(id);
if(!sel) return;
Array.from(sel.options).forEach(opt opt.selected true; );
rememberSelection(id, id 'plugins' ? '__PLANNER_SELECTED_PLUGINS__' : '__PLANNER_SELECTED_DOCS__');
restorePlannerSelections();

function multiSelectNone(id)
const sel document.getElementById(id);
if(!sel) return;
Array.from(sel.options).forEach(opt opt.selected false; );
rememberSelection(id, id 'plugins' ? '__PLANNER_SELECTED_PLUGINS__' : '__PLANNER_SELECTED_DOCS__');
restorePlannerSelections();

function ensurePlannerBindings()
const pluginSel document.getElementById('plugins');
if(pluginSel pluginSel.dataset.plannerBound ! '1')
const handler ()rememberSelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
pluginSel.addEventListener('change', handler);
pluginSel.addEventListener('input', handler);
pluginSel.addEventListener('mousedown', evttoggleOptionSelection(evt, handler));
pluginSel.dataset.plannerBound '1';
handler();

const docSel document.getElementById('planner_docs');
if(docSel docSel.dataset.plannerBound ! '1')
const handler ()rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
docSel.addEventListener('change', handler);
docSel.addEventListener('input', handler);
docSel.addEventListener('mousedown', evttoggleOptionSelection(evt, handler));
docSel.dataset.plannerBound '1';
handler();


function toggleOptionSelection(evt, handler)
const opt evt.target;
if(!opt opt.tagName ! 'OPTION') return;
evt.preventDefault();
const sel opt.parentElement;
const willSelect !opt.selected;
opt.selected willSelect;
if(!opt.selected)
keep focus on select to allow repeated toggles
sel.focus();

handler();

async function planEnqueue()
rememberSelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
const plugins selValues('plugins');
const doc_ids selValues('planner_docs');
const payload
plugins,
only_missing: document.getElementById('only_missing').checked,
changed_since_min: parseInt(document.getElementById('changed_min').value'0'),
limit: parseInt(document.getElementById('limit').value'0'),
map_reduce: document.getElementById('map_reduce').checked
;
const pjson document.getElementById('payload_json').value'';
if(pjson) payload.payload_json pjson;
if(doc_ids.length) payload.doc_ids doc_ids;
await fetch('apiplan', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(payload));
await refresh();

async function enqueueSelected()
rememberSelection('plugins', '__PLANNER_SELECTED_PLUGINS__');
rememberSelection('planner_docs', '__PLANNER_SELECTED_DOCS__');
const plugins selValues('plugins');
let doc_ids selValues('planner_docs');
const manual (document.getElementById('doc_ids').value'').split(',').map(ss.trim()).filter(Boolean);
doc_ids Array.from(new Set( ));
if(doc_ids.length 0)
alert('Select docs from the Docs list or provide doc_ids (comma-separated).');
return;

await fetch('apienqueue', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(plugins, doc_ids));
await refresh();

async function startIngest()
const repo document.getElementById('ingest_path').value'';
const full document.getElementById('ingest_full').checked;
if(!repo) alert('Enter a repo path'); return;
await fetch('apiingest', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(repo, full));

async function startWorker()
const plugins (document.getElementById('worker_plugins').value'').split(',').map(ss.trim()).filter(Boolean);
const batch parseInt(document.getElementById('worker_batch').value'8');
const max_inflight parseInt(document.getElementById('worker_max_inflight').value'2');
await fetch('apiworkerstart', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(plugins, batch, max_inflight));
await refresh();

async function stopWorkerAll()
await fetch('apiworkerstop', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify());
await refresh();

async function startGateway()
const service (document.getElementById('gateway_service').value'').trim();
const logLevel (document.getElementById('gateway_loglevel').value'INFO').trim() 'INFO';
const body log_level: logLevel ;
if(service) body.service service;
const r await fetch('apigatewaystart', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(body));
const data await r.json();
document.getElementById('gateway_status').textContent JSON.stringify(data, null, 2);
await refresh();

async function stopGateway()
const r await fetch('apigatewaystop', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify());
const data await r.json();
document.getElementById('gateway_status').textContent JSON.stringify(data, null, 2);
await refresh();

async function testWorker()
const out document.getElementById('worker_test_result');
try
const r await fetch('apistatus?force_models1');
const txt await r.text();
out.textContent HTTP r.statusntxt;
console.log(' ', r.status, txt);
catch(err)
out.textContent Request failed: err;
console.error(' error', err);


function render(data)
const llm document.getElementById('llm');
llm.innerHTML divLLM: span class"data.llm.reachable?'ok':'err'"data.llm.reachable?'reachable':'unreachable'span span class"mono"data.llm.endpointspandiv
(data.llm.models data.llm.models.length? divsmallmodels: data.llm.models.join(', ')smalldiv: '');
if(data.llm.error) llm.innerHTML div class"err"smalldata.llm.errorsmalldiv;

const gatewayEl document.getElementById('gateway_status');
if(gatewayEl data.gateway)
gatewayEl.textContent JSON.stringify(data.gateway, null, 2);


const q data.queue;
const qdiv document.getElementById('queue');
const rows ;
const thr q.throughput;
const eta q.eta;
const overallEta eta.overall_sec ! null ? , ETA: fmtDur(eta.overall_sec) : '';
rows.push(div class"panel"divbTotalb: q.total0overallEtadivdivby_status: JSON.stringify(q.by_status)divdivoldest_pending_min: q.oldest_pending_min??'na'divdivsmallthroughput: overall thr.overall_dps0s over thr.window_min0msmalldivdiv);
const bp q.by_plugin_status;
const dur q.durations;
Object.keys(bp).sort().forEach(k
const d dur ;
const est d? , avg d.avg_ss over d.n : '';
let etaPlug '';
if(eta.plugin_sec eta.plugin_sec ! null) etaPlug , ETA fmtDur(eta.plugin_sec );
let dps '';
if(thr.plugin_dps thr.plugin_dps ! null) dps , thr.plugin_dps s;
rows.push(div class"panel"divbkbestdpsetaPlugdivdivsmallJSON.stringify(bp )smalldivdiv);
);
qdiv.innerHTML rows.join('');
refresh queue list silently if already populated
if(document.getElementById('queue_list').getAttribute('data-has')'1')
loadQueueList(true);

auto refresh plugin defaults snapshot
(non-intrusive; we only auto-load once)
if(!window.__PLUGINS_LOADED__) loadPluginDefaults(); window.__PLUGINS_LOADED__true;

renderCoverage(data);

const domDiv document.getElementById('domains');
const d data.domains ;
domDiv.innerHTML 'ul' d.map(xlix.domain span class"badge"x.docsspanli).join('') 'ul';

const w data.worker;
const log (w.log_tail ).join('n');
const statusPrefix w.running ? ' n' : ' n';
document.getElementById('worker_log').textContent statusPrefix log;
renderWorkers(data.workers );

window.__STATUS__ data;
if(!window.__DOCS__) loadDocs();

refresh();
setInterval(()
const on document.getElementById('auto_refresh').checked;
const sec parseInt(document.getElementById('refresh_sec').value'5');
if(on) refresh();
, 5000);
function fmtDur(sec)
sec Math.max(0, Math.round(sec));
const h Math.floor(sec3600); const m Math.floor((sec3600)60); const s sec60;
return (h0? h'h ': '') (m0? m'm ': '') s 's';

function renderWorkers(list)
const root document.getElementById('workers_list');
if(!list !list.length) root.innerHTML 'smallNo workerssmall'; return;
root.innerHTML 'div style"overflow-x:auto;"'
'table style"min-width:820px;"theadtrthCurrentththIDththPIDththPluginsththBatchththStartedththStatusthththtrtheadtbody'
list.map(wtrtdsmall class"mono" style"white-space:pre-wrap;"w.current?fmtCurrent(w.current):''smalltdtdw.idtdtdw.pid''tdtdsmall(w.plugins ).join(', ')smalltdtdw.batch''tdtdsmallw.started_at''smalltdtdw.running?'running':'stopped'tdtdbutton classbtn onclick"stopWorker(w.id)"Stopbuttontdtr).join('')
'tbodytablediv';

function fmtCurrent(cur)
if(!cur) return '';
const parts ;
if(cur.plugin) parts.push(plugincur.plugin);
if(cur.docs)
const docs cur.docs.length 80 ? cur.docs.slice(0,77) '...' : cur.docs;
parts.push(docsdocs);

if(cur.since) parts.push(since cur.since);
return parts.map(p p).join('n');

async function stopWorker(id)
await fetch('apiworkerstop', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(id));
await refresh();

async function loadQueueList(silent)
const status document.getElementById('queue_status').value;
const plugin document.getElementById('queue_plugin').value;
const limit parseInt(document.getElementById('queue_limit').value'200');
const params new URLSearchParams();
if(status) params.set('status', status);
if(plugin) params.set('plugin', plugin);
if(limit) params.set('limit', String(limit));
const r await fetch('apiqueuelist?' params.toString());
console.debug(' status', r.status);
const data await r.json();
const items data.items ;
const root document.getElementById('queue_list');
root.setAttribute('data-has','1');
root.innerHTML 'tabletheadtrthIDththStatusththPluginththDocththCreatedthtrtheadtbody'
items.map(itrtdi.idtdtdi.statustdtdi.plugintdtdi.doc_idtdtdi.created_at''tdtr).join('')
'tbodytable';

async function clearQueue()
const mode document.getElementById('queue_clear_mode').value'non-done';
console.debug(' mode', mode);
await fetch('apiqueueclear', method:'POST', headers:'Content-Type':'applicationjson', body: JSON.stringify(mode));
await loadQueueList(true);
await refresh();

async function loadPluginDefaults()
document.getElementById('plugin_defaults_json').value 'Not supported on this build.';

async function savePluginDefaults()
alert('Saving plugin defaults is disabled in this build.');

async function loadCacheStats()
const r await fetch('apicachellmstats');
const data await r.json();
document.getElementById('cache_stats').innerText entries: data.count0, size: data.total_bytes0 bytes;

async function loadCacheList()
const limit parseInt(document.getElementById('cache_limit').value'200');
const r await fetch('apicachellmlist?limit' limit);
const data await r.json();
const items data.items ;
document.getElementById('cache_list').innerHTML 'tabletheadtrthNameththSizeththMTimethtrtheadtbody'
items.map(itrtdi.nametdtdi.sizetdtdi.mtimetdtr).join('') 'tbodytable';

async function clearCache()
await fetch('apicachellmclear', method:'POST');
await loadCacheStats();
document.getElementById('cache_list').innerHTML'';

function renderCoverage(data)
const cov data.attributes_coverage;
const covDiv document.getElementById('coverage');
const bp2 (data.queue).by_plugin_status;
const dur2 (data.queue).durations;
const thr2 (data.queue).throughput;
const eta2 (data.queue).eta;
const overallEtaTxt (eta2 eta2.overall_sec!null) ? Overall ETA: fmtDur(eta2.overall_sec) : '';
document.getElementById('coverage_overall').innerHTML overallEtaTxt? smalloverallEtaTxtsmall : '';
const rows ;
const plugins Object.keys(cov).sort();
for(const k of plugins)
const v cov ;
const st bp2 ; const d dur2 ;
const dps (thr2.plugin_dps) ;
const etaS (eta2.plugin_sec) ;
const isOpen window.__COV_OPEN__.has(k);
rows.push(tr onclick"toggleCov('k')" style"cursor:pointer;"
td style"width:18px;"span id"cov_arrow_k"isOpen?'':''spantd
tdktd
tdv.havev.total_docstd
tdv.pcttd
tdst.pending0td
tdst.running0td
tdst.failed0td
tdst.failed0td
tdd? d.avg_s: ''td
tddps!null? dps: ''td
tdetaS!null? fmtDur(etaS): ''td
td(v.missing_examples ).join(', ')td
tr);
doc rows
if(isOpen)
const pdata window.__COV_DATA__ ;
const present (pdata pdata.present) ? pdata.present : ;
for(const item of present)
const doc_id item.doc_id; const prev item.preview'';
const docOpen (window.__COV_DOC_OPEN__ new Set()).has(doc_id);
rows.push(tr onclick"toggleCovDoc(event,'k','doc_id')" style"cursor:pointer;"
tdtd
td colspan2span id"cov_doc_arrow_k_doc_id"docOpen?'':''span bdoc_idbtd
td colspan8 style"color:555;"escapeHtml(prev)td
tr);
const key kdoc_id;
const hasFull window.__ATTR_ITEM_CACHE__ ! null;
rows.push(tr id"cov_full_k_doc_id" style"display:docOpen?'':'none';"tdtdtd colspan10pre style"white-space:pre-wrap;background:var(--panel-alt);padding:8px;border-radius:6px;border:1px solid var(--border);" id"cov_pre_k_doc_id"hasFull? escapeHtml(JSON.stringify(window.__ATTR_ITEM_CACHE__ , null, 2)) : ''pretdtr);



rows.push('tbodytable');
covDiv.innerHTML rows.join('');

async function toggleCov(plugin)
if(window.__COV_OPEN__.has(plugin))
window.__COV_OPEN__.delete(plugin);
renderCoverage(window.__STATUS__);
return;

open
window.__COV_OPEN__.add(plugin);
if(!window.__COV_DATA__ )
const r await fetch(apiattrplugindocs);
const data await r.json();
window.__COV_DATA__ data;

renderCoverage(window.__STATUS__);

async function toggleCovDoc(evt, plugin, doc_id)
evt.stopPropagation();
const set (window.__COV_DOC_OPEN__ window.__COV_DOC_OPEN__ new Set());
const key plugindoc_id;
if(set.has(doc_id))
set.delete(doc_id);
const row document.getElementById(cov_full_plugin_doc_id);
const arrow document.getElementById(cov_doc_arrow_plugin_doc_id);
if(row) row.style.display 'none';
if(arrow) arrow.textContent '';
return;

set.add(doc_id);
ensure content
if(window.__ATTR_ITEM_CACHE__ null)
const r await fetch(apidocdoc_idattrplugin);
const data await r.json();
window.__ATTR_ITEM_CACHE__ (data.itemdata);

const pre document.getElementById(cov_pre_plugin_doc_id);
const row document.getElementById(cov_full_plugin_doc_id);
const arrow document.getElementById(cov_doc_arrow_plugin_doc_id);
if(pre) pre.textContent JSON.stringify(window.__ATTR_ITEM_CACHE__ , null, 2);
if(row) row.style.display '';
if(arrow) arrow.textContent '';

function escapeHtml(s)
return String(s).replace( g, c("":"","":"","":"",""":""","'":"'" ));

script
script
persist open state for docs and per-doc attributes
window.__DOC_OPEN__ window.__DOC_OPEN__ new Set();
window.__DOC_ATTR_OPEN__ window.__DOC_ATTR_OPEN__ ; doc_id - Set(plugin)
window.__DOC_META__ window.__DOC_META__ ; doc_id - meta, attributes
window.__DOC_CHUNK_ATTR_OPEN__ window.__DOC_CHUNK_ATTR_OPEN__ ; doc_id - Set(chunk-plugin)
window.__CHUNK_ATTR_CACHE__ window.__CHUNK_ATTR_CACHE__ ; key: chunksplugindoc_id - items

async function loadDocs()
const r await fetch('apidocs');
const data await r.json();
window.__DOCS__ data.docs ;
window.__DOCS_COVERAGE_TOTAL__ data.coverage_total 0;
window.__RUNNABLE_PLUGINS__ data.runnable_plugins window.__RUNNABLE_PLUGINS__;
renderDocs();
populatePlannerDocs();
restorePlannerSelections();

function renderDocs()
const docs (window.__DOCS__ ).slice();
const q (document.getElementById('doc_filter').value'').toLowerCase();
const root document.getElementById('docs');
const total window.__DOCS_COVERAGE_TOTAL__ (window.__RUNNABLE_PLUGINS__ ).length;
const rows ;
docs.filter(d!q d.doc_id.toLowerCase().includes(q) (d.domain'').toLowerCase().includes(q))
.forEach(d
const open window.__DOC_OPEN__.has(d.doc_id);
const have d.coverage ? (d.coverage.have0) : 0;
const tot d.coverage ? (d.coverage.totaltotal) : total;
const pct tot ? Math.round((havetot)100) : 0;
rows.push(tr onclick"toggleDoc('d.doc_id')" style"cursor:pointer;"
td style"width:18px;"span id"doc_arrow_d.doc_id"open?'':''spantd
tdbd.doc_idbtd
tdsmallpct (havetot)smalltd
tdsmalld.domain''smalltd
tdspan classmonod.path''spantd
tr);
if(open)
const meta window.__DOC_META__ ;
const attrs meta meta.attributes ? meta.attributes : ;
const plugs Object.keys(attrs).sort();
rows.push(trtdtdtd colspan4div class"row" style"gap:6px;"
button class"btn small" onclick"event.stopPropagation(); enqueueDocAll('d.doc_id', false);"Allbutton
button class"btn small" onclick"event.stopPropagation(); enqueueDocAll('d.doc_id', true);"All (Force)button
divtdtr);
for(const plug of plugs)
const attrOpen (window.__DOC_ATTR_OPEN__ new Set()).has(plug);
const metaInfo attrs ;
const runnable metaInfo.runnable ! false;
const when metaInfo.mtime ? new Date(metaInfo.mtime 1000).toLocaleString() : '';
rows.push(tr onclick"toggleDocAttr(event,'d.doc_id','plug')" style"cursor:pointer;"
tdtd
td colspan2span id"doc_attr_arrow_d.doc_id_plug"attrOpen?'':''span plugwhen? small style'opacity:.6;'(when)small:''td
td colspan2div class"row" style"justify-content:flex-end; gap:4px;"
button class"btn small" runnable?'':'disabled' onclick"event.stopPropagation(); enqueueDocAttr('d.doc_id','plug', false);"Runbutton
button class"btn small" runnable?'':'disabled' onclick"event.stopPropagation(); enqueueDocAttr('d.doc_id','plug', true);"Run (Force)button
divtd
tr);
rows.push(tr id"doc_attr_full_d.doc_id_plug" style"display:attrOpen?'':'none';"tdtdtd colspan4pre style"white-space:pre-wrap;background:var(--panel-alt);padding:8px;border-radius:6px;border:1px solid var(--border);" id"doc_attr_pre_d.doc_id_plug"pretdtr);

Chunk-level attributes (currently exposing chunk-summary)
const cplug 'chunk-summary';
const cOpen (window.__DOC_CHUNK_ATTR_OPEN__ new Set()).has(cplug);
rows.push(tr onclick"toggleDocChunkAttr(event,'d.doc_id','cplug')" style"cursor:pointer;"
tdtd
td colspan2span id"doc_chunk_attr_arrow_d.doc_id_cplug"cOpen?'':''span cplug span style"opacity:.65;"(per-chunk)spantd
td colspan2td
tr);
rows.push(tr id"doc_chunk_attr_full_d.doc_id_cplug" style"display:cOpen?'':'none';"tdtdtd colspan4div style"background:var(--panel-alt);padding:8px;border-radius:6px;max-height:260px;overflow:auto;border:1px solid var(--border);" id"doc_chunk_attr_div_d.doc_id_cplug"divtdtr);

);
rows.push('tbodytable');
root.innerHTML rows.join('');

async function toggleDoc(doc_id)
if(window.__DOC_OPEN__.has(doc_id))
window.__DOC_OPEN__.delete(doc_id);
renderDocs();
return;

window.__DOC_OPEN__.add(doc_id);
if(!window.__DOC_META__ )
const r await fetch(apidocdoc_id);
const data await r.json();
window.__DOC_META__ data;

renderDocs();

async function toggleDocAttr(evt, doc_id, plug)
evt.stopPropagation();
const set (window.__DOC_ATTR_OPEN__ window.__DOC_ATTR_OPEN__ new Set());
const key plugdoc_id;
if(set.has(plug))
set.delete(plug);
const row document.getElementById(doc_attr_full_doc_id_plug);
const arrow document.getElementById(doc_attr_arrow_doc_id_plug);
if(row) row.style.display 'none';
if(arrow) arrow.textContent '';
return;

set.add(plug);
fetch content if not cached
if(window.__ATTR_ITEM_CACHE__ null)
const r await fetch(apidocdoc_idattrplug);
const data await r.json();
window.__ATTR_ITEM_CACHE__ (data.itemdata);

const pre document.getElementById(doc_attr_pre_doc_id_plug);
const row document.getElementById(doc_attr_full_doc_id_plug);
const arrow document.getElementById(doc_attr_arrow_doc_id_plug);
if(pre) pre.textContent JSON.stringify(window.__ATTR_ITEM_CACHE__ , null, 2);
if(row) row.style.display '';
if(arrow) arrow.textContent '';


async function toggleDocChunkAttr(evt, doc_id, plug)
evt.stopPropagation();
const set (window.__DOC_CHUNK_ATTR_OPEN__ window.__DOC_CHUNK_ATTR_OPEN__ new Set());
const key chunksplugdoc_id;
if(set.has(plug))
set.delete(plug);
const row document.getElementById(doc_chunk_attr_full_doc_id_plug);
const arrow document.getElementById(doc_chunk_attr_arrow_doc_id_plug);
if(row) row.style.display 'none';
if(arrow) arrow.textContent '';
return;

set.add(plug);
if(window.__CHUNK_ATTR_CACHE__ null)
const r await fetch(apidocdoc_idchunksattrplug);
const data await r.json();
window.__CHUNK_ATTR_CACHE__ (data.items );

const items window.__CHUNK_ATTR_CACHE__ ;
const div document.getElementById(doc_chunk_attr_div_doc_id_plug);
const row document.getElementById(doc_chunk_attr_full_doc_id_plug);
const arrow document.getElementById(doc_chunk_attr_arrow_doc_id_plug);
if(div)
if(items.length 0)
div.innerHTML 'em style"opacity:.75;"No itemsem';
else
const lines ;
for(const it of items)
const seq (it.seq null ? '' : it.seq);
const chunk it.chunk_id '';
const prev escapeHtml(it.preview'');
lines.push(trtd classmonoseqtdtd classmono style"word-break:break-all;"chunktdtdprevtdtr);

lines.push('tbodytable');
div.innerHTML lines.join('');


if(row) row.style.display '';
if(arrow) arrow.textContent '';

async function enqueueDocAttr(doc_id, plugin, force)
const meta window.__DOC_META__ ;
const attrs meta meta.attributes ? meta.attributes : ;
const info attrs ;
if(!info info.runnable false)
alert(Plugin plugin is not runnable from dashboard.);
return;

const payload force ? overwrite: true, force: true : force: true ;
await fetch('apienqueue',
method:'POST',
headers:'Content-Type':'applicationjson',
body: JSON.stringify(doc_ids: , plugins: , payload)
);
await refreshDocMeta(doc_id, plugin);
renderDocs();

async function enqueueDocAll(doc_id, force)
const meta window.__DOC_META__ ;
const attrs meta meta.attributes ? meta.attributes : ;
const runnable Object.keys(attrs).filter(pattrs attrs .runnable ! false);
if(!runnable.length)
alert('No runnable attributes for this document.');
return;

const payload force ? overwrite: true, force: true : force: true ;
await fetch('apienqueue',
method:'POST',
headers:'Content-Type':'applicationjson',
body: JSON.stringify(doc_ids: , plugins: runnable, payload)
);
await refreshDocMeta(doc_id);
renderDocs();

async function refreshDocMeta(doc_id, plugin)
const r await fetch(apidocdoc_id);
const data await r.json();
window.__DOC_META__ data;
if(plugin)
const key plugindoc_id;
delete window.__ATTR_ITEM_CACHE__ ;
if(window.__DOC_ATTR_OPEN__ window.__DOC_ATTR_OPEN__ .has(plugin))
const r2 await fetch(apidocdoc_idattrplugin);
const js await r2.json();
window.__ATTR_ITEM_CACHE__ (js.itemjs);



async function loadFailedJobs()
const r await fetch('apiqueuelist?statusfailedlimit50');
const data await r.json();
const items data.items ;
const root document.getElementById('failed_jobs');
if(!items.length) root.innerHTML 'smallNo failed jobssmall'; return;
root.innerHTML 'ul' items.map(ilia href"" onclick"viewJob(i.id);return false;"i.plugin :: i.doc_id (i.id)ai.last_error? small classerrescapeHtml((i.last_error'').slice(0,120))small:''li).join('') 'ul';

async function viewJob(id)
const r await fetch('apiqueuejob' id);
if(!r.ok) alert('Job 'id' not found'); return;
const job await r.json();
const pre document.createElement('pre');
pre.style.whiteSpace 'pre-wrap';
pre.style.background 'var(--panel-alt)';
pre.style.padding '8px';
pre.style.borderRadius '6px';
pre.style.border '1px solid var(--border)';
pre.textContent JSON.stringify(job, null, 2);
const root document.getElementById('failed_jobs');
root.innerHTML '';
root.appendChild(pre);


script
body
html



.kn__init__.py


.knconfig.py
import os, yaml, pathlib, re
from dotenv import load_dotenv

def _merge(a, b):
if not isinstance(b, dict): return a
out a.copy()
for k, v in b.items():
if isinstance(v, dict) and isinstance(out.get(k), dict):
out _merge(out , v)
else:
out v
return out

def load_configs():
load_dotenv(overrideTrue)
root pathlib.Path(os.getenv("KN_ROOT", ".knowledge"))
cfg_dir root "config"
models_yml cfg_dir "models.yml"
pipeline_yml cfg_dir "pipeline.yml"
cfg_dir.mkdir(parentsTrue, exist_okTrue)
cfg
"llm":
"base_url": os.getenv("OPENAI_BASE_URL", "http:localhost:1234v1"),
"api_key": os.getenv("OPENAI_API_KEY", "lm-studio"),
"model": os.getenv("OPENAI_MODEL", "meta-llama-3.1-8b-instruct"),
"max_tokens": 4096,
"temperature": 0.2,
"service": os.getenv("OPENAI_SERVICE", "lmstudio"),
,
"embeddings": "name": os.getenv("EMBED_MODEL", "BAAIbge-small-en-v1.5"), "normalize": True, "device": "auto",
"stores":
"vector": "kind": "hnsw", "path": str(root"indexes""embeddings""hnsw.index"),
"graph": "kind": "networkx",
"jobs": "kind": "sqlite", "path": str(root"queues""jobs.sqlite"),
,
"llm_gateway":
"queue_path": str(root"queues""llm_gateway.sqlite"),
"poll_interval_ms": 200,
"wait_timeout_sec": 600,
"health_timeout_sec": 10,
,
"llm_services":
"lmstudio":
"provider": "lmstudio",
"base_url": os.getenv("OPENAI_BASE_URL", "http:localhost:1234v1"),
"api_key": os.getenv("OPENAI_API_KEY", "lm-studio"),
"default_model": os.getenv("OPENAI_MODEL", "meta-llama-3.1-8b-instruct"),
"max_context_tokens": 8192,
"max_response_tokens": 4096,
"timeout": 120,
"max_concurrency": 1,
"min_request_spacing_ms": 0,
"unload_on_switch": True,
"force_model_reload": False,
"idle_unload_sec": 0,

,
"watch": "paths": , "ignore": ,
"ocr": "enabled": os.getenv("OCR_ENABLED","false").lower() "true", "tesseract_cmd": "",
"export": "default_budget_tokens": 600000, "strategy":"hierarchy-first", "exclude_pii": False, "format":"md",
"retrieval": "dense_k":12, "bm25_k":8, "graph_hops":2, "rerank": False,
"graph": "community_detection":"louvain", "edge_conf_threshold":0.55, "cross_domain_bridge_threshold":0.75,
"entity_extraction":"selective", "relation_extraction":"selective",
"chunking": "policies":"default":"max_chars":4000,"overlap":400,
"code":"max_chars":2400,"overlap":200,
"pdf":"max_chars":3500,"overlap":200,
"attributes": "plugins": , "auto_enqueue": True,
"plugins":
"summary-20w":
"llm":
"model": "meta-llama-3.1-8b-instruct",
"timeout": 120,
"max_tokens": 2048,
"temperature": 0.3

,
"summary-short":
"llm":
"model": "meta-llama-3.1-8b-instruct",
"timeout": 120,
"max_tokens": 2048,
"temperature": 0.3

,
"topic-tags":
"llm":
"model": "meta-llama-3.1-8b-instruct",
"timeout": 120,
"max_tokens": 1536,
"temperature": 0.2

,
"multi-basic":
"llm":
"model": "qwen2.5-32b-instruct",
"timeout": 420,
"max_tokens": 8192,
"temperature": 0.2

,
"doc-skeleton":
"llm":
"model": "qwen2.5-32b-instruct",
"timeout": 420,
"max_tokens": 8192,
"temperature": 0.2




if models_yml.exists():
cfg _merge(cfg, yaml.safe_load(models_yml.read_text(encoding"utf-8")))
if pipeline_yml.exists():
cfg _merge(cfg, yaml.safe_load(pipeline_yml.read_text(encoding"utf-8")))
cfg _expand_env_vars(cfg)
cfg str(root)
return cfg

_ENV_VAR_PATTERN re.compile(r"( )")

def _expand_env_vars(obj):
if isinstance(obj, dict):
return k: _expand_env_vars(v) for k, v in obj.items()
if isinstance(obj, list):
return
if isinstance(obj, str):
def repl(m):
key m.group(1)
return os.getenv(key, m.group(0))
return _ENV_VAR_PATTERN.sub(repl, obj)
return obj

.knhashing.py
import hashlib

def content_hash(text: str) - str:
return hashlib.sha256(text.encode("utf-8", errors"ignore")).hexdigest()

def short_hash(h: str, n8) - str:
return h

def chunk_hash(doc_id: str, chunk_index: int) - str:
return f"doc_id-chunk_index:04d"

.knfile_index.py
import pathlib, time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from .utils.io import read_text_safely, ensure_dirs
from .hashing import content_hash, short_hash
from .jobs_sqlite import ensure_db as _ensure_jobs_db, enqueue as _enqueue_job

_manifest path - doc_id, hash, mtime
IGNORE_DIRS ".git", ".knowledge"
IGNORE_EXT ".lock", ".tmp", ".log"

def initial_scan(repo_path: pathlib.Path, cfg):
docs
for p in repo_path.rglob(""):
if p.is_dir():
if p.name in IGNORE_DIRS: continue
continue
if any(part in IGNORE_DIRS for part in p.parts): continue
if p.suffix.lower() in IGNORE_EXT: continue
text read_text_safely(p)
if not text: continue
ch content_hash(text)
doc_id short_hash(ch)
_manifest "doc_id": doc_id, "hash": ch, "mtime": p.stat().st_mtime
docs.append("path": str(p), "doc_id": doc_id, "hash": ch, "text": text)
return docs

class _Evt(FileSystemEventHandler):
def __init__(self, cfg):
self.cfg cfg
def on_any_event(self, event):
p pathlib.Path(getattr(event, 'dest_path', event.src_path))
if p.is_dir() or any(part in IGNORE_DIRS for part in p.parts): return
if not p.exists(): return
text read_text_safely(p)
if text is None: return
ch content_hash(text)
m _manifest.get(str(p))
if m and m ch:
m p.stat().st_mtime
return
record "doc_id": short_hash(ch), "hash": ch, "mtime": p.stat().st_mtime
_manifest record
print(f" change: p")
auto-enqueue attribute jobs (doc-level) if configured
attrs self.cfg.get("attributes", )
if attrs.get("auto_enqueue", False):
plugins attrs.get("plugins", )
for plugin in plugins:
_enqueue_job(self.cfg, plugin, record , payload"path": str(p))
TODO: optionally enqueue ingestgraph rebuild jobs (separate scheduler handles embedgraph nightly)

def watch_changes(repo_path: pathlib.Path, cfg):
ensure_dirs(cfg)
ensure jobs db exists if auto-enqueue is enabled
attrs cfg.get("attributes", )
if attrs.get("auto_enqueue", False):
_ensure_jobs_db(cfg)
obs Observer()
obs.schedule(_Evt(cfg), str(repo_path), recursiveTrue)
obs.start()
try:
while True:
time.sleep(1)
except KeyboardInterrupt:
obs.stop()
obs.join()

.knchunking.py
import pathlib, json, re
from .utils.io import ensure_dirs
from .hashing import chunk_hash

CHUNKS_DIR pathlib.Path('.knowledgeindexeschunks')

def _split_paragraphs(text: str):
split on blank lines; keep headings by not stripping too aggressively
paras re.split(r"nsn", text)
normalize paragraphs
return

def chunk_text(text: str, max_chars3000, overlap_ratio0.1):
paragraph-first packing with small overlap (last paragraph of previous chunk)
paras _split_paragraphs(text)
chunks
current
current_len 0
for para in paras:
p_len len(para) 2 account for join newlines
if current_len p_len max_chars or not current:
current.append(para)
current_len p_len
else:
chunks.append("nn".join(current))
overlap: keep last paragraph from previous chunk
overlap_n 1 if current else 0
tail current if overlap_n else
current tail
current_len sum(len(t) 2 for t in current)
if current:
chunks.append("nn".join(current))
return chunks

def detect_domain(rel_path: pathlib.Path):
parts rel_path.parts
return parts if parts else "root"

def chunk_repo(docs, cfg, repo_root: pathlib.Path):
ensure_dirs(cfg)
policy cfg.get("chunking", ).get("policies", ).get("default", "max_chars":3000,"overlap":300)
chunks
CHUNKS_DIR.mkdir(parentsTrue, exist_okTrue)
for d in docs:
path pathlib.Path(d ).resolve()
try: rel path.relative_to(repo_root)
except Exception: rel pathlib.Path(path.name)
domain detect_domain(rel)
parts chunk_text(d.get("text",""), max_charspolicy.get("max_chars",3000), overlap_ratio(policy.get("overlap",300)max(policy.get("max_chars",3000),1)))
total len(parts)
for idx, t in enumerate(parts):
chunk_id chunk_hash(d , idx)
rec
"doc_id": d ,
"chunk_id": chunk_id,
"text": t,
"meta":
"path": str(path),
"rel": str(rel),
"domain": domain,
"mimetype": "textplain",
"seq": idx,
"total": total,
"prev": chunk_hash(d , idx-1) if idx0 else None,
"next": chunk_hash(d , idx1) if idxtotal-1 else None


chunks.append(rec)
(CHUNKS_DIR f"chunk_id.json").write_text(json.dumps(rec, ensure_asciiFalse), encoding'utf-8')
return chunks

.knembeddings.py
from sentence_transformers import SentenceTransformer
import numpy as np
from .vector_hnsw import HNSWIndex

_model None
def _get_model(name: str, device: str"auto"):
global _model
if _model is None:
_model SentenceTransformer(name, device(None if device"auto" else device))
return _model

def embed_chunks(chunks, cfg):
emc cfg.get("embeddings", )
m _get_model(emc.get("name", "BAAIbge-small-en-v1.5"), emc.get("device", "auto"))
if not chunks:
return
texts for c in chunks
vecs m.encode(texts, normalize_embeddingsemc.get("normalize", True), show_progress_barTrue)
idx HNSWIndex.open(cfg, dimlen(vecs ))
idx.add( for c in chunks, np.asarray(vecs, dtypenp.float32))
idx.save()

.knvector_hnsw.py
import pathlib, json
import numpy as np
try:
import hnswlib optional on Windows
_HNSW_AVAILABLE True
except Exception:
hnswlib None
_HNSW_AVAILABLE False

class _BruteForceIndex:
def __init__(self, space: str, dim: int, store_path: str None None):
self.space space
self.dim dim
self.vectors None np.ndarray shape (N, D)
self.store_path store_path
def load_index(self, path: str):
For brute-force, read from an .npy file alongside meta
npy self.store_path or (path ".npy")
try:
if pathlib.Path(npy).exists():
self.vectors np.load(npy).astype(np.float32)
except Exception:
self.vectors None
def set_ef(self, ef: int):
pass
def init_index(self, max_elements: int, ef_construction: int, M: int):
pass
def add_items(self, vecs: np.ndarray, labels: np.ndarray):
if self.vectors is None:
self.vectors vecs.astype(np.float32)
else:
self.vectors np.vstack( )
def save_index(self, path: str):
Persist vectors so future processes can search
npy self.store_path or (path ".npy")
try:
if self.vectors is not None:
pathlib.Path(npy).parent.mkdir(parentsTrue, exist_okTrue)
np.save(npy, self.vectors)
except Exception:
pass
def knn_query(self, q: np.ndarray, k: int 10):
if self.vectors is None or len(self.vectors) 0:
return np.zeros((len(q), 0), dtypeint), np.zeros((len(q), 0), dtypenp.float32)
cosine or l2
Q q.astype(np.float32)
X self.vectors
if self.space 'cosine':
def normalize(a):
n np.linalg.norm(a, axis1, keepdimsTrue) 1e-12
return a n
Qn normalize(Q)
Xn normalize(X)
sims Qn Xn.T
higher is better; convert to distances for compatibility
dists 1.0 - sims
else:
l2 distance
Q - X2 Q2 X2 - 2 Q XT
Q2 np.sum(Q Q, axis1, keepdimsTrue)
X2 np.sum(X X, axis1)
dists Q2 X2 - 2.0 (Q X.T)
idxs np.argsort(dists, axis1)
rows np.take_along_axis(dists, idxs, axis1)
return idxs, rows
class HNSWIndex:
def __init__(self, path: pathlib.Path, dim768, space'cosine'):
self.path path
self.meta_path path.with_suffix('.meta.json')
self.dim dim
self.space space
if _HNSW_AVAILABLE:
self.index hnswlib.Index(spacespace, dimdim)
else:
self.index _BruteForceIndex(spacespace, dimdim, store_pathstr(self.path) ".npy")
self.inited False
self.ids
classmethod
def open(cls, cfg, dim768):
path pathlib.Path(cfg )
meta path.with_suffix('.meta.json')
if path.exists() and meta.exists():
m json.loads(meta.read_text())
dim m.get("dim", dim)
obj cls(path, dimdim)
if brute-force, nothing to load
try:
obj.index.load_index(str(path))
obj.index.set_ef(128)
except Exception:
pass
obj.inited True
obj.ids m.get("ids", )
return obj
return cls(path, dimdim)
def _ensure_init(self, total10000):
if not self.inited:
try:
self.index.init_index(max_elementstotal, ef_construction200, M16)
self.index.set_ef(128)
except Exception:
pass
self.inited True
def add(self, keys, vecs: np.ndarray):
self._ensure_init(max(10000, len(self.ids) len(keys) 1000))
labels np.arange(len(self.ids), len(self.ids)len(keys))
self.index.add_items(vecs, labels)
self.ids.extend(list(keys))
def save(self):
self.path.parent.mkdir(parentsTrue, exist_okTrue)
try:
self.index.save_index(str(self.path))
except Exception:
pass
self.meta_path.write_text(json.dumps("dim": self.dim, "ids": self.ids))
def search(self, vecs: np.ndarray, k10):
labels, dists self.index.knn_query(vecs, kk)
inv self.ids
mapped for i in row for row in labels
return mapped, dists

.kngraph_nx.py
import pathlib, json, networkx as nx
from collections import defaultdict
GRAPH_PATH pathlib.Path('.knowledgeindexesgraphgraph.jsonl')
COMM_PATH pathlib.Path('.knowledgeindexessummariescommunities.jsonl')
G nx.Graph()
def naive_entities(text: str):
import re
ents set(re.findall(r" b .ww", text))
return list(ents)
def build_or_update_graph(chunks, cfg):
for c in chunks:
ents naive_entities(c )
for e in ents:
if not G.has_node(e):
G.add_node(e, kind'entity')
for i in range(len(ents)):
for j in range(i1, len(ents)):
u, v ents , ents
w G 1 if G.has_edge(u,v) else 1
G.add_edge(u, v, weightw)
GRAPH_PATH.parent.mkdir(parentsTrue, exist_okTrue)
with GRAPH_PATH.open('w', encoding'utf-8') as f:
for u, v, d in G.edges(dataTrue):
f.write(json.dumps("src": u, "dst": v, "weight": d.get("weight",1))"n")
def summarize_communities(cfg):
try:
import community as community_louvain
part community_louvain.best_partition(G) if len(G) else
comm_to_nodes defaultdict(list)
for n, c in part.items():
comm_to_nodes .append(n)
comms ) for k, v in comm_to_nodes.items()
except Exception:
comms
for idx, comp in enumerate(nx.connected_components(G)):
nodes list(comp)
summary ", ".join(nodes )
comms.append("id": f"Cidx", "size": len(nodes), "summary": summary)
COMM_PATH.parent.mkdir(parentsTrue, exist_okTrue)
with COMM_PATH.open('w', encoding'utf-8') as f:
for c in comms:
f.write(json.dumps(c)"n")

.knretrieval.py
import json, pathlib
import numpy as np
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from .vector_hnsw import HNSWIndex
COMM_PATH pathlib.Path('.knowledgeindexessummariescommunities.jsonl')
CHUNK_DIR pathlib.Path('.knowledgeindexeschunks')
_embed_model None
def _get_model(name):
global _embed_model
if _embed_model is None:
_embed_model SentenceTransformer(name)
return _embed_model
def _iter_chunks():
for p in CHUNK_DIR.glob('.json'):
yield json.loads(p.read_text(encoding'utf-8'))
def _matches_scope(rec, scope: str) - bool:
if not scope:
return True
meta rec.get('meta', )
rel str(meta.get('rel', ''))
path str(meta.get('path', ''))
domain str(meta.get('domain', ''))
s scope.lower()
return (s in rel.lower()) or (s in path.lower()) or (s domain.lower())
def _bm25_corpus():
docs list(_iter_chunks())
corpus , (d or "").split()) for d in docs
return docs, corpus
def answer_query(q: str, cfg, scopeNone, topk12):
emc cfg.get('embeddings', )
m _get_model(emc.get('name','BAAIbge-small-en-v1.5'))
qv m.encode( , normalize_embeddingsemc.get('normalize', True))
idx HNSWIndex.open(cfg, dimlen(qv ))
ids, dists idx.search(np.asarray(qv, dtypenp.float32), ktopk)
idset set(ids ) if ids else set()
chunks_by_id
for rec in _iter_chunks():
chunks_by_id rec
dense_hits for i in idset if i in chunks_by_id
bm25_hits
try:
docs, corpus _bm25_corpus()
bm25 BM25Okapi( )
scores bm25.get_scores(q.split())
k cfg.get("retrieval", ).get("bm25_k", 8)
ranked np.argsort(scores)
for idx_i in ranked:
bm25_hits.append(docs )
except Exception:
pass
used set()
merged
for h in dense_hits bm25_hits:
if scope and not _matches_scope(h, scope):
continue
cid h
if cid in used: continue
used.add(cid)
merged.append(h)
if len(merged) topk: break
pref
if COMM_PATH.exists():
for line in COMM_PATH.read_text(encoding'utf-8').splitlines() :
c json.loads(line)
pref.append(f" sizec c ")
text "nn".join(pref) "nn" "nn".join( for h in merged)
return text

def retrieve_context(q: str, cfg, scopeNone, topk64):
"""Return preface lines and selected chunk records for downstream assemblyexport."""
emc cfg.get('embeddings', )
m _get_model(emc.get('name','BAAIbge-small-en-v1.5'))
qv m.encode( , normalize_embeddingsemc.get('normalize', True))
idx HNSWIndex.open(cfg, dimlen(qv ))
ids, dists idx.search(np.asarray(qv, dtypenp.float32), ktopk)
idset set(ids ) if ids else set()
chunks_by_id
for rec in _iter_chunks():
chunks_by_id rec
dense_hits for i in idset if i in chunks_by_id
bm25_hits
try:
docs, corpus _bm25_corpus()
bm25 BM25Okapi( )
scores bm25.get_scores(q.split())
k cfg.get("retrieval", ).get("bm25_k", 8)
ranked np.argsort(scores)
for idx_i in ranked:
bm25_hits.append(docs )
except Exception:
pass
used set()
merged
for h in dense_hits bm25_hits:
if scope and not _matches_scope(h, scope):
continue
cid h
if cid in used: continue
used.add(cid)
merged.append(h)
if len(merged) topk: break
pref
if COMM_PATH.exists():
for line in COMM_PATH.read_text(encoding'utf-8').splitlines() :
c json.loads(line)
pref.append(f" sizec c ")
return pref, merged

.knexporter.py
import pathlib
from .retrieval import retrieve_context
import json, pathlib as _p

def _approx_trim_to_tokens(text: str, max_tokens: int) - str:
if not max_tokens:
return text
4 charstoken heuristic
max_chars max(0, int(max_tokens 4))
return text

def _pii_flagged(doc_id: str) - bool:
p _p.Path('.knowledgeindexesattributespii-scan') f"doc_id.json"
if not p.exists():
return False
try:
data json.loads(p.read_text(encoding'utf-8'))
return (data.get('risk') or 0) 0.8
except Exception:
return False

def export_monofile(q: str, cfg, out: pathlib.Path, budget: intNoneNone, scopeNone, include_meta: boolTrue):
pref, chunks retrieve_context(q, cfg, scopescope, topk64)
is_md cfg.get("export", ).get("format","md") "md"
if cfg.get('export', ).get('exclude_pii', False):
chunks
parts
if is_md:
parts.append(f" Export: qn")
if pref:
parts.append("n".join(pref) ("nn" if is_md else "nn"))
for c in chunks:
meta c.get('meta', )
if include_meta and is_md:
parts.append(f" meta.get('rel', meta.get('path','')) c n")
parts.append(c.get('text',''))
parts.append("nn")
body "".join(parts)
body _approx_trim_to_tokens(body, budget or cfg.get('export',).get('default_budget_tokens'))
out.parent.mkdir(parentsTrue, exist_okTrue)
out.write_text(body, encoding'utf-8')

.knjobs_sqlite.py
import sqlite3, json, pathlib
JOB_ATTR_PREFIX "attribute:"
def _db_path(cfg):
return pathlib.Path(cfg )
def ensure_db(cfg):
p _db_path(cfg); p.parent.mkdir(parentsTrue, exist_okTrue)
con sqlite3.connect(p)
con.execute("""
CREATE TABLE IF NOT EXISTS jobs(
id INTEGER PRIMARY KEY AUTOINCREMENT,
plugin TEXT NOT NULL,
doc_id TEXT NOT NULL,
payload TEXT,
status TEXT DEFAULT 'pending',
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
""")
add optional columns if missing (migrations)
try:
cols for r in con.execute("PRAGMA table_info(jobs)").fetchall()
if 'completed_at' not in cols:
con.execute("ALTER TABLE jobs ADD COLUMN completed_at TIMESTAMP")
if 'retries' not in cols:
con.execute("ALTER TABLE jobs ADD COLUMN retries INTEGER DEFAULT 0")
if 'last_error' not in cols:
con.execute("ALTER TABLE jobs ADD COLUMN last_error TEXT")
except Exception:
pass
limits and counters tables for concurrency control
try:
con.execute("CREATE TABLE IF NOT EXISTS limits(name TEXT PRIMARY KEY, value INTEGER)")
con.execute("CREATE TABLE IF NOT EXISTS counters(name TEXT PRIMARY KEY, value INTEGER)")
except Exception:
pass
con.commit(); con.close()
def enqueue(cfg, plugin: str, doc_id: str, payload: dictNoneNone):
con sqlite3.connect(_db_path(cfg))
con.execute("INSERT INTO jobs(plugin, doc_id, payload) VALUES (?,?,?)",
(plugin, doc_id, json.dumps(payload or )))
con.commit(); con.close()
def dequeue_batch(cfg, wanted_plugins, limit16):
if not wanted_plugins:
return
con sqlite3.connect(_db_path(cfg)); cur con.cursor()
qmarks ",".join( len(wanted_plugins))
cur.execute(f"""
SELECT id, plugin, doc_id, payload FROM jobs
WHERE status'pending' AND plugin IN (qmarks)
ORDER BY COALESCE(retries,0) ASC, id ASC LIMIT ?
""", (wanted_plugins, limit))
rows cur.fetchall(); ids for r in rows
if ids:
cur.execute(f"UPDATE jobs SET status'running' WHERE id IN (','.join( len(ids)))", ids)
con.commit(); con.close()
return , "plugin": r , "doc_id": r , "payload": json.loads(r or '') for r in rows
def ack_job(cfg, job_id: int):
con sqlite3.connect(_db_path(cfg)); con.execute("UPDATE jobs SET status'done', completed_atCURRENT_TIMESTAMP WHERE id?", (job_id,))
con.commit(); con.close()

def fail_and_requeue_job(cfg, job_id: int, error_message: strNoneNone, back_to_pending: boolTrue):
con sqlite3.connect(_db_path(cfg))
if back_to_pending:
con.execute("UPDATE jobs SET status'pending', retriescoalesce(retries,0)1, last_error? WHERE id?", (error_message, job_id))
else:
con.execute("UPDATE jobs SET status'failed', retriescoalesce(retries,0)1, last_error? WHERE id?", (error_message, job_id))
con.commit(); con.close()

def reset_status(cfg, job_ids, status'pending'):
if not job_ids:
return
con sqlite3.connect(_db_path(cfg))
placeholders ",".join( len(job_ids))
con.execute(f"UPDATE jobs SET status? WHERE id IN (placeholders)", (status, job_ids))
con.commit(); con.close()

def reset_running_jobs(cfg, status'pending'):
con sqlite3.connect(_db_path(cfg))
try:
cur con.cursor()
ids for row in cur.execute("select id from jobs where status'running'").fetchall()
finally:
con.close()
if not ids:
return
reset_status(cfg, ids, statusstatus)
return ids

def list_pending_plugins(cfg):
con sqlite3.connect(_db_path(cfg))
rows for r in con.execute("select distinct plugin from jobs where status'pending'").fetchall()
con.close()
return rows

Concurrency primitives (best-effort with SQLite)
def set_limit(cfg, name: str, value: int):
con sqlite3.connect(_db_path(cfg)); cur con.cursor()
cur.execute("INSERT INTO limits(name,value) VALUES(?,?) ON CONFLICT(name) DO UPDATE SET valueexcluded.value", (name, value))
ensure counter row exists
cur.execute("INSERT INTO counters(name,value) VALUES(?,0) ON CONFLICT(name) DO NOTHING", (name,))
con.commit(); con.close()

def get_limit(cfg, name: str) - intNone:
con sqlite3.connect(_db_path(cfg)); cur con.cursor()
row cur.execute("select value from limits where name?", (name,)).fetchone()
con.close()
return row if row else None

def get_counter(cfg, name: str) - int:
con sqlite3.connect(_db_path(cfg)); cur con.cursor()
row cur.execute("select value from counters where name?", (name,)).fetchone()
con.close()
return row if row else 0

def reset_counter(cfg, name: str):
con sqlite3.connect(_db_path(cfg))
try:
con.execute("UPDATE counters SET value0 WHERE name?", (name,))
con.commit()
finally:
con.close()

def try_acquire(cfg, name: str) - bool:
con sqlite3.connect(_db_path(cfg))
try:
con.isolation_level None
con.execute("BEGIN IMMEDIATE")
cur con.cursor()
row cur.execute("select value from limits where name?", (name,)).fetchone()
limit row if row else None
if limit is None:
default no limit
limit 1
cur.execute("INSERT INTO limits(name,value) VALUES(?,?) ON CONFLICT(name) DO NOTHING", (name, limit))
row cur.execute("select value from counters where name?", (name,)).fetchone()
current row if row else 0
if current limit:
if row:
cur.execute("update counters set valuevalue1 where name?", (name,))
else:
cur.execute("insert into counters(name,value) values(?,1)", (name,))
con.execute("COMMIT")
return True
con.execute("ROLLBACK")
return False
except Exception:
try:
con.execute("ROLLBACK")
except Exception:
pass
return False
finally:
con.close()

def release(cfg, name: str):
con sqlite3.connect(_db_path(cfg))
try:
con.execute("update counters set valuemax(value-1,0) where name?", (name,))
con.commit()
finally:
con.close()
def iter_docs_for_jobs(jobs):
from pathlib import Path
cdir Path('.knowledgeindexeschunks'); out
for j in jobs:
for p in sorted(cdir.glob(f"j -.json")):
out json.loads(p.read_text(encoding'utf-8')); break
return out

.knutilsio.py
import pathlib, chardet, os
try:
import pytesseract
from PIL import Image
except Exception:
pytesseract None
Image None
try:
import fitz PyMuPDF
except Exception:
fitz None
ROOT pathlib.Path('.knowledge')
def ensure_dirs(cfgNone):
(ROOT"indexes""chunks").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""embeddings").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""graph").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""summaries").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""attributes").mkdir(parentsTrue, exist_okTrue)
(ROOT"indexes""manifests").mkdir(parentsTrue, exist_okTrue)
(ROOT"queues").mkdir(parentsTrue, exist_okTrue)
(ROOT"exports""dumps").mkdir(parentsTrue, exist_okTrue)
def read_text_safely(path: pathlib.Path):
try:
suffix path.suffix.lower()
if suffix '.pdf' and fitz is not None:
text_parts
with fitz.open(str(path)) as doc:
for page in doc:
text_parts.append(page.get_text("text"))
txt "n".join(text_parts)
if txt.strip():
return txt
fall through to OCR if enabled
OCR for images (or empty PDFs) when enabled
if (suffix in '.png','.jpg','.jpeg','.tif','.tiff' or (suffix'.pdf' and fitz is not None)) and (os.getenv('OCR_ENABLED','false').lower()'true') and pytesseract and Image:
tcmd os.getenv('TESSERACT_CMD') or ""
if tcmd:
try: pytesseract.pytesseract.tesseract_cmd tcmd
except Exception: pass
try:
if suffix'.pdf' and fitz is not None:
render pages to images and OCR
out
with fitz.open(str(path)) as doc:
for page in doc:
pix page.get_pixmap(dpi200)
img Image.frombytes("RGB", , pix.samples)
out.append(pytesseract.image_to_string(img))
ocr_txt "n".join(out)
else:
ocr_txt pytesseract.image_to_string(Image.open(str(path)))
if ocr_txt.strip():
return ocr_txt
except Exception:
pass
default: try bytes decode with chardet
data path.read_bytes()
enc chardet.detect(data).get('encoding') or 'utf-8'
return data.decode(enc, errors'ignore')
except Exception:
return None

.knutilsskeleton.py
import json, pathlib

SKELETON_DIR pathlib.Path('.knowledgeindexesattributesdoc-skeleton')

def load_skeleton_text(doc_id: str) - strNone:
p SKELETON_DIR f"doc_id.json"
if not p.exists():
return None
try:
data json.loads(p.read_text(encoding'utf-8'))
val data.get('value') or
parts
if isinstance(val, dict):
outline val.get('outline')
if outline:
parts.append("Outline:n" outline)
s100 val.get('summary100') or val.get('summary')
if s100:
parts.append("Summary:n" s100)
kp val.get('keyphrases')
if kp:
parts.append("Keyphrases: " kp)
return "nn".join(parts) if parts else None
except Exception:
return None



.knutilscache.py
import hashlib, json, pathlib
from typing import Any, Optional

CACHE_DIR pathlib.Path('.knowledgecachellm')

def _hash_key(s: str) - str:
return hashlib.sha1(s.encode('utf-8', errors'ignore')).hexdigest()

def get_cached_response(key: str) - Optional :
CACHE_DIR.mkdir(parentsTrue, exist_okTrue)
p CACHE_DIR (_hash_key(key) '.json')
if not p.exists():
return None
try:
data json.loads(p.read_text(encoding'utf-8'))
return data.get('content')
except Exception:
return None

def set_cached_response(key: str, content: str) - None:
CACHE_DIR.mkdir(parentsTrue, exist_okTrue)
p CACHE_DIR (_hash_key(key) '.json')
p.write_text(json.dumps('content': content, ensure_asciiFalse), encoding'utf-8')



.knutils__init__.py
"""Utility helpers for kn package."""

__all__




.knutilsllm_client.py
"""Backward-compatible LLM client that delegates to the gateway."""

from __future__ import annotations

from typing import Any, Dict, Optional

from kn.llm_gateway.client import submit_chat_request


def chat(
prompt: str,
max_tokens: int 512,
temperature: float 0.2,
overrides: Optional None,
cache_key: Optional None,
plugin_name: Optional None,
force: bool False,
overwrite: bool False,
) - str:
"""Proxy chat helper used by legacy code paths."""

return submit_chat_request(
prompt,
max_tokensmax_tokens,
temperaturetemperature,
overridesoverrides,
cache_keycache_key,
plugin_nameplugin_name,
forceforce,
overwriteoverwrite,
)








.knllm_gateway__init__.py
"""LLM gateway package for managing service workers and queue interactions."""

from .client import submit_chat_request, submit_generic_request
from .runner import run_gateway_service

__all__
"submit_chat_request",
"submit_generic_request",
"run_gateway_service",





.knllm_gatewaymodels.py
"""Data models for LLM gateway requests and responses."""

from __future__ import annotations

import dataclasses
import time
from typing import Any, Dict, Optional


dataclasses.dataclass(slotsTrue)
class GatewayRequest:
"""A unit of work destined for an LLM service agent."""

request_id: str
service: str
model: str
payload: Dict
metadata: Dict
created_ts: float dataclasses.field(default_factorylambda: time.time())

def to_dict(self) - Dict :
return
"request_id": self.request_id,
"service": self.service,
"model": self.model,
"payload": json.loads(json.dumps(self.payload, ensure_asciiFalse, defaultstr)),
"metadata": json.loads(json.dumps(self.metadata, ensure_asciiFalse, defaultstr)),
"created_ts": self.created_ts,


classmethod
def from_row(cls, row: Dict ) - "GatewayRequest":
return cls(
request_idrow ,
servicerow ,
modelrow ,
payloadrow ,
metadatarow.get("metadata", ) or ,
created_tsrow.get("created_ts", time.time()),
)


dataclasses.dataclass(slotsTrue)
class GatewayResponse:
"""Response emitted by an LLM service agent."""

request_id: str
success: bool
content: Optional
raw: Optional
error: Optional
latency_ms: Optional

def to_dict(self) - Dict :
return
"request_id": self.request_id,
"success": self.success,
"content": self.content,
"raw": self.raw,
"error": self.error,
"latency_ms": self.latency_ms,









.knllm_gatewaystorage.py
"""Persistence helpers for the LLM gateway SQLite queue."""

from __future__ import annotations

import json
import sqlite3
import threading
import time
from pathlib import Path
from typing import Any, Dict, Optional

from .models import GatewayRequest, GatewayResponse

_SCHEMA """
CREATE TABLE IF NOT EXISTS requests (
id INTEGER PRIMARY KEY AUTOINCREMENT,
request_id TEXT UNIQUE,
service TEXT NOT NULL,
model TEXT NOT NULL,
payload TEXT NOT NULL,
metadata TEXT,
created_ts REAL NOT NULL,
status TEXT NOT NULL DEFAULT 'pending'
);

CREATE TABLE IF NOT EXISTS responses (
id INTEGER PRIMARY KEY AUTOINCREMENT,
request_id TEXT UNIQUE,
success INTEGER NOT NULL,
content TEXT,
raw TEXT,
error TEXT,
latency_ms REAL,
created_ts REAL NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_requests_status ON requests(status);
CREATE INDEX IF NOT EXISTS idx_requests_service ON requests(service);
"""


class QueueStorage:
"""Thin SQLite wrapper for storing gateway requests and responses."""

def __init__(self, db_path: Path, busy_timeout_ms: int 5000):
self.db_path Path(db_path)
self._lock threading.Lock()
self._busy_timeout_ms busy_timeout_ms
self._ensure_schema()

def _connect(self) - sqlite3.Connection:
con sqlite3.connect(str(self.db_path))
con.execute(f"PRAGMA busy_timeout self._busy_timeout_ms")
return con

def _ensure_schema(self):
self.db_path.parent.mkdir(parentsTrue, exist_okTrue)
with self._connect() as con:
con.executescript(_SCHEMA)
con.commit()

def enqueue(self, req: GatewayRequest):
payload_json _safe_json(req.payload)
metadata_json _safe_json(req.metadata or )
with self._connect() as con:
con.execute(
"INSERT OR REPLACE INTO requests(request_id, service, model, payload, metadata, created_ts, status)"
" VALUES(?,?,?,?,?,?, 'pending')",
(
req.request_id,
req.service,
req.model,
payload_json,
metadata_json,
req.created_ts,
),
)
con.commit()

def dequeue_pending(self, service: str) - Optional :
with self._lock:
with self._connect() as con:
cur con.execute(
"SELECT request_id, service, model, payload, metadata, created_ts FROM requests"
" WHERE status'pending' AND service? ORDER BY id LIMIT 1",
(service,),
)
row cur.fetchone()
if not row:
return None
con.execute("UPDATE requests SET status'running' WHERE request_id?", (row ,))
con.commit()
payload json.loads(row ) if row else
metadata json.loads(row ) if row else
return GatewayRequest(
request_idrow ,
servicerow ,
modelrow ,
payloadpayload,
metadatametadata,
created_tsrow ,
)

def store_response(self, response: GatewayResponse):
content _safe_text(response.content)
raw_json _safe_json(response.raw) if response.raw is not None else None
error_text _safe_text(response.error)
with self._connect() as con:
con.execute(
"INSERT OR REPLACE INTO responses(request_id, success, content, raw, error, latency_ms, created_ts)"
" VALUES(?,?,?,?,?,?,?)",
(
response.request_id,
1 if response.success else 0,
content,
raw_json,
error_text,
response.latency_ms,
time.time(),
),
)
con.execute("UPDATE requests SET status? WHERE request_id?", ("done" if response.success else "failed", response.request_id))
con.commit()

def fetch_response(self, request_id: str) - Optional :
with self._connect() as con:
cur con.execute(
"SELECT success, content, raw, error, latency_ms FROM responses WHERE request_id?",
(request_id,),
)
row cur.fetchone()
if not row:
return None
raw json.loads(row ) if row else None
return GatewayResponse(
request_idrequest_id,
successbool(row ),
contentrow ,
rawraw,
errorrow ,
latency_msrow ,
)

def mark_failed(self, request_id: str, error_message: str):
resp GatewayResponse(
request_idrequest_id,
successFalse,
contentNone,
rawNone,
errorerror_message,
latency_msNone,
)
self.store_response(resp)

def purge_stale(self, max_age_sec: float):
cutoff time.time() - max_age_sec
with self._connect() as con:
con.execute("DELETE FROM requests WHERE status IN ('done','failed') AND created_ts?", (cutoff,))
con.execute("DELETE FROM responses WHERE created_ts?", (cutoff,))
con.commit()

def stats(self, service: Optional None) - Dict :
where ""
params ()
if service:
where " WHERE service?"
params (service,)
with self._connect() as con:
pending con.execute(f"SELECT COUNT() FROM requests WHERE status'pending'" (" AND service?" if service else ""), params).fetchone()
running con.execute(f"SELECT COUNT() FROM requests WHERE status'running'" (" AND service?" if service else ""), params).fetchone()
return "pending": pending, "running": running


def _safe_json(value: Any) - str:
try:
text json.dumps(value, ensure_asciiFalse)
except Exception:
text json.dumps(str(value), ensure_asciiFalse)
return _safe_text(text)


def _safe_text(value: Optional ) - Optional :
if value is None:
return None
if not isinstance(value, str):
value str(value)
return value.encode('utf-8', errors'replace').decode('utf-8')








.knllm_gatewayservice_base.py
"""Base classes and utilities for LLM service agents."""

from __future__ import annotations

import abc
import logging
import threading
import time
from typing import Any, Dict, Optional


class ServiceAgent(abc.ABC):
"""Abstract base for a throttled, single-backend LLM agent."""

def __init__(self, name: str, config: Dict ):
self.name name
self.config config
self.logger logging.getLogger(f"llm_service.name")
self._lock threading.Lock()
self._last_request_ts: Optional None
self._current_model: Optional None
self._last_model_switch: Optional None

------------------------------------------------------------------
Abstract API
------------------------------------------------------------------
abc.abstractmethod
def ensure_model_loaded(self, model: str):
"""Make sure the requested model is loaded and ready."""

abc.abstractmethod
def invoke(self, payload: Dict ) - Dict :
"""Perform the actual request against the backend."""

abc.abstractmethod
def unload_current_model(self):
"""Attempt to unload the currently loaded model if supported."""

abc.abstractmethod
def health(self) - Dict :
"""Return backend-specific health information."""

------------------------------------------------------------------
Shared helpers
------------------------------------------------------------------
def guard_rate_limits(self):
spacing_ms int(self.config.get("min_request_spacing_ms", 0))
if spacing_ms 0:
return
with self._lock:
if self._last_request_ts is None:
self._last_request_ts time.time()
return
elapsed_ms (time.time() - self._last_request_ts) 1000.0
if elapsed_ms spacing_ms:
delay (spacing_ms - elapsed_ms) 1000.0
if delay 0:
time.sleep(delay)
self._last_request_ts time.time()

def should_unload_idle(self) - bool:
idle_sec int(self.config.get("idle_unload_sec", 0) or 0)
if idle_sec 0:
return False
if self._last_request_ts is None:
return False
return (time.time() - self._last_request_ts) idle_sec

def update_model_tracking(self, model: str):
if model ! self._current_model:
self._current_model model
self._last_model_switch time.time()

def current_model(self) - Optional :
return self._current_model

def model_switch_info(self) - Optional :
return self._last_model_switch








.knllm_gatewayservices.py
"""Service agent registry and configuration loader."""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from kn.config import load_configs

from .service_base import ServiceAgent
from .services_lmstudio import LmStudioAgent
from .errors import ServiceNotConfiguredError

_REGISTRY
"lmstudio": LmStudioAgent,



def load_service_agent(name: str, cfg: Optional None) - ServiceAgent:
config cfg or load_configs().get("llm_services", ).get(name)
if not config:
raise ServiceNotConfiguredError(f"LLM service 'name' not configured")
provider config.get("provider") or name
cls _REGISTRY.get(provider)
if not cls:
raise ServiceNotConfiguredError(f"No service agent registered for provider 'provider'")
logging.getLogger("llm_service").debug("Loading service agent s (providers)", name, provider)
return cls(namename, configconfig)








.knllm_gatewayclient.py
"""Client-facing helpers to submit requests to the LLM gateway."""

from __future__ import annotations

import os
import time
import uuid
from pathlib import Path
from typing import Any, Dict, Optional

from kn.config import load_configs
from kn.utils.cache import get_cached_response, set_cached_response

from .errors import QueueTimeoutError, RequestRejectedError
from .models import GatewayRequest, GatewayResponse
from .storage import QueueStorage


def _storage(cfg: Optional None) - QueueStorage:
cfg cfg or load_configs()
path cfg.get("llm_gateway", ).get("queue_path")
if path:
return QueueStorage(Path(path))
return QueueStorage(Path(cfg.get("_root", ".knowledge")) "queues" "llm_gateway.sqlite")


def submit_chat_request(
prompt: str,
max_tokens: int 512,
temperature: float 0.2,
overrides: Optional None,
cache_key: Optional None,
plugin_name: Optional None,
cfg: Optional None,
wait: bool True,
force: bool False,
overwrite: bool False,
) - str:
cfg cfg or load_configs()
merged_overrides _merge_overrides(cfg, overrides, plugin_name)
service_name merged_overrides.get("service") or cfg.get("llm", ).get("service", "lmstudio")
model_name merged_overrides.get("model") or cfg.get("llm", ).get("model")
max_tokens int(merged_overrides.get("max_tokens", max_tokens))
temperature float(merged_overrides.get("temperature", temperature))

payload
"model": model_name,
"messages": ,
"max_tokens": max_tokens,
"temperature": temperature,

if "top_p" in merged_overrides:
payload merged_overrides

local_models os.getenv("LOCAL_MODELS_DIR")
if local_models:
payload.setdefault("extra_body", ) local_models

ck cache_key or f"service_namemodel_namemax_tokenstemperatureprompt"
cached get_cached_response(ck)
if cached is not None:
if not force and not overwrite:
return cached

req_id str(uuid.uuid4())
metadata
"cache_key": ck,
"plugin": plugin_name,

if force or overwrite:
metadata dict(metadata)
metadata force or overwrite
if overwrite:
metadata True
request GatewayRequest(request_idreq_id, serviceservice_name, modelmodel_name, payloadpayload, metadatametadata)
storage _storage(cfg)
storage.enqueue(request)

if not wait:
return ""

timeout_sec float(cfg.get("llm_gateway", ).get("wait_timeout_sec", 600))
deadline time.time() timeout_sec
while time.time() deadline:
response storage.fetch_response(request.request_id)
if response:
return _handle_response(response, ck)
time.sleep(0.2)
raise QueueTimeoutError(f"LLM gateway timed out waiting for request req_id")


def submit_generic_request(service: str, payload: Dict , metadata: Optional None, cfg: Optional None, wait: bool True, timeout_override: Optional None) - GatewayResponse:
cfg cfg or load_configs()
req_id str(uuid.uuid4())
storage _storage(cfg)
request GatewayRequest(request_idreq_id, serviceservice, modelpayload.get("model", ""), payloadpayload, metadatametadata or )
storage.enqueue(request)
if not wait:
return GatewayResponse(request_idreq_id, successTrue, contentNone, rawNone, errorNone, latency_msNone)
timeout_sec float(timeout_override if timeout_override is not None else cfg.get("llm_gateway", ).get("wait_timeout_sec", 600))
deadline time.time() timeout_sec
while time.time() deadline:
response storage.fetch_response(req_id)
if response:
return response
time.sleep(0.2)
raise QueueTimeoutError(f"Timeout waiting for generic gateway request req_id")


def _handle_response(response: GatewayResponse, cache_key: Optional ) - str:
if not response.success:
raise RequestRejectedError(response.error or "Gateway request failed")
content ""
if response.content:
content response.content
elif response.raw:
choices (response.raw or ).get("choices", )
if choices:
content choices .get("message", ).get("content", "")
if cache_key and content:
set_cached_response(cache_key, content)
return content


def _merge_overrides(cfg: Dict , overrides: Optional , plugin_name: Optional ) - Dict :
merged: Dict
plugin_block (cfg.get("plugins") or ).get(plugin_name or "") if plugin_name else None
if plugin_block and plugin_block.get("llm"):
merged.update(plugin_block )
if overrides:
merged.update(overrides)
return merged



.knllm_gatewayrunner.py
"""Gateway daemon that drains the queue and forwards to service agents."""

from __future__ import annotations

import logging
import time
from pathlib import Path
from typing import Dict, Optional

from kn.config import load_configs

from .errors import GatewayError
from .models import GatewayRequest, GatewayResponse
from .services import load_service_agent
from .storage import QueueStorage

logger logging.getLogger("llm_gateway.runner")


def run_gateway_service(service_name: str, loop_forever: bool True, cfg: Optional None):
cfg cfg or load_configs()
gateway_cfg cfg.get("llm_gateway", )
poll_interval_ms int(gateway_cfg.get("poll_interval_ms", 200))
storage _load_storage(gateway_cfg, cfg)
agent load_service_agent(service_name, cfg.get("llm_services", ).get(service_name))

logger.info("Gateway service 's' starting (pollsms)", service_name, poll_interval_ms)
while True:
req storage.dequeue_pending(service_name)
if not req:
if not loop_forever:
break
time.sleep(poll_interval_ms 1000.0)
if agent.should_unload_idle():
logger.info("Idle timeout reached; unloading model")
agent.unload_current_model()
continue

logger.info("Processing request s models", req.request_id, req.model)
try:
response _handle_request(agent, req)
except Exception as exc: pylint: disablebroad-except
logger.exception("Gateway request failed: s", exc)
storage.mark_failed(req.request_id, str(exc))
else:
storage.store_response(response)

if not loop_forever:
break


def _handle_request(agent, req: GatewayRequest) - GatewayResponse:
payload req.payload or
action payload.get("action")
if action "health":
result agent.health()
ok bool(result.get("ok", True)) if isinstance(result, dict) else True
return GatewayResponse(
request_idreq.request_id,
successok,
contentNone,
rawresult if isinstance(result, dict) else "result": result,
errorNone if ok else (result.get("error") if isinstance(result, dict) else "health check failed"),
latency_msNone,
)

agent.guard_rate_limits()
agent.ensure_model_loaded(req.model)
payload req.payload

max_context int(agent.config.get("max_context_tokens", 8192))
if _estimate_prompt_tokens(payload) max_context:
raise GatewayError(f"Context too large for model req.model")

result agent.invoke(payload)
latency_ms result.pop("_latency_ms", None)
content None
if result.get("choices"):
content result .get("message", ).get("content")
return GatewayResponse(
request_idreq.request_id,
successTrue,
contentcontent,
rawresult,
errorNone,
latency_mslatency_ms,
)


def _estimate_prompt_tokens(payload: Dict) - int:
messages payload.get("messages", )
total_chars sum(len(m.get("content", "")) for m in messages)
return int(total_chars 3.5) 1


def _load_storage(gateway_cfg: Dict, cfg: Dict) - QueueStorage:
queue_path gateway_cfg.get("queue_path")
if queue_path:
return QueueStorage(Path(queue_path))
root Path(cfg.get("_root", ".knowledge"))
return QueueStorage(root "queues" "llm_gateway.sqlite")








.knllm_gatewayerrors.py
class GatewayError(Exception):
"""Base exception for gateway-level errors."""


class QueueTimeoutError(GatewayError):
"""Raised when a request response is not received within the wait window."""


class RequestRejectedError(GatewayError):
"""Raised when the downstream service reports a failure for the request."""


class ServiceNotConfiguredError(GatewayError):
"""Raised when a requested service does not have a configuration block."""


.knllm_gatewayservices_lmstudio.py
"""LM Studio-specific service agent implementation."""

from __future__ import annotations

import logging
import time
from typing import Any, Dict

import requests

from .service_base import ServiceAgent


class LmStudioAgent(ServiceAgent):
"""Agent that serializes requests to a single LM Studio backend."""

def __init__(self, name: str, config: Dict ):
super().__init__(name, config)
self.session requests.Session()
self.base_url config.get("base_url", "http:127.0.0.1:12345v1").rstrip("")
self.api_key config.get("api_key", "lm-studio")
self.timeout int(config.get("timeout", 120))
self.manage_models bool(config.get("manage_models"))

------------------------------------------------------------------
def ensure_model_loaded(self, model: str):
if not self.manage_models:
if self.current_model() ! model:
self.logger.debug("Skipping explicit load for s (manage_models disabled)", model)
self.update_model_tracking(model)
return
force_reload bool(self.config.get("force_model_reload"))
if self.current_model() model and not force_reload:
return
if self.config.get("unload_on_switch", True) and self.current_model() and self.current_model() ! model:
self.unload_current_model()
payload "model": model
response self.session.post(
f"self.base_urlmodelsload",
headersself._headers(),
jsonpayload,
timeoutself.timeout,
)
if response.status_code ! 200:
response.raise_for_status()
self.logger.info("Loaded model s", model)
self.update_model_tracking(model)

def invoke(self, payload: Dict ) - Dict :
request_started time.time()
response self.session.post(
f"self.base_urlchatcompletions",
headersself._headers(),
jsonpayload,
timeoutself.timeout,
)
latency_ms (time.time() - request_started) 1000.0
response.raise_for_status()
data response.json()
data.setdefault("_latency_ms", latency_ms)
return data

def unload_current_model(self):
if not self.manage_models:
self.logger.debug("Skipping explicit unload (manage_models disabled)")
self.update_model_tracking(model"")
return
model self.current_model()
if not model:
return
try:
response self.session.post(
f"self.base_urlmodelsunload",
headersself._headers(),
json"model": model,
timeoutself.timeout,
)
if response.status_code 200:
self.logger.info("Unloaded model s", model)
else:
self.logger.warning("Failed to unload model s: s", model, response.text)
finally:
self.update_model_tracking(model"")

def health(self) - Dict :
try:
response self.session.get(
f"self.base_urlmodels",
headersself._headers(),
timeoutself.timeout,
)
response.raise_for_status()
data response.json()
return
"ok": True,
"available_models": ),

except Exception as exc: pylint: disablebroad-except
return "ok": False, "error": str(exc)

------------------------------------------------------------------
def _headers(self) - Dict :
headers
"Content-Type": "applicationjson",

if self.api_key:
headers f"Bearer self.api_key"
return headers








.pluginsattributessummary_20w.py
import json
import pathlib
import sys

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributessummary-20w')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

_ATTEMPT_LIMITS


def _safe_text(text: str) - str:
if not isinstance(text, str):
text str(text)
return text.encode('utf-8', errors'replace').decode('utf-8')


def _trim_text(text: str, limit: int) - str:
if len(text) limit:
return text
snippet text
cut snippet.rfind('nn')
if cut int(limit 0.6):
snippet snippet
return snippet 'nn '


def _write_attr(doc_id: str, value: str, confidence: float 0.8, notes: dict None None):
payload
"doc_id": doc_id,
"attribute": "summary-20w",
"value": value.strip(),
"confidence": confidence,
"pass": "cheap",

if notes:
payload.update(notes)
OUTDIR.joinpath(f"doc_id.json").write_text(json.dumps(payload, ensure_asciiFalse), encoding'utf-8')


def _fallback_value(reason: str) - str:
return f" "


for line in sys.stdin:
job json.loads(line)
doc_id job.get('doc_id')
text job.get('text', '')
if not text:
print(json.dumps("status": "skip", "doc_id": doc_id, ensure_asciiFalse))
continue

text _safe_text(text)
success False
last_error None

for limit in _ATTEMPT_LIMITS:
trimmed _trim_text(text, limit)
prompt _safe_text(f"Summarize the following in 20 words, terse and factual. If truncated, focus on the excerpt provided.nntrimmed")
try:
out chat(prompt, max_tokens100, temperature0.1, plugin_name'summary-20w', overwriteTrue)
out _safe_text(out)
_write_attr(doc_id, out)
print(json.dumps("status": "ok", "doc_id": doc_id, "length": len(out), ensure_asciiFalse))
success True
break
except RequestRejectedError as exc:
last_error str(exc)
if 'Context too large' in last_error:
continue
_write_attr(doc_id, _fallback_value('LLM request error'), notes"error": last_error)
print(json.dumps("status": "error", "doc_id": doc_id, "error": last_error, ensure_asciiFalse))
success True
break
except Exception as exc:
last_error str(exc)
_write_attr(doc_id, _fallback_value('unexpected error'), notes"error": last_error)
print(json.dumps("status": "error", "doc_id": doc_id, "error": last_error, ensure_asciiFalse))
success True
break

if not success:
msg last_error or 'context too large even after truncation'
_write_attr(doc_id, _fallback_value('context too large'), notes"error": msg)
print(json.dumps("status": "error", "doc_id": doc_id, "error": msg, ensure_asciiFalse))

.pluginsattributestopic_tags.py
import json
import pathlib
import sys

from kn.llm_gateway.client import submit_chat_request
from kn.utils.skeleton import load_skeleton_text

OUTDIR pathlib.Path('.knowledgeindexesattributestopic-tags')
OUTDIR.mkdir(parentsTrue, exist_okTrue)


def _safe_text(text: str) - str:
if not isinstance(text, str):
text str(text)
return text.encode('utf-8', errors'replace').decode('utf-8')


for line in sys.stdin:
job json.loads(line)
text load_skeleton_text(job.get('doc_id')) or job.get('text', '')
if not text:
continue
text _safe_text(text)
prompt _safe_text(
"Extract 3-7 topical tags (comma-separated, lowercase, no spaces, use-hyphens).n"
"Prefer domain-relevant terms.nn"
f"TEXT:ntextn"
)
out submit_chat_request(prompt, max_tokens64, temperature0.2, plugin_name'topic-tags', overwriteTrue)
out _safe_text(out)
OUTDIR.joinpath(f"job .json").write_text(json.dumps(
"doc_id": job ,
"attribute": "topic-tags",
"value": out.strip(),
"confidence": 0.7,
"pass": "cheap"
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps("status": "ok", "doc_id": job , ensure_asciiFalse))

.pluginsattributesglossary.py
import json
import pathlib
import sys

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributesglossary')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Extract 5-12 project-specific terms from the text with brief definitions. "
"Return a bullet list 'term: definition' focusing on unique or overloaded terms."
)

_ATTEMPT_LIMITS


def _safe_text(text: str) - str:
if not isinstance(text, str):
text str(text)
return text.encode('utf-8', errors'replace').decode('utf-8')


def _trim_text(text: str, limit: int) - str:
if len(text) limit:
return text
snippet text
cut snippet.rfind('nn')
if cut int(limit 0.6):
snippet snippet
return snippet 'nn '


def _write_attr(doc_id: str, value: str, confidence: float 0.6, error: str None None):
payload
'doc_id': doc_id,
'attribute': 'glossary',
'value': value.strip(),
'confidence': confidence,

if error:
payload error
OUTDIR.joinpath(f"doc_id.json").write_text(json.dumps(payload, ensure_asciiFalse), encoding'utf-8')


for line in sys.stdin:
job json.loads(line)
doc_id job.get('doc_id')
text job.get('text', '') or ''
if not text.strip():
print(json.dumps('status': 'skip', 'doc_id': doc_id))
continue

text _safe_text(text)
success False
last_error None

for limit in _ATTEMPT_LIMITS:
trimmed _trim_text(text, limit)
try:
out chat(PROMPT "nn" trimmed, max_tokens300, temperature0.2, plugin_name'glossary', overwriteTrue)
out _safe_text(out)
_write_attr(doc_id, out)
print(json.dumps('status': 'ok', 'doc_id': doc_id))
success True
break
except RequestRejectedError as exc:
last_error str(exc)
if 'Context too large' in last_error or exc.args and exc.args .startswith('404'):
continue
_write_attr(doc_id, ' ', errorlast_error)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': last_error))
success True
break
except Exception as exc:
last_error str(exc)
_write_attr(doc_id, ' ', errorlast_error)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': last_error))
success True
break

if not success:
msg last_error or 'context too large or endpoint missing'
_write_attr(doc_id, ' ', errormsg)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': msg))



.pluginsattributesrequirements.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributesrequirements')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Extract requirement-like statements (shallshouldmust or implied constraints). "
"Output 5-15 bullets, each concise and testable. If none, return an empty list."
)

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
out chat(PROMPT "nn" text, max_tokens350, temperature0.2, plugin_name'requirements', overwriteTrue)
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'requirements', 'value': out.strip(), 'confidence': 0.6
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributestodo_items.py
import sys, json, pathlib, re

OUTDIR pathlib.Path('.knowledgeindexesattributestodo-items')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PAT re.compile(r"(?im)(?: )?s(TODOFIXMENOTE) (.4,120)")

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
items
for m in PAT.finditer(text):
items.append('kind': m.group(1).upper(), 'text': m.group(2).strip())
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'todo-items', 'value': items, 'confidence': 0.6
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributesfaq_pairs.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributesfaq-pairs')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Generate 5-10 FAQ QA pairs strictly grounded in the provided text. "
"Format as '- Q: ...n A: ...'. Avoid hallucinations; answer 'unknown' if not supported."
)

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
out chat(PROMPT "nn" text, max_tokens600, temperature0.2, plugin_name'faq-pairs', overwriteTrue)
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'faq-pairs', 'value': out.strip(), 'confidence': 0.6
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributessummaries.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributessummaries')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

TEMPLATES
'short': ("Summarize the text in 50 words, terse, factual.", 120),
'medium': ("Summarize the text in 150 words, clear sections if relevant.", 360),
'long': ("Summarize the text in 400 words, comprehensive but concise.", 900),
'outline': ("Produce a hierarchical outline (bullets) of the main ideas.", 360),


for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
payload job.get('payload') or
mode (payload.get('mode') or 'short').lower()
words payload.get('words')
tpl, max_tokens TEMPLATES.get(mode, TEMPLATES )
if words:
tpl f"Summarize the text in words words, terse and factual."
max_tokens int(words) 3
prompt tpl "nn" text
out chat(prompt, max_tokensmax_tokens, temperature0.2, plugin_name'summaries', overwriteTrue)
(OUTDIR f"job _mode.json").write_text(json.dumps(
'doc_id': job , 'attribute': f'summary-mode', 'value': out.strip(), 'mode': mode, 'confidence': 0.7
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ,'mode':mode))



.pluginsattributeskeyphrases.py
import json
import pathlib
import sys

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributeskeyphrases')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Extract 8-20 keyphrases capturing entities, terms, and concepts.n"
"- Return a comma-separated listn"
"- Lowercase, hyphenate multi-word terms, no spaces around commasn"
"- Prefer domain-specific terminologynnTEXT:nbodyn"
)

_ATTEMPT_LIMITS


def _safe_text(text: str) - str:
if not isinstance(text, str):
text str(text)
return text.encode('utf-8', errors'replace').decode('utf-8')


def _trim_text(text: str, limit: int) - str:
if len(text) limit:
return text
snippet text
cut snippet.rfind('nn')
if cut int(limit 0.6):
snippet snippet
return snippet 'nn '


def _write_attr(doc_id: str, value, confidence0.7, error: str None None):
payload
'doc_id': doc_id,
'attribute': 'keyphrases',
'value': value,
'confidence': confidence,

if error:
payload error
OUTDIR.joinpath(f"doc_id.json").write_text(json.dumps(payload, ensure_asciiFalse), encoding'utf-8')


for line in sys.stdin:
job json.loads(line)
doc_id job.get('doc_id')
text job.get('text', '') or ''
if not text.strip():
print(json.dumps('status': 'skip', 'doc_id': doc_id))
continue

text _safe_text(text)
success False
last_error None

for limit in _ATTEMPT_LIMITS:
trimmed _trim_text(text, limit)
prompt PROMPT.format(bodytrimmed)
try:
out chat(prompt, max_tokens120, temperature0.2, plugin_name'keyphrases', overwriteTrue)
out _safe_text(out)
tags
_write_attr(doc_id, tags)
print(json.dumps('status': 'ok', 'doc_id': doc_id, 'count': len(tags)))
success True
break
except RequestRejectedError as exc:
last_error str(exc)
if 'Context too large' in last_error:
continue
_write_attr(doc_id, , errorlast_error)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': last_error))
success True
break
except Exception as exc:
last_error str(exc)
_write_attr(doc_id, , errorlast_error)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': last_error))
success True
break

if not success:
msg last_error or 'context too large even after truncation'
_write_attr(doc_id, 'summary_unavailable:context-too-large', errormsg)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': msg))



.pluginsattributesbridge_candidates.py
import sys, json, pathlib, re
from collections import defaultdict

OUTDIR pathlib.Path('.knowledgeindexesattributesbridge-candidates')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

CHUNKS_DIR pathlib.Path('.knowledgeindexeschunks')

def naive_entities(text: str):
return list(set(re.findall(r" b .ww", text)))

def build_entity_domains_map(limit_files: intNoneNone):
ent_to_domains: dict defaultdict(set)
count 0
for p in CHUNKS_DIR.glob('.json'):
rec json.loads(p.read_text(encoding'utf-8'))
dom rec.get('meta',).get('domain','root')
for e in naive_entities(rec.get('text','')):
ent_to_domains .add(dom)
count 1
if limit_files and count limit_files:
break
return ent_to_domains

ENT_DOMAINS build_entity_domains_map()

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
dom (job.get('meta') or ).get('domain','root')
ents naive_entities(text)
bridges
for e in ents:
doms ENT_DOMAINS.get(e, set())
other
if other:
bridges.append('entity': e, 'other_domains': sorted(other))
rank by number of other domains desc
bridges.sort(keylambda x: (-len(x ), x .lower()))
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'bridge-candidates', 'value': bridges
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributesrisk_scan.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUTDIR pathlib.Path('.knowledgeindexesattributesrisk-scan')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Assess the following text for risks. Return JSON with fields:"
" contradictions:boolean, speculation:boolean, outdated:boolean, pii_hint:boolean, notes:string."
" Keep notes 40 words.nnTEXT:nbodyn"
)

for line in sys.stdin:
job json.loads(line)
text load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
if not text.strip():
continue
raw chat(PROMPT.format(bodytext), max_tokens160, temperature0.1, plugin_name'risk-scan', overwriteTrue)
best-effort JSON parse; if fails, wrap as notes
try:
data json.loads(raw)
if not isinstance(data, dict):
raise ValueError()
except Exception:
data 'contradictions': False, 'speculation': False, 'outdated': False, 'pii_hint': False, 'notes': raw.strip()
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'risk-scan', 'value': data
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributesrecent_summary.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributesrecent-summary')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Summarize the most salient updates and current state. Be concise (120 words)."
" If there are TODOs or open questions, list them as short bullets.nnTEXT:nbodyn"
)

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
out chat(PROMPT.format(bodytext), max_tokens220, temperature0.2, plugin_name'recent-summary', overwriteTrue)
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'recent-summary', 'value': out.strip(), 'confidence': 0.7
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributessummary_short.py
import sys
import json
import pathlib
from kn.llm_gateway.client import submit_chat_request

OUTDIR pathlib.Path('.knowledgeindexesattributessummary-short')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

for line in sys.stdin:
job json.loads(line)
text job.get('text', '')
if not text:
print(json.dumps('status': 'skip', 'doc_id': job.get('doc_id'), ensure_asciiFalse))
continue
doc_id job.get('doc_id')
payload job.get('payload') or
prompt payload.get('prompt') or "Summarize the text in 120 words, clear and structured.nn" text
try:
out submit_chat_request(
prompt,
max_tokenspayload.get('max_tokens', job.get('max_tokens', 512)),
temperaturepayload.get('temperature', 0.2),
overridespayload.get('llm'),
cache_keyf"summary-shortdoc_id",
plugin_name'summary-short',
overwriteTrue
)
except Exception as exc:
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': str(exc), ensure_asciiFalse))
continue
rec
'doc_id': doc_id,
'attribute': 'summary-short',
'value': out.strip(),
'confidence': 0.75,
'pass': payload.get('pass', 'default')

OUTDIR.joinpath(f"doc_id.json").write_text(json.dumps(rec, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status': 'ok', 'doc_id': doc_id, 'length': len(out), ensure_asciiFalse))



.pluginsattributessummary_medium.py
import sys, json, pathlib

from kn.llm_gateway.errors import RequestRejectedError
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text


OUTDIR pathlib.Path('.knowledgeindexesattributessummary-medium')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT "Summarize the text in 150 words, clear, objective.nnbodyn"
_ATTEMPT_LIMITS


def _safe_text(text: str) - str:
if not isinstance(text, str):
text str(text or '')
return text.encode('utf-8', errors'replace').decode('utf-8')


def _trim_text(text: str, limit: int) - str:
if len(text) limit:
return text
snippet text
cut snippet.rfind('nn')
if cut int(limit 0.6):
snippet snippet
return snippet 'nn '


def _write_attr(doc_id: str, value: str, confidence: float 0.7, error: str None None):
payload
'doc_id': doc_id,
'attribute': 'summary-medium',
'value': value.strip(),
'confidence': confidence,

if error:
payload error
OUTDIR.joinpath(f"doc_id.json").write_text(json.dumps(payload, ensure_asciiFalse), encoding'utf-8')


def _fallback(reason: str) - str:
return f" "


for line in sys.stdin:
job json.loads(line)
doc_id job.get('doc_id')
text load_skeleton_text(doc_id) or job.get('text', '') or ''
if not text.strip():
print(json.dumps('status': 'skip', 'doc_id': doc_id, ensure_asciiFalse))
continue

text _safe_text(text)
success False
last_err None

for limit in _ATTEMPT_LIMITS:
trimmed _trim_text(text, limit)
prompt PROMPT.format(bodytrimmed)
try:
out chat(prompt, max_tokens380, temperature0.2, plugin_name'summary-medium', overwriteTrue)
out _safe_text(out)
_write_attr(doc_id, out)
print(json.dumps('status': 'ok', 'doc_id': doc_id, 'length': len(out), ensure_asciiFalse))
success True
break
except RequestRejectedError as exc:
last_err str(exc)
if 'Context too large' in last_err:
continue
_write_attr(doc_id, _fallback('LLM request error'), errorlast_err)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': last_err, ensure_asciiFalse))
success True
break
except Exception as exc:
last_err str(exc)
_write_attr(doc_id, _fallback('unexpected error'), errorlast_err)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': last_err, ensure_asciiFalse))
success True
break

if not success:
reason last_err or 'context too large'
_write_attr(doc_id, _fallback('context too large'), errorreason)
print(json.dumps('status': 'error', 'doc_id': doc_id, 'error': reason, ensure_asciiFalse))



.pluginsattributessummary_long.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUTDIR pathlib.Path('.knowledgeindexesattributessummary-long')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT "Summarize the text in 400 words, comprehensive but concise.nnbodyn"

for line in sys.stdin:
job json.loads(line)
text load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
if not text.strip():
continue
out chat(PROMPT.format(bodytext), max_tokens900, temperature0.2, plugin_name'summary-long', overwriteTrue)
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'summary-long', 'value': out.strip(), 'confidence': 0.7
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributessummary_outline.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUTDIR pathlib.Path('.knowledgeindexesattributessummary-outline')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT "Produce a hierarchical outline (bulleted) of the main ideas. Keep concise.nnbodyn"

for line in sys.stdin:
job json.loads(line)
text load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
if not text.strip():
continue
out chat(PROMPT.format(bodytext), max_tokens360, temperature0.2, plugin_name'summary-outline', overwriteTrue)
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'attribute': 'summary-outline', 'value': out.strip(), 'confidence': 0.7
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':job ))



.pluginsattributesmulti_basic.py
import sys, json, pathlib
from kn.utils.llm_client import chat
from kn.utils.skeleton import load_skeleton_text

OUT pathlib.Path('.knowledgeindexesattributes')
OUT.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"You are given a document. Produce a compact JSON with keys:"
" summary_short (50w), summary_medium (150w), outline (bulleted), tags (3-7 comma-separated), risk (JSON with contradictionsspeculationoutdatedpii_hint boolean and notes40w).n"
"Return only JSON.nnTEXT:nbodyn"
)

def write_attr(attr, doc_id, value, extraNone):
(OUTattr).mkdir(parentsTrue, exist_okTrue)
rec 'doc_id': doc_id, 'attribute': attr, 'value': value
if isinstance(extra, dict): rec.update(extra)
(OUTattrf"doc_id.json").write_text(json.dumps(rec, ensure_asciiFalse), encoding'utf-8')

for line in sys.stdin:
job json.loads(line)
text load_skeleton_text(job.get('doc_id')) or job.get('text','') or ''
if not text.strip():
continue
doc_id job.get('doc_id')
Allow per-plugin model override via payload
overrides (job.get('payload') or ).get('llm') or
keep under tighter limits to reduce timeouts
raw chat(PROMPT.format(bodytext), max_tokens400, temperature0.2, overridesoverrides, plugin_name'multi-basic', overwriteTrue)
data
try:
data json.loads(raw)
except Exception:
best-effort parse: write whole text as medium summary
data 'summary_medium': raw.strip()
write individual attribute files
if 'summary_short' in data:
write_attr('summary-short', doc_id, data , 'confidence':0.7)
if 'summary_medium' in data:
write_attr('summary-medium', doc_id, data , 'confidence':0.7)
if 'outline' in data:
write_attr('summary-outline', doc_id, data , 'confidence':0.7)
tags data.get('tags')
if isinstance(tags, str):
write_attr('keyphrases', doc_id, tags, 'confidence':0.7)
risk data.get('risk')
if isinstance(risk, dict):
write_attr('risk-scan', doc_id, risk)
print(json.dumps('status':'ok','doc_id':doc_id))



.pluginsattributesdoc_skeleton.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributesdoc-skeleton')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"Produce a compact JSON skeleton for the document with keys: outline (bulleted), summary100 (100 words), keyphrases (comma-separated)."
" Return only JSON.nnTEXT:nbodyn"
)

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
doc_id job.get('doc_id')
overrides (job.get('payload') or ).get('llm') or
Qwen 32B has 12k context; keep outputs compact
raw chat(PROMPT.format(bodytext), max_tokens240, temperature0.2, overridesoverrides, plugin_name'doc-skeleton', overwriteTrue)
try:
data json.loads(raw)
except Exception:
data 'summary100': raw.strip()
(OUTDIRf"doc_id.json").write_text(json.dumps('doc_id':doc_id,'attribute':'doc-skeleton','value':data, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':doc_id))



.pluginsattributeschunk_summary.py
import sys, json, pathlib
from kn.utils.llm_client import chat

OUTDIR pathlib.Path('.knowledgeindexesattributeschunk-summary')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT "Summarize this chunk in 2-3 sentences, terse and factual.nnbodyn"

for line in sys.stdin:
job json.loads(line)
text job.get('text','') or ''
if not text.strip():
continue
out chat(PROMPT.format(bodytext), max_tokens120, temperature0.2, plugin_name'chunk-summary', overwriteTrue)
(OUTDIR f"job .json").write_text(json.dumps(
'doc_id': job , 'chunk_id': job.get('chunk_id'), 'attribute': 'chunk-summary', 'value': out.strip(), 'confidence': 0.7
, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','chunk_id': job.get('chunk_id')))



.pluginsattributesdoc_reduce.py
import sys, json, pathlib
from kn.utils.llm_client import chat

CHUNK_SUM_DIR pathlib.Path('.knowledgeindexesattributeschunk-summary')
OUTDIR pathlib.Path('.knowledgeindexesattributesdoc-reduce')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

PROMPT (
"You are given ordered chunk summaries for a single document.n"
"Produce a compact JSON with keys: outline (bulleted), summary (150 words), tags (3-7 comma-separated).n"
"Return only JSON.nnSUMMARIES:nbodyn"
)

def load_chunk_summaries(doc_id: str):
items
for p in sorted(CHUNK_SUM_DIR.glob('.json')):
try:
rec json.loads(p.read_text(encoding'utf-8'))
if rec.get('doc_id') doc_id:
items.append(rec)
except Exception:
continue
try to sort by chunk_id numeric tail if present in metadata; fallback original order
def _key(r):
cid r.get('chunk_id','')
try:
return int(cid.split('-') , 16)
except Exception:
return 0
items.sort(key_key)
return items

for line in sys.stdin:
job json.loads(line)
doc_id job.get('doc_id')
if not doc_id:
continue
chunks load_chunk_summaries(doc_id)
if not chunks:
continue
ordered 'n- '.join( )
raw chat(PROMPT.format(bodyordered), max_tokens360, temperature0.2, plugin_name'doc-reduce', overwriteTrue)
try:
data json.loads(raw)
except Exception:
data 'summary': raw.strip()
(OUTDIRf"doc_id.json").write_text(json.dumps('doc_id':doc_id,'attribute':'doc-reduce','value':data, ensure_asciiFalse), encoding'utf-8')
print(json.dumps('status':'ok','doc_id':doc_id))



.pluginsattributespii_scan.py
import sys, json, re, pathlib

OUTDIR pathlib.Path('.knowledgeindexesattributespii-scan')
OUTDIR.mkdir(parentsTrue, exist_okTrue)

EMAIL re.compile(r" . 2,")
PHONE re.compile(r"?d 6,d")
CREDIT re.compile(r"b(?:d ?)13,16b")


def score_flags(text):
findings
for m in EMAIL.findall(text or ""):
findings.append("type": "email", "value": m, "severity": 0.5)
for m in PHONE.findall(text or ""):
findings.append("type": "phone", "value": m, "severity": 0.4)
for m in CREDIT.findall(text or ""):
findings.append("type": "credit", "value": m, "severity": 0.9)
risk max((f for f in findings), default0.0)
return findings, risk


for line in sys.stdin:
job json.loads(line)
text job.get('text', '') or ""
findings, risk score_flags(text)
OUTDIR.joinpath(f"job .json").write_text(
json.dumps(

"doc_id": job ,
"attribute": "pii-scan",
"value": findings,
"risk": risk,
"confidence": 0.6,
,
ensure_asciiFalse,
),
encoding'utf-8',
)
print(json.dumps("status": "ok", "doc_id": job ))


