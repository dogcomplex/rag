## kn-graphRAG-starter

Local-first Graph RAG toolkit: ingest a code/content repo, chunk it, embed, build a graph, enrich with attribute plugins, and query with lightweight retrieval. Optimized for Windows + local LLMs (LM Studio), but works cross‑platform.

### Features
- Ingest and incremental updates over a target repo or folder
- Chunking + embeddings (SentenceTransformers)
- Graph build + community summaries (NetworkX + Louvain)
- Attribute plugins (summaries, keyphrases, glossary, PII scan, FAQs, etc.)
- Simple dashboard/API to plan/enqueue jobs and inspect coverage

## Quickstart (Windows PowerShell)
1) Python 3.10+ installed
2) From the project root, run:
   - `.\u200bsetup.ps1 -RepoPath ".\FAE" -Model "llama-3.1-8b-instruct"`

What it does:
- Creates `.venv`, installs dependencies from `requirements-win.txt`
- Writes a `.env` with defaults (LLM and embedding models, repo path, knowledge root)
- Runs initial ingest: scan → chunk → embed → graph → summarize

LM Studio optional: start a server at `http://localhost:12345` (or adjust `OPENAI_BASE_URL` in `.env`).

## Manual setup
If you prefer manual steps:
1) Create and activate a venv, then install deps
   - Windows: `python -m venv .venv; . .\.venv\Scripts\Activate.ps1; pip install -r requirements-win.txt`
   - Non‑Windows: `python -m venv .venv; source .venv/bin/activate; pip install -r requirements.txt`
2) Configure `.env` (auto‑generated by `setup.ps1`, example keys):
   - `OPENAI_BASE_URL` (e.g., `http://127.0.0.1:12345/v1`)
   - `OPENAI_API_KEY` (placeholder for local providers)
   - `OPENAI_MODEL` (chat/instruct model id)
   - `EMBED_MODEL=BAAI/bge-small-en-v1.5`
   - `REPO_PATH=./FAE`
   - `KN_ROOT=.knowledge`
   - `OCR_ENABLED=false`
3) Ingest a repo/folder:
   - `python bin/ingest_build_graph.py --repo <path> --full`

## Common workflows
### Query
```bash
python bin/query_rag.py --q "What does the planner do?" --topk 12
```

### Dashboard API server
```bash
python bin/dashboard_server.py
```
Open `http://localhost:5051` for the minimal dashboard.

Selected endpoints:
- `GET /api/status` – system status, plugin coverage, queue, LLM health
- `POST /api/ingest { repo, full }` – start ingest
- `POST /api/plan { plugins, only_missing, limit, ... }` – plan jobs
- `POST /api/worker/start { plugins, batch }` – start a worker
- `POST /api/worker/stop { id? }` – stop a worker (or all)

### Attribute enrichment worker
```bash
python bin/enrich_worker.py --plugins summaries,keyphrases --batch 32 --watch
```
Runs continuously with `--watch`, pulling jobs from `.knowledge/queues/jobs.sqlite`.

### Exports and reports
- `bin/export_graph.py` – export graph
- `bin/export_monofile.py` – export consolidated artifacts
- `bin/render_report.py` + `bin/report_*` – static coverage/report generation

## Data layout (`.knowledge`)
- `indexes/chunks/` – chunk JSONs
- `indexes/attributes/<plugin>/` – plugin outputs
- `queues/jobs.sqlite` – job queue
- `exports/reports/` – generated report site

## Plugins
See `plugins/attributes/` for available attribute extractors:
- `summaries`, `topic-tags`, `pii-scan`, `glossary`, `requirements`, `todo-items`,
  `faq-pairs`, `keyphrases`, `bridge-candidates`, `risk-scan`, `recent-summary`, etc.

## Configuration
`kn.config.load_configs()` merges defaults and `.knowledge/config/*.yml`.
- Plugin defaults are persisted to `.knowledge/config/models.yml` under `plugins`.
- The dashboard exposes `GET/POST /api/plugins/config` to read/update these.

## Notes (Windows)
- `requirements-win.txt` includes `python-magic-bin` and references CUDA wheels if you want PyTorch GPU. CPU works fine for small repos.
- `hnswlib` is disabled on Windows by default (`requirements.txt` enables it for non‑Windows). Vector search falls back to a pure‑Python path when needed.

## Troubleshooting
- LLM health shows unreachable: confirm `OPENAI_BASE_URL` (LM Studio or other local endpoint) and that the server is running.
- No chunks/attributes found: ensure `REPO_PATH` points to a folder with files, and rerun ingest with `--full` for a fresh build.

## License
See repository terms or contact the author.
